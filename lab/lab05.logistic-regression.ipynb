{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# logistic regression 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = np.array([[1,2], [2,3], [3,1], [4,3], [5,3], [6,2]], dtype='float32')\n",
    "y_data = np.array([[0], [0], [0], [1], [1], [1]], dtype='float32')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 및 손실 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "def logistic_regression(x, y, w, b):\n",
    "    z = tf.add(tf.matmul(x, w), b)\n",
    "    g = tf.sigmoid(z)\n",
    "    return g\n",
    "\n",
    "# define cross entropy loss\n",
    "cost = lambda prob, y: -y*tf.math.log(prob) - (1-y)*tf.math.log(1-prob)\n",
    "\n",
    "# define cast function\n",
    "def get_class(prob):\n",
    "    return tf.cast(prob > 0.5, dtype='float32')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 및 모니터링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost: 8.442277 , accuracy:  0.5\n",
      "cost: 4.8966146 , accuracy:  0.0\n",
      "cost: 5.2159467 , accuracy:  0.5\n",
      "cost: 4.954792 , accuracy:  0.5\n",
      "cost: 6.0589056 , accuracy:  0.5\n",
      "cost: 3.9269507 , accuracy:  0.5\n",
      "cost: 4.3612866 , accuracy:  0.5\n",
      "cost: 3.8159628 , accuracy:  0.5\n",
      "cost: 4.219742 , accuracy:  0.5\n",
      "cost: 3.502408 , accuracy:  0.5\n",
      "cost: 3.6915386 , accuracy:  0.6666667\n",
      "cost: 3.2872195 , accuracy:  0.6666667\n",
      "cost: 3.3643222 , accuracy:  0.8333333\n",
      "cost: 3.113732 , accuracy:  0.6666667\n",
      "cost: 3.1294491 , accuracy:  0.8333333\n",
      "cost: 2.973683 , accuracy:  0.6666667\n",
      "cost: 2.959144 , accuracy:  0.8333333\n",
      "cost: 2.8593364 , accuracy:  0.6666667\n",
      "cost: 2.8317885 , accuracy:  0.8333333\n",
      "cost: 2.764369 , accuracy:  0.8333333\n",
      "cost: 2.7326763 , accuracy:  0.8333333\n",
      "cost: 2.6838274 , accuracy:  0.8333333\n",
      "cost: 2.6521025 , accuracy:  0.8333333\n",
      "cost: 2.6138735 , accuracy:  0.8333333\n",
      "cost: 2.5836806 , accuracy:  0.8333333\n",
      "cost: 2.5515625 , accuracy:  0.8333333\n",
      "cost: 2.52322 , accuracy:  0.8333333\n",
      "cost: 2.494694 , accuracy:  0.8333333\n",
      "cost: 2.4680176 , accuracy:  0.8333333\n",
      "cost: 2.4416986 , accuracy:  0.8333333\n",
      "cost: 2.4163804 , accuracy:  0.8333333\n",
      "cost: 2.3915224 , accuracy:  0.8333333\n",
      "cost: 2.3672934 , accuracy:  0.8333333\n",
      "cost: 2.3435006 , accuracy:  0.8333333\n",
      "cost: 2.3201716 , accuracy:  0.8333333\n",
      "cost: 2.2972367 , accuracy:  0.8333333\n",
      "cost: 2.2746897 , accuracy:  0.8333333\n",
      "cost: 2.252503 , accuracy:  0.8333333\n",
      "cost: 2.2306662 , accuracy:  0.8333333\n",
      "cost: 2.209166 , accuracy:  0.8333333\n",
      "cost: 2.187995 , accuracy:  0.8333333\n",
      "cost: 2.1671445 , accuracy:  0.8333333\n",
      "cost: 2.1466088 , accuracy:  0.8333333\n",
      "cost: 2.1263816 , accuracy:  0.8333333\n",
      "cost: 2.106457 , accuracy:  0.8333333\n",
      "cost: 2.0868304 , accuracy:  0.8333333\n",
      "cost: 2.067497 , accuracy:  0.8333333\n",
      "cost: 2.048452 , accuracy:  0.8333333\n",
      "cost: 2.02969 , accuracy:  0.8333333\n",
      "cost: 2.011207 , accuracy:  0.8333333\n",
      "cost: 1.9929985 , accuracy:  0.8333333\n",
      "cost: 1.9750597 , accuracy:  0.8333333\n",
      "cost: 1.957387 , accuracy:  0.8333333\n",
      "cost: 1.9399755 , accuracy:  0.8333333\n",
      "cost: 1.9228208 , accuracy:  0.8333333\n",
      "cost: 1.9059184 , accuracy:  0.8333333\n",
      "cost: 1.8892651 , accuracy:  0.8333333\n",
      "cost: 1.8728557 , accuracy:  0.8333333\n",
      "cost: 1.8566868 , accuracy:  0.8333333\n",
      "cost: 1.8407539 , accuracy:  0.8333333\n",
      "cost: 1.8250532 , accuracy:  0.8333333\n",
      "cost: 1.8095808 , accuracy:  0.8333333\n",
      "cost: 1.7943329 , accuracy:  0.8333333\n",
      "cost: 1.7793058 , accuracy:  0.8333333\n",
      "cost: 1.764495 , accuracy:  0.8333333\n",
      "cost: 1.7498976 , accuracy:  0.8333333\n",
      "cost: 1.7355094 , accuracy:  0.8333333\n",
      "cost: 1.7213272 , accuracy:  0.8333333\n",
      "cost: 1.707347 , accuracy:  0.8333333\n",
      "cost: 1.6935657 , accuracy:  0.8333333\n",
      "cost: 1.6799796 , accuracy:  0.8333333\n",
      "cost: 1.6665854 , accuracy:  0.8333333\n",
      "cost: 1.6533797 , accuracy:  0.8333333\n",
      "cost: 1.6403593 , accuracy:  0.8333333\n",
      "cost: 1.6275214 , accuracy:  0.8333333\n",
      "cost: 1.614862 , accuracy:  0.8333333\n",
      "cost: 1.6023784 , accuracy:  0.8333333\n",
      "cost: 1.5900674 , accuracy:  0.8333333\n",
      "cost: 1.5779263 , accuracy:  0.8333333\n",
      "cost: 1.5659517 , accuracy:  0.8333333\n",
      "cost: 1.5541412 , accuracy:  0.8333333\n",
      "cost: 1.5424917 , accuracy:  0.8333333\n",
      "cost: 1.5310003 , accuracy:  0.8333333\n",
      "cost: 1.5196644 , accuracy:  1.0\n",
      "cost: 1.5084809 , accuracy:  1.0\n",
      "cost: 1.4974478 , accuracy:  1.0\n",
      "cost: 1.4865624 , accuracy:  1.0\n",
      "cost: 1.4758216 , accuracy:  1.0\n",
      "cost: 1.4652231 , accuracy:  1.0\n",
      "cost: 1.454765 , accuracy:  1.0\n",
      "cost: 1.4444438 , accuracy:  1.0\n",
      "cost: 1.4342583 , accuracy:  1.0\n",
      "cost: 1.4242057 , accuracy:  1.0\n",
      "cost: 1.4142832 , accuracy:  1.0\n",
      "cost: 1.4044892 , accuracy:  1.0\n",
      "cost: 1.3948213 , accuracy:  1.0\n",
      "cost: 1.3852773 , accuracy:  1.0\n",
      "cost: 1.3758556 , accuracy:  1.0\n",
      "cost: 1.3665533 , accuracy:  1.0\n",
      "cost: 1.357369 , accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "# define trainable variables\n",
    "rand_w = tf.random.uniform(shape=[x_data.shape[-1], 1], seed=42)\n",
    "rand_b = tf.random.uniform(shape=[], seed=42)\n",
    "\n",
    "w = tf.Variable(rand_w)\n",
    "b = tf.Variable(rand_b)\n",
    "\n",
    "# hyperparameters\n",
    "epochs = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "# define gradient descent optimizer on OSX M2 chip\n",
    "sgd = tf.optimizers.legacy.SGD(learning_rate=learning_rate)\n",
    "\n",
    "# training\n",
    "for e in range(epochs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        prob = logistic_regression(x_data, y_data, w, b)\n",
    "        loss = cost(prob, y_data)\n",
    "    \n",
    "    # get differentiations\n",
    "    dl_dw, dl_db = tape.gradient(loss, [w, b])\n",
    "\n",
    "    # get accuracy\n",
    "    y_pred = get_class(prob)\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(y_data, y_pred), dtype='float32')).numpy()\n",
    "\n",
    "    print(\"cost:\", tf.reduce_sum(loss).numpy(), \", accuracy: \", accuracy)\n",
    "\n",
    "    w.assign_sub(learning_rate * dl_dw)\n",
    "    b.assign_sub(learning_rate * dl_db)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
