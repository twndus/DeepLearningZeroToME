{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN practice\n",
    "\n",
    "1. simple CNN\n",
    "2. deep CNN\n",
    "3. ?\n",
    "4. ?\n",
    "5. ensemble\n",
    "6. low memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-03 14:57:51.770855: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-03 14:57:51.919790: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-07-03 14:57:52.588461: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda-11.7/lib64:/usr/local/cuda/extras/CUPTI/:/usr/local/cuda-11.7/lib64:/usr/local/cuda/extras/CUPTI/\n",
      "2023-07-03 14:57:52.588566: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda-11.7/lib64:/usr/local/cuda/extras/CUPTI/:/usr/local/cuda-11.7/lib64:/usr/local/cuda/extras/CUPTI/\n",
      "2023-07-03 14:57:52.588575: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from mnist import MNIST"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28, 1), (60000,), (10000, 28, 28, 1), (10000,))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mndata = MNIST('../data/mnist/')\n",
    "\n",
    "train_images, train_labels = mndata.load_training()\n",
    "test_images, test_labels = mndata.load_testing()\n",
    "\n",
    "train_images = np.array(train_images).reshape((-1, 28, 28, 1))\n",
    "train_labels = np.array(train_labels)\n",
    "test_images = np.array(test_images).reshape((-1, 28, 28, 1))\n",
    "test_labels = np.array(test_labels)\n",
    "\n",
    "train_images.shape, train_labels.shape, test_images.shape, test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = 0\n",
    "\n",
    "plt.title(train_labels[0])\n",
    "plt.imshow(train_images[0], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-03 14:57:58.337019: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-03 14:57:58.337296: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-03 14:57:58.371595: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-03 14:57:58.371888: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-03 14:57:58.372080: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-03 14:57:58.372252: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-03 14:57:58.373092: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-03 14:57:58.546829: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-03 14:57:58.547074: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-03 14:57:58.547252: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-03 14:57:58.547409: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-03 14:57:58.547585: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-03 14:57:58.547754: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-03 14:57:59.661288: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-03 14:57:59.661559: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-03 14:57:59.661774: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-03 14:57:59.661953: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-03 14:57:59.662119: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-03 14:57:59.662279: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30961 MB memory:  -> device: 0, name: Tesla V100S-PCIE-32GB, pci bus id: 0000:00:06.0, compute capability: 7.0\n",
      "2023-07-03 14:57:59.662857: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-03 14:57:59.663032: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 30961 MB memory:  -> device: 1, name: Tesla V100S-PCIE-32GB, pci bus id: 0000:00:07.0, compute capability: 7.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(TensorShape([60000, 10]), TensorShape([10000, 10]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels_1hot = tf.one_hot(train_labels, 10)\n",
    "test_labels_1hot = tf.one_hot(test_labels, 10)\n",
    "\n",
    "train_labels_1hot.shape, test_labels_1hot.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((600, 100, 28, 28, 1), (600, 100, 10), (100, 100, 28, 28, 1), (100, 100, 10))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 100\n",
    "\n",
    "train_batch_images = np.array([train_images[idx: idx+100] for idx in range(train_images.shape[0]//batch_size)])\n",
    "train_batch_labels = np.array([train_labels_1hot[idx: idx+100] for idx in range(train_images.shape[0]//batch_size)])\n",
    "test_batch_images = np.array([test_images[idx: idx+100] for idx in range(test_images.shape[0]//batch_size)])\n",
    "test_batch_labels = np.array([test_labels_1hot[idx: idx+100] for idx in range(test_images.shape[0]//batch_size)])\n",
    "\n",
    "\n",
    "train_batch_images.shape, train_batch_labels.shape, test_batch_images.shape, test_batch_labels.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple CNN\n",
    "\n",
    "5 layers (last FC-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SimpleCNN():\n",
    "#     def __init__(self):\n",
    "#         pass\n",
    "\n",
    "#     def stack_layers(self):\n",
    "#         l1_output, w1 = self.conv_block(x, 32, dropout_rate, 'conv1')\n",
    "#         # print(\"l1 output shape: \", l1_output.shape)\n",
    "#         l2_output, w2 = conv_block(l1_output, 64, dropout_rate, 'conv2')\n",
    "#         # print(\"l2 output shape: \", l2_output.shape)\n",
    "#         l3_output, w3 = conv_block(l2_output, 128, dropout_rate, 'conv3')\n",
    "#         # print(\"l3 output shape: \", l3_output.shape)\n",
    "\n",
    "#         # fc layers\n",
    "#         flatten = tf.reshape(l3_output, (x.shape[0], -1))\n",
    "#         # print(\"flatten output shape: \", flatten.shape)\n",
    "#         l4_output, w4 = fc_layer(flatten, 4096)\n",
    "#         # print(\"l4 output shape: \", l4_output.shape)\n",
    "#         l5_output, w5 = fc_layer(l4_output, 4096)\n",
    "#         # print(\"l5 output shape: \", l5_output.shape)\n",
    "#         l6_output, w6 = last_layer(l5_output, 10)\n",
    "\n",
    "#     def conv_block(self, x, filter_nums, dropout_rate, name, weights=None):\n",
    "#     if weights:\n",
    "#         print(weights)\n",
    "#         filters = weights\n",
    "#     else:\n",
    "#         filters = tf.Variable(tf.random.normal([3, 3, 1, filter_nums]))\n",
    "#     conv_output = tf.nn.conv2d(x, filters, (1,1,1,1), 'SAME', name=name)\n",
    "#     relu_output = tf.nn.relu(conv_output)\n",
    "#     pool_output = tf.nn.max_pool2d(\n",
    "#         relu_output, ksize=(1,2,2,1), strides=(1,2,2,1), padding='SAME')\n",
    "#     block_output = tf.nn.dropout(pool_output, rate=dropout_rate)\n",
    "#     return block_output, filters\n",
    "\n",
    "# def fc_layer(self, x, units, weights=None):\n",
    "#     if weights:\n",
    "#         w, b = weights\n",
    "#     else:\n",
    "#         w = tf.Variable(tf.random.normal((x.shape[-1], units)))\n",
    "#         b = tf.Variable(tf.random.normal((units, )))\n",
    "#     fc_output = tf.nn.relu(tf.matmul(x, w)+b)\n",
    "#     return fc_output, [w, b]\n",
    "\n",
    "# def last_layer(self, x, classes, weights=None):\n",
    "#     if weights:\n",
    "#         w, b = weights\n",
    "#     else:\n",
    "#         w = tf.Variable(tf.random.normal((x.shape[-1], classes)))\n",
    "#         b = tf.Variable(tf.random.normal((classes, )))\n",
    "#     logits = tf.matmul(x, w)+b\n",
    "#     return logits, [w, b]#tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SimpleCNN(tf.keras.Model):\n",
    "#     def __init__(self, dropout_rate):\n",
    "#         super(SimpleCNN, self).__init__()\n",
    "#         self.dropout_rate = dropout_rate\n",
    "\n",
    "#         # layers\n",
    "#         self.conv1 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='SAME')\n",
    "#         self.pool1 = tf.keras.layers.MaxPool2D((2, 2), padding='SAME')\n",
    "#         self.dropout1 = tf.keras.layers.Dropout(rate=self.dropout_rate)\n",
    "\n",
    "#         self.conv2 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='SAME')\n",
    "#         self.conv3 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='SAME')\n",
    "\n",
    "#         self.flatten = tf.keras.layers.Flatten()\n",
    "#         self.d1 = tf.keras.layers.Dense(4096, activation='relu')\n",
    "#         self.d2 = tf.keras.layers.Dense(10, activation='softmax')\n",
    "        \n",
    "#     def call(self, x):\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.pool1(x)\n",
    "#         x = self.dropout1(x)\n",
    "\n",
    "#         x = self.conv2(x)\n",
    "#         x = self.pool1(x)\n",
    "#         x = self.dropout1(x)\n",
    "\n",
    "#         x = self.conv3(x)\n",
    "#         x = self.pool1(x)\n",
    "#         x = self.dropout1(x)\n",
    "\n",
    "#         x = self.flatten(x)\n",
    "#         x = self.d1(x)\n",
    "#         x = self.d1(x)\n",
    "\n",
    "#         x = self.d2(x)\n",
    "#         return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_rate = 0.01\n",
    "# SimpleCNN(dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(x, filter_nums, dropout_rate, name, weights=None):\n",
    "    if isinstance(weights, np.ndarray):\n",
    "        filters = weights\n",
    "    else:\n",
    "        filters = tf.Variable(tf.random.normal([3, 3, 1, filter_nums]))\n",
    "    conv_output = tf.nn.conv2d(x, filters, (1,1,1,1), 'SAME', name=name)\n",
    "    relu_output = tf.nn.relu(conv_output)\n",
    "    pool_output = tf.nn.max_pool2d(\n",
    "        relu_output, ksize=(1,2,2,1), strides=(1,2,2,1), padding='SAME')\n",
    "    block_output = tf.nn.dropout(pool_output, rate=dropout_rate)\n",
    "    return block_output, filters\n",
    "\n",
    "def fc_layer(x, units, weights=None):\n",
    "    # if weights:\n",
    "        # w, b = weights\n",
    "    # else:\n",
    "    if not isinstance(weights, np.ndarray):\n",
    "        weights = tf.Variable(tf.random.normal((x.shape[-1], units)))\n",
    "        # b = tf.Variable(tf.random.normal((units, )))\n",
    "    fc_output = tf.nn.relu(tf.matmul(x, weights))#+b)\n",
    "    return fc_output, weights#[w, b]\n",
    "\n",
    "def last_layer(x, classes, weights=None):\n",
    "    # if weights:\n",
    "    #     w, b = weights\n",
    "    # else:\n",
    "    if not isinstance(weights, np.ndarray):\n",
    "        weights = tf.Variable(tf.random.normal((x.shape[-1], classes)))\n",
    "        # b = tf.Variable(tf.random.normal((classes, )))\n",
    "    logits = tf.matmul(x, weights)#+b\n",
    "    return logits, weights#[w, b]#tf.nn.softmax(logits)\n",
    "    \n",
    "\n",
    "def model(x, dropout_rate, weights=None):\n",
    "    if not weights:\n",
    "        # convolution blocks\n",
    "        l1_output, w1 = conv_block(x, 32, dropout_rate, 'conv1')\n",
    "        # print(\"l1 output shape: \", l1_output.shape)\n",
    "        l2_output, w2 = conv_block(l1_output, 64, dropout_rate, 'conv2')\n",
    "        # print(\"l2 output shape: \", l2_output.shape)\n",
    "        l3_output, w3 = conv_block(l2_output, 128, dropout_rate, 'conv3')\n",
    "        # print(\"l3 output shape: \", l3_output.shape)\n",
    "\n",
    "        # fc layers\n",
    "        flatten = tf.reshape(l3_output, (x.shape[0], -1))\n",
    "        # print(\"flatten output shape: \", flatten.shape)\n",
    "        l4_output, w4 = fc_layer(flatten, 4096)\n",
    "        # print(\"l4 output shape: \", l4_output.shape)\n",
    "        l5_output, w5 = fc_layer(l4_output, 4096)\n",
    "        # print(\"l5 output shape: \", l5_output.shape)\n",
    "        l6_output, w6 = last_layer(l5_output, 10)\n",
    "    else:\n",
    "        # convolution blocks\n",
    "        l1_output, w1 = conv_block(x, 32, dropout_rate, 'conv1', weights=weights[0])\n",
    "        # print(\"l1 output shape: \", l1_output.shape)\n",
    "        l2_output, w2 = conv_block(l1_output, 64, dropout_rate, 'conv2', weights=weights[1])\n",
    "        # print(\"l2 output shape: \", l2_output.shape)\n",
    "        l3_output, w3 = conv_block(l2_output, 128, dropout_rate, 'conv3', weights=weights[2])\n",
    "        # print(\"l3 output shape: \", l3_output.shape)\n",
    "\n",
    "        # fc layers\n",
    "        flatten = tf.reshape(l3_output, (x.shape[0], -1))\n",
    "        # print(\"flatten output shape: \", flatten.shape)\n",
    "        l4_output, w4 = fc_layer(flatten, 4096, weights=weights[3])\n",
    "        # print(\"l4 output shape: \", l4_output.shape)\n",
    "        l5_output, w5 = fc_layer(l4_output, 4096, weights=weights[4])\n",
    "        # print(\"l5 output shape: \", l5_output.shape)\n",
    "        l6_output, w6 = last_layer(l5_output, 10, weights=weights[5])\n",
    "\n",
    "    weights = [w1, w2, w3, w4, w5, w6]\n",
    "    return l6_output, weights\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [\n",
    "    tf.Variable(tf.random.normal([3, 3, 1, 32])),\n",
    "    tf.Variable(tf.random.normal([3, 3, 32, 64])),\n",
    "    tf.Variable(tf.random.normal([3, 3, 64, 128])),\n",
    "    tf.Variable(tf.random.normal((2048, 4096))),\n",
    "    tf.Variable(tf.random.normal((4096, 4096))),\n",
    "    tf.Variable(tf.random.normal((16777216, 10))),\n",
    "]\n",
    "    \n",
    "\n",
    "def model(x, dropout_rate, weights):\n",
    "\n",
    "    # convolution blocks\n",
    "    l1_output, w1 = conv_block(x, 32, dropout_rate, 'conv1', weights=weights[0])\n",
    "    print(\"l1 output shape: \", l1_output.shape)\n",
    "    l2_output, w2 = conv_block(l1_output, 64, dropout_rate, 'conv2', weights=weights[1])\n",
    "    print(\"l2 output shape: \", l2_output.shape)\n",
    "    l3_output, w3 = conv_block(l2_output, 128, dropout_rate, 'conv3', weights=weights[2])\n",
    "    print(\"l3 output shape: \", l3_output.shape)\n",
    "\n",
    "    # fc layers\n",
    "    flatten = tf.reshape(l3_output, (x.shape[0], -1))\n",
    "    print(\"flatten output shape: \", flatten.shape)\n",
    "    l4_output, w4 = fc_layer(flatten, 4096, weights=weights[3])\n",
    "    print(\"l4 output shape: \", l4_output.shape)\n",
    "    l5_output, w5 = fc_layer(l4_output, 4096, weights=weights[4])\n",
    "    print(\"l5 output shape: \", l5_output.shape)\n",
    "    l6_output, w6 = last_layer(l5_output, 10, weights=weights[5])\n",
    "    print(\"l6 output shape: \", l6_output.shape)\n",
    "\n",
    "    return l6_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l1 output shape:  (10000, 14, 14, 32)\n",
      "l2 output shape:  (10000, 7, 7, 64)\n",
      "l3 output shape:  (10000, 4, 4, 128)\n",
      "flatten output shape:  (10000, 2048)\n",
      "l4 output shape:  (10000, 4096)\n",
      "l5 output shape:  (10000, 4096)\n",
      "l6 output shape:  (10000, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10000, 10), dtype=float32, numpy=\n",
       "array([[-6.5424128e+07, -8.4711440e+07, -3.4729464e+07, ...,\n",
       "         4.3908692e+07,  7.9222400e+07, -5.7026248e+07],\n",
       "       [-1.2718055e+08,  9.2449800e+07,  3.8009336e+07, ...,\n",
       "        -6.3827200e+05,  3.7152492e+07, -2.1108564e+07],\n",
       "       [-7.5003160e+07, -7.6753328e+07, -1.3477851e+08, ...,\n",
       "         1.0408584e+07,  7.1463592e+07,  3.8940800e+05],\n",
       "       ...,\n",
       "       [-1.8872368e+07, -1.7453936e+07,  8.3614520e+07, ...,\n",
       "         3.5351208e+07, -3.7263020e+06, -5.2631384e+07],\n",
       "       [-9.9962224e+07, -5.0924164e+07,  3.4496696e+07, ...,\n",
       "        -9.3001280e+06,  3.1400262e+07,  8.2252336e+07],\n",
       "       [-3.6061036e+07, -3.6497040e+06,  3.9922460e+07, ...,\n",
       "        -2.8455808e+07,  2.8780164e+07,  1.2107393e+08]], dtype=float32)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(test_images, 0.01, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output, weights = model(test_images, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "sgd = tf.keras.optimizers.SGD(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mymodel = SimpleCNN(dropout_rate)\n",
    "# mymodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l1 output shape:  (100, 14, 14, 32)\n",
      "l2 output shape:  (100, 7, 7, 64)\n",
      "l3 output shape:  (100, 4, 4, 128)\n",
      "flatten output shape:  (100, 2048)\n",
      "l4 output shape:  (100, 4096)\n",
      "l5 output shape:  (100, 4096)\n",
      "l6 output shape:  (100, 10)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No gradients provided for any variable: (['Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0'],). Provided `grads_and_vars` is ((None, <tf.Variable 'Variable:0' shape=(3, 3, 1, 32) dtype=float32, numpy=\narray([[[[-4.46374357e-01, -4.57058966e-01, -2.86359459e-01,\n           4.46623683e-01,  2.99263954e-01, -4.78893638e-01,\n          -2.83502012e-01, -7.18055785e-01, -2.98658347e+00,\n          -1.08817637e-01,  7.48511493e-01, -1.14748311e+00,\n          -1.96982607e-01,  4.65005577e-01, -1.67358494e+00,\n          -2.26929235e+00,  3.15762490e-01, -9.49713409e-01,\n          -4.81016427e-01,  1.20072436e+00, -4.20764208e-01,\n           1.11314619e+00,  1.33051443e+00, -1.67879358e-01,\n           4.28146094e-01,  6.10830724e-01, -8.67114604e-01,\n           5.98382294e-01,  1.69573331e+00, -8.14088166e-01,\n          -1.97542533e-01,  2.17293239e+00]],\n\n        [[-1.02044475e+00, -9.89765823e-01, -1.14343905e+00,\n          -7.44643569e-01,  3.39721352e-01,  7.52491117e-01,\n          -7.82482445e-01,  1.10035908e+00,  9.75798905e-01,\n          -5.99294901e-01, -6.09506369e-01,  9.75454152e-02,\n          -5.37510542e-03, -7.30174959e-01, -1.25461566e+00,\n          -1.22158892e-01, -6.98209345e-01,  3.99319410e-01,\n          -9.11409974e-01,  9.19651151e-01, -3.33198532e-02,\n           2.65116274e-01, -1.76061058e+00, -1.43641040e-01,\n           3.29883963e-01,  4.83391821e-01, -1.13998912e-01,\n           1.23605454e+00,  4.76044379e-02, -1.00405836e+00,\n           9.95380044e-01,  2.06563517e-01]],\n\n        [[-2.51540281e-02,  1.02151617e-01, -1.95787883e+00,\n           4.03535873e-01, -5.41930199e-01,  3.30893040e-01,\n           2.94419497e-01,  1.55750656e+00, -1.46861577e+00,\n          -7.77053595e-01, -2.32788146e-01,  2.58847564e-01,\n          -3.54052454e-01,  1.29159653e+00,  4.41014975e-01,\n          -3.18432003e-01, -1.85193026e+00, -1.44543471e-02,\n          -4.49677557e-01,  9.49026108e-01, -1.34294522e+00,\n           6.47508979e-01, -1.02256715e+00, -4.59131837e-01,\n           4.60736573e-01,  9.26914275e-01, -6.84469640e-01,\n           2.93858618e-01,  2.99307793e-01, -7.03691006e-01,\n          -2.51844525e+00, -5.26520312e-01]]],\n\n\n       [[[ 1.30579281e+00,  1.01817441e+00,  1.51665330e-01,\n           3.63568753e-01,  2.63712794e-01, -1.34926274e-01,\n          -7.69452751e-01,  9.63407993e-01, -1.20501792e+00,\n           1.98265266e+00,  6.74444586e-02, -5.56971226e-03,\n           5.72734177e-01, -3.42248797e-01, -1.44733489e-03,\n           5.65562733e-02,  1.05346000e+00,  1.18501341e+00,\n          -4.95853692e-01, -1.91169095e+00, -2.37743661e-01,\n          -5.70372045e-01,  8.77448916e-01, -1.13035375e-02,\n           1.43206573e+00, -1.18012047e+00, -5.14664352e-02,\n           4.35669115e-03,  8.48125398e-01,  9.42740619e-01,\n          -2.14213967e-01, -1.07443237e+00]],\n\n        [[ 8.65586400e-01, -7.92875700e-03,  5.56516349e-01,\n           3.80583376e-01, -1.00019741e+00,  4.96763110e-01,\n          -6.35385692e-01, -3.12143350e+00,  1.69625616e+00,\n          -9.60971117e-01,  1.10172343e+00,  7.95498431e-01,\n           1.26037705e+00,  1.62342370e+00, -1.20309687e+00,\n          -9.87407088e-01, -1.67532635e+00,  2.83772647e-01,\n           9.79496419e-01, -2.01935887e+00,  1.76873398e+00,\n          -6.49094999e-01,  5.63079603e-02,  3.86566937e-01,\n          -2.02477646e+00, -1.72992969e+00,  1.53683650e+00,\n          -9.22025681e-01, -9.18089807e-01, -1.24929525e-01,\n          -2.76320398e-01, -4.59710121e-01]],\n\n        [[ 9.93881524e-01, -1.63387501e+00,  1.30018368e-01,\n          -9.41696391e-02,  1.66353416e+00,  8.70338559e-01,\n           1.14783287e+00, -1.09798086e+00, -2.01788306e+00,\n          -1.01912963e+00,  4.94600236e-01,  9.34244096e-01,\n           1.23413754e+00,  1.91494927e-01,  3.81858259e-01,\n           8.34523976e-01, -1.46336555e-01, -9.46782291e-01,\n          -1.06047320e+00,  1.32545626e+00, -8.46556306e-01,\n           1.43611506e-01, -9.34186995e-01, -2.15576127e-01,\n          -1.29622221e+00, -8.35605443e-01, -1.21255648e+00,\n           7.61729956e-01, -2.84239560e-01,  2.79416773e-03,\n           1.77972400e+00,  1.36968553e+00]]],\n\n\n       [[[-5.22562027e-01, -8.50449279e-02, -1.29093081e-01,\n          -7.16582388e-02,  5.08056700e-01,  1.63603306e-01,\n           8.82339925e-02, -1.10961616e+00, -5.02679646e-02,\n           4.54222895e-02,  1.26529896e+00,  5.29581904e-01,\n          -1.82404006e+00,  3.06689262e-01,  1.66156162e-02,\n           8.92909110e-01, -5.03337801e-01, -1.73193380e-01,\n           8.27788949e-01,  1.43493795e+00, -1.03174186e+00,\n           3.66357505e-01, -9.47789550e-01, -1.16388428e+00,\n          -1.94683981e+00, -2.01042438e+00, -6.63417101e-01,\n          -3.08352172e-01,  1.08572555e+00, -8.74366611e-02,\n           1.45379186e+00, -1.24435794e+00]],\n\n        [[-1.28534818e+00,  7.27278600e-03, -1.29042566e+00,\n           1.75715685e-01,  1.03171265e+00, -2.06258106e+00,\n          -2.13632211e-01,  1.18459165e+00, -1.71014142e+00,\n          -1.32192171e+00, -9.83447552e-01,  1.30596113e+00,\n           1.35420537e+00, -1.93993449e+00, -4.70129997e-01,\n           3.49866562e-02,  1.23114765e+00, -1.24937975e+00,\n           2.15291262e-01, -8.15374196e-01, -2.26351404e+00,\n          -1.70012498e+00,  7.88780212e-01,  3.44678640e-01,\n          -2.35504180e-01,  1.16268623e+00, -6.52290761e-01,\n          -9.88883302e-02, -1.22947268e-01, -7.50595212e-01,\n          -6.90015078e-01, -6.57445669e-01]],\n\n        [[-1.00115567e-01, -1.11972308e+00, -2.12159252e+00,\n          -1.05498123e+00, -2.98450381e-01,  6.65704608e-01,\n           4.79665041e-01, -2.78942317e-01, -6.27496779e-01,\n          -2.87386984e-01,  4.87277925e-01,  6.77610397e-01,\n           2.15942669e+00, -4.70326573e-01, -7.14919567e-01,\n           3.80876750e-01,  1.28937769e+00, -7.40702748e-02,\n           6.40107095e-01, -6.92902580e-02, -6.08073175e-01,\n           3.16828638e-01,  1.12426889e+00,  1.34023976e+00,\n          -2.43315816e-01,  1.22526836e+00,  7.36046553e-01,\n          -1.32707071e+00,  1.94109511e+00,  1.75711715e+00,\n          -5.64565599e-01,  8.18723321e-01]]]], dtype=float32)>), (None, <tf.Variable 'Variable:0' shape=(3, 3, 32, 64) dtype=float32, numpy=\narray([[[[ 0.5742976 , -0.3257716 ,  0.7157085 , ...,  0.12876545,\n          -0.61766225,  1.8888229 ],\n         [ 0.6007729 , -0.05132572,  0.4089197 , ...,  1.1092671 ,\n          -1.2821591 , -0.36297435],\n         [ 2.0510113 ,  0.7980814 , -0.75792843, ..., -0.2983111 ,\n           0.8761811 , -0.96343   ],\n         ...,\n         [ 0.64902264,  0.03209786,  0.11504378, ..., -0.46831474,\n           0.25417608, -0.50426036],\n         [ 0.47385392,  0.4333501 , -0.3898146 , ..., -1.8415954 ,\n          -0.37201685,  0.15316094],\n         [-1.3679469 , -1.239432  ,  0.21301425, ...,  0.01362448,\n          -0.41699177, -1.1189593 ]],\n\n        [[ 1.7353776 , -1.2741671 , -1.1247419 , ..., -0.13769649,\n          -0.8903486 , -0.65525174],\n         [ 0.36565787, -1.2835016 , -1.0384338 , ...,  1.1261103 ,\n          -0.14076254,  0.92622703],\n         [-0.0687462 , -0.39210078,  0.75454247, ...,  0.15024137,\n          -0.37856036,  0.9627028 ],\n         ...,\n         [-0.9754773 , -0.30977505,  1.539721  , ...,  0.5437999 ,\n           0.24939829,  0.77600795],\n         [ 1.11347   , -0.60389227,  0.05671273, ..., -0.31950575,\n          -1.9732152 , -1.0541453 ],\n         [ 0.22271143, -1.1628379 , -0.1366515 , ...,  0.2930414 ,\n           0.06321444,  0.46832606]],\n\n        [[ 0.6339528 ,  0.38769284,  0.11830927, ...,  0.36312464,\n          -0.17941399,  0.39467886],\n         [-0.2580037 , -0.5361726 ,  2.225796  , ..., -0.34727022,\n          -0.83798784,  0.1329164 ],\n         [-0.62044775,  0.8765151 ,  1.127291  , ...,  0.589618  ,\n          -0.09301887,  0.34357613],\n         ...,\n         [ 1.6755428 ,  0.665379  ,  0.7915034 , ..., -0.44498414,\n          -1.10197   , -0.8125848 ],\n         [ 0.92872083,  0.8373899 ,  1.811266  , ..., -0.07491866,\n          -0.09396855,  0.1796412 ],\n         [-1.4548267 , -0.76733273, -0.87561053, ..., -0.14144047,\n           0.2310802 , -1.0990016 ]]],\n\n\n       [[[-0.36694565, -1.1648756 ,  1.5598497 , ..., -1.1607583 ,\n           0.17922416,  0.7108589 ],\n         [-0.41345602,  0.1141213 , -2.2770967 , ...,  0.9759471 ,\n           0.63797027, -0.39245012],\n         [ 1.2491534 , -0.29781523, -1.4906039 , ..., -0.24308354,\n           1.6247276 ,  0.13844222],\n         ...,\n         [ 1.210118  , -0.2826725 ,  0.78332776, ..., -0.13901146,\n          -1.0900372 ,  1.1998197 ],\n         [ 1.5286746 , -0.6667917 , -1.1467243 , ..., -1.350748  ,\n          -0.9885575 , -1.0058706 ],\n         [-1.4509991 ,  0.47376344,  1.8842841 , ..., -0.70836174,\n           0.2931081 ,  1.0224787 ]],\n\n        [[ 0.3529996 , -0.45694572,  0.793417  , ...,  0.93791175,\n          -0.30559725,  1.006192  ],\n         [ 0.01969974, -1.5089504 ,  0.17154527, ...,  0.270148  ,\n           1.596498  , -0.7781684 ],\n         [-0.51022625,  0.09091796, -0.8970824 , ..., -0.84768856,\n          -0.1721594 ,  0.8561636 ],\n         ...,\n         [ 0.25344187, -0.4832385 , -1.5359877 , ..., -0.990042  ,\n           2.1256526 ,  1.1286489 ],\n         [ 0.6427445 ,  0.8804484 ,  0.68329555, ..., -1.070301  ,\n          -2.0749612 ,  0.45743257],\n         [-0.2866886 , -0.00933866,  1.0408363 , ..., -0.67542934,\n          -1.0616208 , -0.6111436 ]],\n\n        [[ 0.76001215, -0.12232382, -0.73278064, ..., -0.87359154,\n          -1.0181208 ,  0.35851464],\n         [-1.0406752 , -0.762757  ,  0.76008636, ...,  0.55717087,\n          -0.4876055 , -1.0570252 ],\n         [ 0.22162235, -0.08595155, -0.35711655, ...,  1.1159077 ,\n          -1.0265319 , -1.2356895 ],\n         ...,\n         [ 0.06366328,  1.2381185 ,  0.20489357, ..., -1.2009985 ,\n          -0.68337595,  0.9754597 ],\n         [ 1.0680906 ,  0.2458029 ,  1.1229001 , ...,  1.257323  ,\n          -0.4700108 , -0.53492105],\n         [ 0.94464594,  0.9868698 ,  0.24857695, ..., -0.08555923,\n          -1.1844997 ,  0.2244427 ]]],\n\n\n       [[[ 0.49327335,  0.8838914 ,  0.20902556, ..., -0.5130002 ,\n           1.0726483 , -2.1286476 ],\n         [ 0.9584264 ,  0.22268394,  0.15022106, ...,  1.7925487 ,\n           0.62899345,  0.5457151 ],\n         [ 0.64099747,  0.9890575 ,  0.39511666, ..., -0.75951177,\n           1.0040483 , -1.2515258 ],\n         ...,\n         [-0.03706541, -1.7926925 ,  0.81277335, ...,  2.5105093 ,\n          -0.09364387,  0.85225147],\n         [ 0.23037775,  0.7537916 , -1.3613101 , ...,  0.49335292,\n           1.5534178 , -1.5978302 ],\n         [-1.0774537 , -0.27763304,  1.4898823 , ...,  0.70726836,\n           1.9401126 ,  0.5748569 ]],\n\n        [[ 0.06223935,  0.75427794,  0.5428247 , ...,  0.6690734 ,\n          -1.0003419 ,  1.1346037 ],\n         [-0.6877528 , -0.6466139 ,  0.21461111, ...,  1.61751   ,\n          -0.32101643, -0.57705283],\n         [-0.19281943,  0.8240895 , -0.22318046, ...,  0.43618807,\n          -0.5712092 , -1.2893207 ],\n         ...,\n         [-0.8698364 , -0.22855668,  0.27486598, ...,  1.3646259 ,\n           0.55083984,  0.14811045],\n         [ 0.43255237,  0.3534132 , -0.91342056, ...,  0.61958945,\n          -0.50397545,  2.0892224 ],\n         [ 1.0744714 ,  0.4713145 , -0.28398645, ...,  0.45357665,\n           1.3028349 ,  0.15850328]],\n\n        [[ 1.770637  ,  1.1444994 ,  1.1160434 , ..., -0.15974842,\n          -0.7728421 , -1.2234498 ],\n         [ 1.3155771 ,  1.2190208 , -0.6990985 , ...,  2.4306624 ,\n           0.30076036,  1.3573644 ],\n         [ 0.86885726, -1.4552228 , -0.2563026 , ...,  1.650021  ,\n           1.0811288 , -0.01785163],\n         ...,\n         [ 1.1125338 , -0.9642631 ,  0.2967542 , ...,  1.0643098 ,\n           0.01457456,  0.96201915],\n         [ 1.2027147 ,  0.03213558,  0.17613402, ..., -0.4251969 ,\n           0.5548503 , -1.993067  ],\n         [ 1.4833893 , -0.05377081,  0.40393656, ..., -1.6360776 ,\n          -0.5950743 , -1.388898  ]]]], dtype=float32)>), (None, <tf.Variable 'Variable:0' shape=(3, 3, 64, 128) dtype=float32, numpy=\narray([[[[-7.29683280e-01, -8.95023704e-01,  1.09726846e+00, ...,\n          -4.04530764e-01, -3.81192237e-01,  5.83117723e-01],\n         [ 3.30283821e-01,  3.57561797e-01, -1.26730049e+00, ...,\n           9.16636109e-01, -1.06285250e+00, -1.37013388e+00],\n         [ 5.37770152e-01,  1.90656081e-01, -2.45018080e-01, ...,\n          -2.09335899e+00, -4.15503949e-01, -1.04058993e+00],\n         ...,\n         [-8.02668219e-04, -2.16971152e-02, -8.27942565e-02, ...,\n          -3.10868979e-01, -1.64794695e+00, -7.98368216e-01],\n         [ 1.39739239e+00, -2.21068546e-01, -4.39953059e-01, ...,\n          -1.74895132e+00,  5.89586675e-01, -5.21441936e-01],\n         [ 2.50138491e-01, -7.50475451e-02, -2.00792432e-01, ...,\n          -2.16332674e+00,  6.88037038e-01, -1.94988355e-01]],\n\n        [[-7.80901238e-02,  1.98596537e+00, -1.57067561e+00, ...,\n           5.36437750e-01, -6.61062598e-01,  7.21580148e-01],\n         [-1.31662977e+00, -1.22744632e+00, -7.20474362e-01, ...,\n          -1.31433845e+00,  4.48815882e-01, -2.31448084e-01],\n         [ 4.82255101e-01,  8.87447298e-01, -1.48477796e-02, ...,\n           1.14897974e-01,  9.36942637e-01,  9.11545694e-01],\n         ...,\n         [-2.50488281e-01, -2.13215575e-01,  2.23570421e-01, ...,\n           2.95684189e-01,  7.32581794e-01,  8.56560647e-01],\n         [ 1.47378480e+00, -1.41686463e+00, -5.49516857e-01, ...,\n          -1.37964630e+00, -7.95753658e-01,  7.93656290e-01],\n         [ 1.77946591e+00,  3.18699867e-01, -1.34606159e+00, ...,\n          -1.93043157e-01, -2.71116104e-02, -9.48424414e-02]],\n\n        [[-5.80799460e-01,  1.40116107e+00,  1.49066401e+00, ...,\n          -9.27434444e-01,  1.57346475e+00,  4.75933641e-01],\n         [-6.64080203e-01, -9.75729287e-01,  2.06581712e-01, ...,\n          -9.50545371e-02,  5.38978755e-01, -1.06536560e-01],\n         [-1.38342649e-01,  1.63680708e+00,  1.80012673e-01, ...,\n          -1.10754967e+00,  1.48520362e+00,  3.61322850e-01],\n         ...,\n         [ 4.50562924e-01, -3.32437724e-01, -1.06149185e+00, ...,\n          -2.27270290e-01,  2.81678677e-01,  7.08908498e-01],\n         [ 6.65559888e-01,  9.22284901e-01, -1.04319096e+00, ...,\n          -3.22948784e-01,  1.01376498e+00, -1.42535663e+00],\n         [-5.21644533e-01, -2.59584546e-01, -3.18011865e-02, ...,\n           8.60061705e-01,  1.16984653e+00, -1.53289747e+00]]],\n\n\n       [[[-3.40325415e-01, -1.57512292e-01, -1.84662193e-01, ...,\n           1.26969879e-02,  9.20426369e-01, -2.61748523e-01],\n         [ 2.97662640e+00,  2.99985647e-01, -8.48378897e-01, ...,\n           4.05259192e-01, -7.40129948e-01, -1.22085428e+00],\n         [-1.90381134e+00, -2.22941613e+00,  1.98236138e-01, ...,\n          -5.82535148e-01,  1.58373237e+00, -4.55058753e-01],\n         ...,\n         [ 8.66636336e-01, -7.56187201e-01, -7.48025537e-01, ...,\n           4.06533331e-01, -2.64865279e-01, -1.33331448e-01],\n         [-8.70618701e-01, -9.89741683e-01,  2.16027188e+00, ...,\n           6.05837643e-01, -6.92633092e-01, -1.37433636e+00],\n         [-1.14836109e+00,  6.69782341e-01, -5.70257306e-01, ...,\n          -1.01807606e+00,  1.42891741e+00,  7.45598376e-01]],\n\n        [[-7.06545293e-01, -5.87592304e-01, -2.60058101e-02, ...,\n          -7.79668093e-01,  6.92550242e-01,  7.77922213e-01],\n         [-3.72175008e-01,  2.25194961e-01,  1.44588757e+00, ...,\n           1.76243639e+00, -1.39819622e-01, -3.35311621e-01],\n         [-2.90456057e-01, -7.47987688e-01,  6.36043191e-01, ...,\n           1.42178309e+00,  5.50854027e-01, -5.71317196e-01],\n         ...,\n         [-1.57668638e+00, -6.05517566e-01, -3.88553262e-01, ...,\n          -1.85436964e+00, -2.48358154e+00,  1.15375292e+00],\n         [-1.08524501e+00, -5.95606416e-02, -5.12914121e-01, ...,\n           5.69578350e-01,  1.28104329e-01,  3.33560735e-01],\n         [ 5.54360211e-01, -5.83832562e-01, -5.26507139e-01, ...,\n          -1.35436440e+00,  2.07998300e+00,  4.85952169e-01]],\n\n        [[ 1.04911971e+00, -9.08865392e-01, -4.14614469e-01, ...,\n           3.03357959e-01, -1.55097187e+00, -6.36150658e-01],\n         [ 1.51064944e+00,  4.06687647e-01, -4.89547133e-01, ...,\n           1.43332541e+00, -4.72061545e-01,  1.31633556e+00],\n         [ 2.78873015e-02, -1.79080141e+00,  1.43610001e+00, ...,\n           4.18388546e-02,  1.88090253e+00,  4.05120939e-01],\n         ...,\n         [-3.73851895e-01, -1.12736678e+00, -5.27697504e-01, ...,\n          -9.73719358e-01,  1.40084195e+00, -9.41471338e-01],\n         [ 2.32826248e-01,  1.83230412e+00,  7.33403265e-01, ...,\n          -1.59891903e+00, -8.18348527e-01,  1.33358479e-01],\n         [ 7.50488043e-01,  1.11224465e-01, -3.77994955e-01, ...,\n           6.66510701e-01,  2.07301393e-01,  9.14226711e-01]]],\n\n\n       [[[ 1.34025562e+00,  8.73906255e-01,  6.61551654e-01, ...,\n          -5.14451742e-01, -7.71323383e-01, -1.90158293e-01],\n         [ 6.67252183e-01, -2.18700409e+00,  2.21714467e-01, ...,\n          -5.20140350e-01, -1.85554817e-01, -8.65980089e-01],\n         [ 3.02952260e-01,  1.51958156e+00, -2.16464472e+00, ...,\n           8.63169014e-01, -7.00808644e-01,  8.84770751e-01],\n         ...,\n         [-2.06297541e+00, -1.28928626e+00, -3.39811444e-02, ...,\n           7.20473886e-01,  1.38344848e+00, -9.92812335e-01],\n         [ 7.14931428e-01, -1.22550404e+00, -5.62436998e-01, ...,\n          -1.55283213e-01,  1.54804420e+00, -4.84626800e-01],\n         [-1.50236118e+00,  6.37772143e-01, -1.40540302e+00, ...,\n          -3.79150122e-01,  1.69119036e+00, -1.01191688e+00]],\n\n        [[ 5.67518473e-01, -7.75559604e-01,  6.46784246e-01, ...,\n          -7.39046633e-01,  6.79668844e-01, -6.16860151e-01],\n         [ 2.68910319e-01, -6.38873100e-01, -1.21581554e+00, ...,\n          -7.93320119e-01,  4.86129701e-01, -3.63647491e-01],\n         [-1.26991045e+00, -1.31193435e+00, -2.50399541e-02, ...,\n           7.23347783e-01, -9.15158272e-01, -8.29089046e-01],\n         ...,\n         [ 2.34938931e+00,  4.30969968e-02, -1.65134752e+00, ...,\n          -5.89898586e-01, -3.12494844e-01, -3.75780165e-01],\n         [ 7.54496932e-01,  2.54743576e-01, -1.16674326e-01, ...,\n           1.05851173e-01, -2.58211285e-01, -8.32748055e-01],\n         [-1.79597533e+00, -2.28258586e+00,  2.29049593e-01, ...,\n           2.49734763e-02, -3.40122312e-01, -8.21850538e-01]],\n\n        [[-1.19683909e+00,  1.10589005e-01, -1.58813536e+00, ...,\n           3.75664085e-02, -1.71809506e+00,  1.52564481e-01],\n         [ 1.62822568e+00, -1.61814904e+00,  1.99683762e+00, ...,\n           1.93806672e+00, -8.09747934e-01,  5.67946695e-02],\n         [ 1.16530216e+00,  6.53673649e-01, -1.64411902e-01, ...,\n           3.17775398e-01, -5.82751870e-01, -3.50965738e-01],\n         ...,\n         [-6.07018054e-01, -7.35100806e-01, -1.29851922e-01, ...,\n           4.55302298e-01, -3.32525790e-01,  6.55355006e-02],\n         [-7.77397335e-01, -3.90509993e-01,  1.76041210e+00, ...,\n          -1.58300447e+00, -1.22222221e+00, -7.62192309e-01],\n         [-5.59000731e-01,  1.98529571e-01, -8.34661901e-01, ...,\n          -9.10509467e-01, -1.46658108e-01,  2.27484420e-01]]]],\n      dtype=float32)>), (None, <tf.Variable 'Variable:0' shape=(2048, 4096) dtype=float32, numpy=\narray([[-0.6489288 ,  1.5488482 , -0.38752925, ..., -0.24290921,\n         0.8652805 ,  0.80998415],\n       [ 0.17648779, -0.88412064,  0.4322222 , ..., -0.65048176,\n        -0.7345997 , -0.03624457],\n       [-1.5834057 , -1.5994263 , -0.6956852 , ...,  2.29792   ,\n        -0.085099  ,  0.06805535],\n       ...,\n       [-0.18510137, -0.05362412,  0.80253553, ...,  0.38436428,\n        -1.7765448 ,  1.0120195 ],\n       [-1.0982237 ,  0.23367853, -1.5787557 , ...,  0.57286924,\n         2.078075  ,  0.4749554 ],\n       [ 2.0558589 ,  0.22972709, -0.48774865, ...,  0.26835167,\n        -2.0388482 , -1.3748724 ]], dtype=float32)>), (None, <tf.Variable 'Variable:0' shape=(4096, 4096) dtype=float32, numpy=\narray([[-0.26768598,  0.8802896 ,  0.526819  , ..., -1.9921635 ,\n        -0.04647652,  1.0839005 ],\n       [-0.7534231 ,  0.84776443, -0.4140452 , ..., -0.4164535 ,\n        -0.03978001, -1.4535445 ],\n       [-0.21479389, -2.009014  , -0.19381271, ...,  0.3966699 ,\n         1.2068441 , -1.6350113 ],\n       ...,\n       [-0.3606555 , -0.02573317, -0.85532594, ..., -0.34130508,\n         0.69565344, -1.527207  ],\n       [-1.8382951 ,  0.49391708, -2.0732875 , ..., -2.8922045 ,\n        -1.3050829 ,  1.651409  ],\n       [-2.1008158 ,  0.96197605,  1.2161593 , ..., -3.5359821 ,\n        -1.2985113 , -0.42262682]], dtype=float32)>), (None, <tf.Variable 'Variable:0' shape=(16777216, 10) dtype=float32, numpy=\narray([[ 0.406626  ,  0.46041396,  1.0252984 , ...,  1.7960837 ,\n         0.9462326 ,  1.2893928 ],\n       [ 0.21971773, -0.35172385,  0.38129607, ..., -1.3419645 ,\n         0.37491724,  0.17371483],\n       [ 1.3082541 ,  0.4430669 ,  1.930099  , ...,  0.06991876,\n         0.31245357,  0.38494864],\n       ...,\n       [ 0.25364342,  1.5957029 ,  0.3679995 , ..., -0.46600085,\n        -0.1058808 , -0.3876578 ],\n       [ 0.16804138,  1.2013139 , -0.09088121, ..., -0.301738  ,\n         0.84256583,  0.270196  ],\n       [-0.8946129 ,  1.4676023 ,  0.5614097 , ...,  0.8179129 ,\n        -0.48790362,  1.4808813 ]], dtype=float32)>)).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[39m# else:\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     \u001b[39m#     pred, weights = model(batch_images, 0.01)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     loss \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39msoftmax_cross_entropy_with_logits(batch_labels, pred)\n\u001b[0;32m---> 13\u001b[0m sgd\u001b[39m.\u001b[39;49mminimize(loss, var_list\u001b[39m=\u001b[39;49mweights, tape\u001b[39m=\u001b[39;49mtape)\n\u001b[1;32m     14\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mepoch: \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m, batches: \u001b[39m\u001b[39m{\u001b[39;00mb\u001b[39m}\u001b[39;00m\u001b[39m, loss: \u001b[39m\u001b[39m{\u001b[39;00mtf\u001b[39m.\u001b[39mreduce_sum(loss)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/recommender/lib/python3.10/site-packages/keras/optimizers/optimizer_experimental/optimizer.py:527\u001b[0m, in \u001b[0;36m_BaseOptimizer.minimize\u001b[0;34m(self, loss, var_list, tape)\u001b[0m\n\u001b[1;32m    506\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Minimize `loss` by updating `var_list`.\u001b[39;00m\n\u001b[1;32m    507\u001b[0m \n\u001b[1;32m    508\u001b[0m \u001b[39mThis method simply computes gradient using `tf.GradientTape` and calls\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    524\u001b[0m \u001b[39m  None\u001b[39;00m\n\u001b[1;32m    525\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    526\u001b[0m grads_and_vars \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_gradients(loss, var_list, tape)\n\u001b[0;32m--> 527\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_gradients(grads_and_vars)\n",
      "File \u001b[0;32m~/miniconda3/envs/recommender/lib/python3.10/site-packages/keras/optimizers/optimizer_experimental/optimizer.py:1139\u001b[0m, in \u001b[0;36mOptimizer.apply_gradients\u001b[0;34m(self, grads_and_vars, name, skip_gradients_aggregation, **kwargs)\u001b[0m\n\u001b[1;32m   1135\u001b[0m experimental_aggregate_gradients \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\n\u001b[1;32m   1136\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mexperimental_aggregate_gradients\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m )\n\u001b[1;32m   1138\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m skip_gradients_aggregation \u001b[39mand\u001b[39;00m experimental_aggregate_gradients:\n\u001b[0;32m-> 1139\u001b[0m     grads_and_vars \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maggregate_gradients(grads_and_vars)\n\u001b[1;32m   1140\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mapply_gradients(grads_and_vars, name\u001b[39m=\u001b[39mname)\n",
      "File \u001b[0;32m~/miniconda3/envs/recommender/lib/python3.10/site-packages/keras/optimizers/optimizer_experimental/optimizer.py:1105\u001b[0m, in \u001b[0;36mOptimizer.aggregate_gradients\u001b[0;34m(self, grads_and_vars)\u001b[0m\n\u001b[1;32m   1093\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39maggregate_gradients\u001b[39m(\u001b[39mself\u001b[39m, grads_and_vars):\n\u001b[1;32m   1094\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Aggregate gradients on all devices.\u001b[39;00m\n\u001b[1;32m   1095\u001b[0m \n\u001b[1;32m   1096\u001b[0m \u001b[39m    By default we will perform reduce_sum of gradients across devices. Users\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1103\u001b[0m \u001b[39m      List of (gradient, variable) pairs.\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1105\u001b[0m     \u001b[39mreturn\u001b[39;00m optimizer_utils\u001b[39m.\u001b[39;49mall_reduce_sum_gradients(grads_and_vars)\n",
      "File \u001b[0;32m~/miniconda3/envs/recommender/lib/python3.10/site-packages/keras/optimizers/optimizer_v2/utils.py:33\u001b[0m, in \u001b[0;36mall_reduce_sum_gradients\u001b[0;34m(grads_and_vars)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Returns all-reduced gradients aggregated via summation.\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \n\u001b[1;32m     26\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39m  List of (gradient, variable) pairs where gradients have been all-reduced.\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     32\u001b[0m grads_and_vars \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(grads_and_vars)\n\u001b[0;32m---> 33\u001b[0m filtered_grads_and_vars \u001b[39m=\u001b[39m filter_empty_gradients(grads_and_vars)\n\u001b[1;32m     34\u001b[0m \u001b[39mif\u001b[39;00m filtered_grads_and_vars:\n\u001b[1;32m     35\u001b[0m     \u001b[39mif\u001b[39;00m tf\u001b[39m.\u001b[39m__internal__\u001b[39m.\u001b[39mdistribute\u001b[39m.\u001b[39mstrategy_supports_no_merge_call():\n",
      "File \u001b[0;32m~/miniconda3/envs/recommender/lib/python3.10/site-packages/keras/optimizers/optimizer_v2/utils.py:77\u001b[0m, in \u001b[0;36mfilter_empty_gradients\u001b[0;34m(grads_and_vars)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m filtered:\n\u001b[1;32m     76\u001b[0m     variable \u001b[39m=\u001b[39m ([v\u001b[39m.\u001b[39mname \u001b[39mfor\u001b[39;00m _, v \u001b[39min\u001b[39;00m grads_and_vars],)\n\u001b[0;32m---> 77\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     78\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNo gradients provided for any variable: \u001b[39m\u001b[39m{\u001b[39;00mvariable\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     79\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mProvided `grads_and_vars` is \u001b[39m\u001b[39m{\u001b[39;00mgrads_and_vars\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     80\u001b[0m     )\n\u001b[1;32m     81\u001b[0m \u001b[39mif\u001b[39;00m vars_with_empty_grads:\n\u001b[1;32m     82\u001b[0m     logging\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m     83\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mGradients do not exist for variables \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m when minimizing the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     84\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mloss. If you\u001b[39m\u001b[39m'\u001b[39m\u001b[39mre using `model.compile()`, did you forget to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     85\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mprovide a `loss` argument?\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     86\u001b[0m         ([v\u001b[39m.\u001b[39mname \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m vars_with_empty_grads]),\n\u001b[1;32m     87\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: No gradients provided for any variable: (['Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0'],). Provided `grads_and_vars` is ((None, <tf.Variable 'Variable:0' shape=(3, 3, 1, 32) dtype=float32, numpy=\narray([[[[-4.46374357e-01, -4.57058966e-01, -2.86359459e-01,\n           4.46623683e-01,  2.99263954e-01, -4.78893638e-01,\n          -2.83502012e-01, -7.18055785e-01, -2.98658347e+00,\n          -1.08817637e-01,  7.48511493e-01, -1.14748311e+00,\n          -1.96982607e-01,  4.65005577e-01, -1.67358494e+00,\n          -2.26929235e+00,  3.15762490e-01, -9.49713409e-01,\n          -4.81016427e-01,  1.20072436e+00, -4.20764208e-01,\n           1.11314619e+00,  1.33051443e+00, -1.67879358e-01,\n           4.28146094e-01,  6.10830724e-01, -8.67114604e-01,\n           5.98382294e-01,  1.69573331e+00, -8.14088166e-01,\n          -1.97542533e-01,  2.17293239e+00]],\n\n        [[-1.02044475e+00, -9.89765823e-01, -1.14343905e+00,\n          -7.44643569e-01,  3.39721352e-01,  7.52491117e-01,\n          -7.82482445e-01,  1.10035908e+00,  9.75798905e-01,\n          -5.99294901e-01, -6.09506369e-01,  9.75454152e-02,\n          -5.37510542e-03, -7.30174959e-01, -1.25461566e+00,\n          -1.22158892e-01, -6.98209345e-01,  3.99319410e-01,\n          -9.11409974e-01,  9.19651151e-01, -3.33198532e-02,\n           2.65116274e-01, -1.76061058e+00, -1.43641040e-01,\n           3.29883963e-01,  4.83391821e-01, -1.13998912e-01,\n           1.23605454e+00,  4.76044379e-02, -1.00405836e+00,\n           9.95380044e-01,  2.06563517e-01]],\n\n        [[-2.51540281e-02,  1.02151617e-01, -1.95787883e+00,\n           4.03535873e-01, -5.41930199e-01,  3.30893040e-01,\n           2.94419497e-01,  1.55750656e+00, -1.46861577e+00,\n          -7.77053595e-01, -2.32788146e-01,  2.58847564e-01,\n          -3.54052454e-01,  1.29159653e+00,  4.41014975e-01,\n          -3.18432003e-01, -1.85193026e+00, -1.44543471e-02,\n          -4.49677557e-01,  9.49026108e-01, -1.34294522e+00,\n           6.47508979e-01, -1.02256715e+00, -4.59131837e-01,\n           4.60736573e-01,  9.26914275e-01, -6.84469640e-01,\n           2.93858618e-01,  2.99307793e-01, -7.03691006e-01,\n          -2.51844525e+00, -5.26520312e-01]]],\n\n\n       [[[ 1.30579281e+00,  1.01817441e+00,  1.51665330e-01,\n           3.63568753e-01,  2.63712794e-01, -1.34926274e-01,\n          -7.69452751e-01,  9.63407993e-01, -1.20501792e+00,\n           1.98265266e+00,  6.74444586e-02, -5.56971226e-03,\n           5.72734177e-01, -3.42248797e-01, -1.44733489e-03,\n           5.65562733e-02,  1.05346000e+00,  1.18501341e+00,\n          -4.95853692e-01, -1.91169095e+00, -2.37743661e-01,\n          -5.70372045e-01,  8.77448916e-01, -1.13035375e-02,\n           1.43206573e+00, -1.18012047e+00, -5.14664352e-02,\n           4.35669115e-03,  8.48125398e-01,  9.42740619e-01,\n          -2.14213967e-01, -1.07443237e+00]],\n\n        [[ 8.65586400e-01, -7.92875700e-03,  5.56516349e-01,\n           3.80583376e-01, -1.00019741e+00,  4.96763110e-01,\n          -6.35385692e-01, -3.12143350e+00,  1.69625616e+00,\n          -9.60971117e-01,  1.10172343e+00,  7.95498431e-01,\n           1.26037705e+00,  1.62342370e+00, -1.20309687e+00,\n          -9.87407088e-01, -1.67532635e+00,  2.83772647e-01,\n           9.79496419e-01, -2.01935887e+00,  1.76873398e+00,\n          -6.49094999e-01,  5.63079603e-02,  3.86566937e-01,\n          -2.02477646e+00, -1.72992969e+00,  1.53683650e+00,\n          -9.22025681e-01, -9.18089807e-01, -1.24929525e-01,\n          -2.76320398e-01, -4.59710121e-01]],\n\n        [[ 9.93881524e-01, -1.63387501e+00,  1.30018368e-01,\n          -9.41696391e-02,  1.66353416e+00,  8.70338559e-01,\n           1.14783287e+00, -1.09798086e+00, -2.01788306e+00,\n          -1.01912963e+00,  4.94600236e-01,  9.34244096e-01,\n           1.23413754e+00,  1.91494927e-01,  3.81858259e-01,\n           8.34523976e-01, -1.46336555e-01, -9.46782291e-01,\n          -1.06047320e+00,  1.32545626e+00, -8.46556306e-01,\n           1.43611506e-01, -9.34186995e-01, -2.15576127e-01,\n          -1.29622221e+00, -8.35605443e-01, -1.21255648e+00,\n           7.61729956e-01, -2.84239560e-01,  2.79416773e-03,\n           1.77972400e+00,  1.36968553e+00]]],\n\n\n       [[[-5.22562027e-01, -8.50449279e-02, -1.29093081e-01,\n          -7.16582388e-02,  5.08056700e-01,  1.63603306e-01,\n           8.82339925e-02, -1.10961616e+00, -5.02679646e-02,\n           4.54222895e-02,  1.26529896e+00,  5.29581904e-01,\n          -1.82404006e+00,  3.06689262e-01,  1.66156162e-02,\n           8.92909110e-01, -5.03337801e-01, -1.73193380e-01,\n           8.27788949e-01,  1.43493795e+00, -1.03174186e+00,\n           3.66357505e-01, -9.47789550e-01, -1.16388428e+00,\n          -1.94683981e+00, -2.01042438e+00, -6.63417101e-01,\n          -3.08352172e-01,  1.08572555e+00, -8.74366611e-02,\n           1.45379186e+00, -1.24435794e+00]],\n\n        [[-1.28534818e+00,  7.27278600e-03, -1.29042566e+00,\n           1.75715685e-01,  1.03171265e+00, -2.06258106e+00,\n          -2.13632211e-01,  1.18459165e+00, -1.71014142e+00,\n          -1.32192171e+00, -9.83447552e-01,  1.30596113e+00,\n           1.35420537e+00, -1.93993449e+00, -4.70129997e-01,\n           3.49866562e-02,  1.23114765e+00, -1.24937975e+00,\n           2.15291262e-01, -8.15374196e-01, -2.26351404e+00,\n          -1.70012498e+00,  7.88780212e-01,  3.44678640e-01,\n          -2.35504180e-01,  1.16268623e+00, -6.52290761e-01,\n          -9.88883302e-02, -1.22947268e-01, -7.50595212e-01,\n          -6.90015078e-01, -6.57445669e-01]],\n\n        [[-1.00115567e-01, -1.11972308e+00, -2.12159252e+00,\n          -1.05498123e+00, -2.98450381e-01,  6.65704608e-01,\n           4.79665041e-01, -2.78942317e-01, -6.27496779e-01,\n          -2.87386984e-01,  4.87277925e-01,  6.77610397e-01,\n           2.15942669e+00, -4.70326573e-01, -7.14919567e-01,\n           3.80876750e-01,  1.28937769e+00, -7.40702748e-02,\n           6.40107095e-01, -6.92902580e-02, -6.08073175e-01,\n           3.16828638e-01,  1.12426889e+00,  1.34023976e+00,\n          -2.43315816e-01,  1.22526836e+00,  7.36046553e-01,\n          -1.32707071e+00,  1.94109511e+00,  1.75711715e+00,\n          -5.64565599e-01,  8.18723321e-01]]]], dtype=float32)>), (None, <tf.Variable 'Variable:0' shape=(3, 3, 32, 64) dtype=float32, numpy=\narray([[[[ 0.5742976 , -0.3257716 ,  0.7157085 , ...,  0.12876545,\n          -0.61766225,  1.8888229 ],\n         [ 0.6007729 , -0.05132572,  0.4089197 , ...,  1.1092671 ,\n          -1.2821591 , -0.36297435],\n         [ 2.0510113 ,  0.7980814 , -0.75792843, ..., -0.2983111 ,\n           0.8761811 , -0.96343   ],\n         ...,\n         [ 0.64902264,  0.03209786,  0.11504378, ..., -0.46831474,\n           0.25417608, -0.50426036],\n         [ 0.47385392,  0.4333501 , -0.3898146 , ..., -1.8415954 ,\n          -0.37201685,  0.15316094],\n         [-1.3679469 , -1.239432  ,  0.21301425, ...,  0.01362448,\n          -0.41699177, -1.1189593 ]],\n\n        [[ 1.7353776 , -1.2741671 , -1.1247419 , ..., -0.13769649,\n          -0.8903486 , -0.65525174],\n         [ 0.36565787, -1.2835016 , -1.0384338 , ...,  1.1261103 ,\n          -0.14076254,  0.92622703],\n         [-0.0687462 , -0.39210078,  0.75454247, ...,  0.15024137,\n          -0.37856036,  0.9627028 ],\n         ...,\n         [-0.9754773 , -0.30977505,  1.539721  , ...,  0.5437999 ,\n           0.24939829,  0.77600795],\n         [ 1.11347   , -0.60389227,  0.05671273, ..., -0.31950575,\n          -1.9732152 , -1.0541453 ],\n         [ 0.22271143, -1.1628379 , -0.1366515 , ...,  0.2930414 ,\n           0.06321444,  0.46832606]],\n\n        [[ 0.6339528 ,  0.38769284,  0.11830927, ...,  0.36312464,\n          -0.17941399,  0.39467886],\n         [-0.2580037 , -0.5361726 ,  2.225796  , ..., -0.34727022,\n          -0.83798784,  0.1329164 ],\n         [-0.62044775,  0.8765151 ,  1.127291  , ...,  0.589618  ,\n          -0.09301887,  0.34357613],\n         ...,\n         [ 1.6755428 ,  0.665379  ,  0.7915034 , ..., -0.44498414,\n          -1.10197   , -0.8125848 ],\n         [ 0.92872083,  0.8373899 ,  1.811266  , ..., -0.07491866,\n          -0.09396855,  0.1796412 ],\n         [-1.4548267 , -0.76733273, -0.87561053, ..., -0.14144047,\n           0.2310802 , -1.0990016 ]]],\n\n\n       [[[-0.36694565, -1.1648756 ,  1.5598497 , ..., -1.1607583 ,\n           0.17922416,  0.7108589 ],\n         [-0.41345602,  0.1141213 , -2.2770967 , ...,  0.9759471 ,\n           0.63797027, -0.39245012],\n         [ 1.2491534 , -0.29781523, -1.4906039 , ..., -0.24308354,\n           1.6247276 ,  0.13844222],\n         ...,\n         [ 1.210118  , -0.2826725 ,  0.78332776, ..., -0.13901146,\n          -1.0900372 ,  1.1998197 ],\n         [ 1.5286746 , -0.6667917 , -1.1467243 , ..., -1.350748  ,\n          -0.9885575 , -1.0058706 ],\n         [-1.4509991 ,  0.47376344,  1.8842841 , ..., -0.70836174,\n           0.2931081 ,  1.0224787 ]],\n\n        [[ 0.3529996 , -0.45694572,  0.793417  , ...,  0.93791175,\n          -0.30559725,  1.006192  ],\n         [ 0.01969974, -1.5089504 ,  0.17154527, ...,  0.270148  ,\n           1.596498  , -0.7781684 ],\n         [-0.51022625,  0.09091796, -0.8970824 , ..., -0.84768856,\n          -0.1721594 ,  0.8561636 ],\n         ...,\n         [ 0.25344187, -0.4832385 , -1.5359877 , ..., -0.990042  ,\n           2.1256526 ,  1.1286489 ],\n         [ 0.6427445 ,  0.8804484 ,  0.68329555, ..., -1.070301  ,\n          -2.0749612 ,  0.45743257],\n         [-0.2866886 , -0.00933866,  1.0408363 , ..., -0.67542934,\n          -1.0616208 , -0.6111436 ]],\n\n        [[ 0.76001215, -0.12232382, -0.73278064, ..., -0.87359154,\n          -1.0181208 ,  0.35851464],\n         [-1.0406752 , -0.762757  ,  0.76008636, ...,  0.55717087,\n          -0.4876055 , -1.0570252 ],\n         [ 0.22162235, -0.08595155, -0.35711655, ...,  1.1159077 ,\n          -1.0265319 , -1.2356895 ],\n         ...,\n         [ 0.06366328,  1.2381185 ,  0.20489357, ..., -1.2009985 ,\n          -0.68337595,  0.9754597 ],\n         [ 1.0680906 ,  0.2458029 ,  1.1229001 , ...,  1.257323  ,\n          -0.4700108 , -0.53492105],\n         [ 0.94464594,  0.9868698 ,  0.24857695, ..., -0.08555923,\n          -1.1844997 ,  0.2244427 ]]],\n\n\n       [[[ 0.49327335,  0.8838914 ,  0.20902556, ..., -0.5130002 ,\n           1.0726483 , -2.1286476 ],\n         [ 0.9584264 ,  0.22268394,  0.15022106, ...,  1.7925487 ,\n           0.62899345,  0.5457151 ],\n         [ 0.64099747,  0.9890575 ,  0.39511666, ..., -0.75951177,\n           1.0040483 , -1.2515258 ],\n         ...,\n         [-0.03706541, -1.7926925 ,  0.81277335, ...,  2.5105093 ,\n          -0.09364387,  0.85225147],\n         [ 0.23037775,  0.7537916 , -1.3613101 , ...,  0.49335292,\n           1.5534178 , -1.5978302 ],\n         [-1.0774537 , -0.27763304,  1.4898823 , ...,  0.70726836,\n           1.9401126 ,  0.5748569 ]],\n\n        [[ 0.06223935,  0.75427794,  0.5428247 , ...,  0.6690734 ,\n          -1.0003419 ,  1.1346037 ],\n         [-0.6877528 , -0.6466139 ,  0.21461111, ...,  1.61751   ,\n          -0.32101643, -0.57705283],\n         [-0.19281943,  0.8240895 , -0.22318046, ...,  0.43618807,\n          -0.5712092 , -1.2893207 ],\n         ...,\n         [-0.8698364 , -0.22855668,  0.27486598, ...,  1.3646259 ,\n           0.55083984,  0.14811045],\n         [ 0.43255237,  0.3534132 , -0.91342056, ...,  0.61958945,\n          -0.50397545,  2.0892224 ],\n         [ 1.0744714 ,  0.4713145 , -0.28398645, ...,  0.45357665,\n           1.3028349 ,  0.15850328]],\n\n        [[ 1.770637  ,  1.1444994 ,  1.1160434 , ..., -0.15974842,\n          -0.7728421 , -1.2234498 ],\n         [ 1.3155771 ,  1.2190208 , -0.6990985 , ...,  2.4306624 ,\n           0.30076036,  1.3573644 ],\n         [ 0.86885726, -1.4552228 , -0.2563026 , ...,  1.650021  ,\n           1.0811288 , -0.01785163],\n         ...,\n         [ 1.1125338 , -0.9642631 ,  0.2967542 , ...,  1.0643098 ,\n           0.01457456,  0.96201915],\n         [ 1.2027147 ,  0.03213558,  0.17613402, ..., -0.4251969 ,\n           0.5548503 , -1.993067  ],\n         [ 1.4833893 , -0.05377081,  0.40393656, ..., -1.6360776 ,\n          -0.5950743 , -1.388898  ]]]], dtype=float32)>), (None, <tf.Variable 'Variable:0' shape=(3, 3, 64, 128) dtype=float32, numpy=\narray([[[[-7.29683280e-01, -8.95023704e-01,  1.09726846e+00, ...,\n          -4.04530764e-01, -3.81192237e-01,  5.83117723e-01],\n         [ 3.30283821e-01,  3.57561797e-01, -1.26730049e+00, ...,\n           9.16636109e-01, -1.06285250e+00, -1.37013388e+00],\n         [ 5.37770152e-01,  1.90656081e-01, -2.45018080e-01, ...,\n          -2.09335899e+00, -4.15503949e-01, -1.04058993e+00],\n         ...,\n         [-8.02668219e-04, -2.16971152e-02, -8.27942565e-02, ...,\n          -3.10868979e-01, -1.64794695e+00, -7.98368216e-01],\n         [ 1.39739239e+00, -2.21068546e-01, -4.39953059e-01, ...,\n          -1.74895132e+00,  5.89586675e-01, -5.21441936e-01],\n         [ 2.50138491e-01, -7.50475451e-02, -2.00792432e-01, ...,\n          -2.16332674e+00,  6.88037038e-01, -1.94988355e-01]],\n\n        [[-7.80901238e-02,  1.98596537e+00, -1.57067561e+00, ...,\n           5.36437750e-01, -6.61062598e-01,  7.21580148e-01],\n         [-1.31662977e+00, -1.22744632e+00, -7.20474362e-01, ...,\n          -1.31433845e+00,  4.48815882e-01, -2.31448084e-01],\n         [ 4.82255101e-01,  8.87447298e-01, -1.48477796e-02, ...,\n           1.14897974e-01,  9.36942637e-01,  9.11545694e-01],\n         ...,\n         [-2.50488281e-01, -2.13215575e-01,  2.23570421e-01, ...,\n           2.95684189e-01,  7.32581794e-01,  8.56560647e-01],\n         [ 1.47378480e+00, -1.41686463e+00, -5.49516857e-01, ...,\n          -1.37964630e+00, -7.95753658e-01,  7.93656290e-01],\n         [ 1.77946591e+00,  3.18699867e-01, -1.34606159e+00, ...,\n          -1.93043157e-01, -2.71116104e-02, -9.48424414e-02]],\n\n        [[-5.80799460e-01,  1.40116107e+00,  1.49066401e+00, ...,\n          -9.27434444e-01,  1.57346475e+00,  4.75933641e-01],\n         [-6.64080203e-01, -9.75729287e-01,  2.06581712e-01, ...,\n          -9.50545371e-02,  5.38978755e-01, -1.06536560e-01],\n         [-1.38342649e-01,  1.63680708e+00,  1.80012673e-01, ...,\n          -1.10754967e+00,  1.48520362e+00,  3.61322850e-01],\n         ...,\n         [ 4.50562924e-01, -3.32437724e-01, -1.06149185e+00, ...,\n          -2.27270290e-01,  2.81678677e-01,  7.08908498e-01],\n         [ 6.65559888e-01,  9.22284901e-01, -1.04319096e+00, ...,\n          -3.22948784e-01,  1.01376498e+00, -1.42535663e+00],\n         [-5.21644533e-01, -2.59584546e-01, -3.18011865e-02, ...,\n           8.60061705e-01,  1.16984653e+00, -1.53289747e+00]]],\n\n\n       [[[-3.40325415e-01, -1.57512292e-01, -1.84662193e-01, ...,\n           1.26969879e-02,  9.20426369e-01, -2.61748523e-01],\n         [ 2.97662640e+00,  2.99985647e-01, -8.48378897e-01, ...,\n           4.05259192e-01, -7.40129948e-01, -1.22085428e+00],\n         [-1.90381134e+00, -2.22941613e+00,  1.98236138e-01, ...,\n          -5.82535148e-01,  1.58373237e+00, -4.55058753e-01],\n         ...,\n         [ 8.66636336e-01, -7.56187201e-01, -7.48025537e-01, ...,\n           4.06533331e-01, -2.64865279e-01, -1.33331448e-01],\n         [-8.70618701e-01, -9.89741683e-01,  2.16027188e+00, ...,\n           6.05837643e-01, -6.92633092e-01, -1.37433636e+00],\n         [-1.14836109e+00,  6.69782341e-01, -5.70257306e-01, ...,\n          -1.01807606e+00,  1.42891741e+00,  7.45598376e-01]],\n\n        [[-7.06545293e-01, -5.87592304e-01, -2.60058101e-02, ...,\n          -7.79668093e-01,  6.92550242e-01,  7.77922213e-01],\n         [-3.72175008e-01,  2.25194961e-01,  1.44588757e+00, ...,\n           1.76243639e+00, -1.39819622e-01, -3.35311621e-01],\n         [-2.90456057e-01, -7.47987688e-01,  6.36043191e-01, ...,\n           1.42178309e+00,  5.50854027e-01, -5.71317196e-01],\n         ...,\n         [-1.57668638e+00, -6.05517566e-01, -3.88553262e-01, ...,\n          -1.85436964e+00, -2.48358154e+00,  1.15375292e+00],\n         [-1.08524501e+00, -5.95606416e-02, -5.12914121e-01, ...,\n           5.69578350e-01,  1.28104329e-01,  3.33560735e-01],\n         [ 5.54360211e-01, -5.83832562e-01, -5.26507139e-01, ...,\n          -1.35436440e+00,  2.07998300e+00,  4.85952169e-01]],\n\n        [[ 1.04911971e+00, -9.08865392e-01, -4.14614469e-01, ...,\n           3.03357959e-01, -1.55097187e+00, -6.36150658e-01],\n         [ 1.51064944e+00,  4.06687647e-01, -4.89547133e-01, ...,\n           1.43332541e+00, -4.72061545e-01,  1.31633556e+00],\n         [ 2.78873015e-02, -1.79080141e+00,  1.43610001e+00, ...,\n           4.18388546e-02,  1.88090253e+00,  4.05120939e-01],\n         ...,\n         [-3.73851895e-01, -1.12736678e+00, -5.27697504e-01, ...,\n          -9.73719358e-01,  1.40084195e+00, -9.41471338e-01],\n         [ 2.32826248e-01,  1.83230412e+00,  7.33403265e-01, ...,\n          -1.59891903e+00, -8.18348527e-01,  1.33358479e-01],\n         [ 7.50488043e-01,  1.11224465e-01, -3.77994955e-01, ...,\n           6.66510701e-01,  2.07301393e-01,  9.14226711e-01]]],\n\n\n       [[[ 1.34025562e+00,  8.73906255e-01,  6.61551654e-01, ...,\n          -5.14451742e-01, -7.71323383e-01, -1.90158293e-01],\n         [ 6.67252183e-01, -2.18700409e+00,  2.21714467e-01, ...,\n          -5.20140350e-01, -1.85554817e-01, -8.65980089e-01],\n         [ 3.02952260e-01,  1.51958156e+00, -2.16464472e+00, ...,\n           8.63169014e-01, -7.00808644e-01,  8.84770751e-01],\n         ...,\n         [-2.06297541e+00, -1.28928626e+00, -3.39811444e-02, ...,\n           7.20473886e-01,  1.38344848e+00, -9.92812335e-01],\n         [ 7.14931428e-01, -1.22550404e+00, -5.62436998e-01, ...,\n          -1.55283213e-01,  1.54804420e+00, -4.84626800e-01],\n         [-1.50236118e+00,  6.37772143e-01, -1.40540302e+00, ...,\n          -3.79150122e-01,  1.69119036e+00, -1.01191688e+00]],\n\n        [[ 5.67518473e-01, -7.75559604e-01,  6.46784246e-01, ...,\n          -7.39046633e-01,  6.79668844e-01, -6.16860151e-01],\n         [ 2.68910319e-01, -6.38873100e-01, -1.21581554e+00, ...,\n          -7.93320119e-01,  4.86129701e-01, -3.63647491e-01],\n         [-1.26991045e+00, -1.31193435e+00, -2.50399541e-02, ...,\n           7.23347783e-01, -9.15158272e-01, -8.29089046e-01],\n         ...,\n         [ 2.34938931e+00,  4.30969968e-02, -1.65134752e+00, ...,\n          -5.89898586e-01, -3.12494844e-01, -3.75780165e-01],\n         [ 7.54496932e-01,  2.54743576e-01, -1.16674326e-01, ...,\n           1.05851173e-01, -2.58211285e-01, -8.32748055e-01],\n         [-1.79597533e+00, -2.28258586e+00,  2.29049593e-01, ...,\n           2.49734763e-02, -3.40122312e-01, -8.21850538e-01]],\n\n        [[-1.19683909e+00,  1.10589005e-01, -1.58813536e+00, ...,\n           3.75664085e-02, -1.71809506e+00,  1.52564481e-01],\n         [ 1.62822568e+00, -1.61814904e+00,  1.99683762e+00, ...,\n           1.93806672e+00, -8.09747934e-01,  5.67946695e-02],\n         [ 1.16530216e+00,  6.53673649e-01, -1.64411902e-01, ...,\n           3.17775398e-01, -5.82751870e-01, -3.50965738e-01],\n         ...,\n         [-6.07018054e-01, -7.35100806e-01, -1.29851922e-01, ...,\n           4.55302298e-01, -3.32525790e-01,  6.55355006e-02],\n         [-7.77397335e-01, -3.90509993e-01,  1.76041210e+00, ...,\n          -1.58300447e+00, -1.22222221e+00, -7.62192309e-01],\n         [-5.59000731e-01,  1.98529571e-01, -8.34661901e-01, ...,\n          -9.10509467e-01, -1.46658108e-01,  2.27484420e-01]]]],\n      dtype=float32)>), (None, <tf.Variable 'Variable:0' shape=(2048, 4096) dtype=float32, numpy=\narray([[-0.6489288 ,  1.5488482 , -0.38752925, ..., -0.24290921,\n         0.8652805 ,  0.80998415],\n       [ 0.17648779, -0.88412064,  0.4322222 , ..., -0.65048176,\n        -0.7345997 , -0.03624457],\n       [-1.5834057 , -1.5994263 , -0.6956852 , ...,  2.29792   ,\n        -0.085099  ,  0.06805535],\n       ...,\n       [-0.18510137, -0.05362412,  0.80253553, ...,  0.38436428,\n        -1.7765448 ,  1.0120195 ],\n       [-1.0982237 ,  0.23367853, -1.5787557 , ...,  0.57286924,\n         2.078075  ,  0.4749554 ],\n       [ 2.0558589 ,  0.22972709, -0.48774865, ...,  0.26835167,\n        -2.0388482 , -1.3748724 ]], dtype=float32)>), (None, <tf.Variable 'Variable:0' shape=(4096, 4096) dtype=float32, numpy=\narray([[-0.26768598,  0.8802896 ,  0.526819  , ..., -1.9921635 ,\n        -0.04647652,  1.0839005 ],\n       [-0.7534231 ,  0.84776443, -0.4140452 , ..., -0.4164535 ,\n        -0.03978001, -1.4535445 ],\n       [-0.21479389, -2.009014  , -0.19381271, ...,  0.3966699 ,\n         1.2068441 , -1.6350113 ],\n       ...,\n       [-0.3606555 , -0.02573317, -0.85532594, ..., -0.34130508,\n         0.69565344, -1.527207  ],\n       [-1.8382951 ,  0.49391708, -2.0732875 , ..., -2.8922045 ,\n        -1.3050829 ,  1.651409  ],\n       [-2.1008158 ,  0.96197605,  1.2161593 , ..., -3.5359821 ,\n        -1.2985113 , -0.42262682]], dtype=float32)>), (None, <tf.Variable 'Variable:0' shape=(16777216, 10) dtype=float32, numpy=\narray([[ 0.406626  ,  0.46041396,  1.0252984 , ...,  1.7960837 ,\n         0.9462326 ,  1.2893928 ],\n       [ 0.21971773, -0.35172385,  0.38129607, ..., -1.3419645 ,\n         0.37491724,  0.17371483],\n       [ 1.3082541 ,  0.4430669 ,  1.930099  , ...,  0.06991876,\n         0.31245357,  0.38494864],\n       ...,\n       [ 0.25364342,  1.5957029 ,  0.3679995 , ..., -0.46600085,\n        -0.1058808 , -0.3876578 ],\n       [ 0.16804138,  1.2013139 , -0.09088121, ..., -0.301738  ,\n         0.84256583,  0.270196  ],\n       [-0.8946129 ,  1.4676023 ,  0.5614097 , ...,  0.8179129 ,\n        -0.48790362,  1.4808813 ]], dtype=float32)>))."
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "\n",
    "for i in range(epochs):\n",
    "\n",
    "    for b, (batch_images, batch_labels) in enumerate(zip(train_batch_images, train_batch_labels)):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # if len(weights):\n",
    "            #     pred, _ = model(batch_images, 0.01, weights)\n",
    "            pred = model(batch_images, 0.01, weights)\n",
    "            # else:\n",
    "            #     pred, weights = model(batch_images, 0.01)\n",
    "            loss = tf.nn.softmax_cross_entropy_with_logits(batch_labels, pred)\n",
    "        sgd.minimize(loss, var_list=weights, tape=tape)\n",
    "        print(f\"epoch: {i}, batches: {b}, loss: {tf.reduce_sum(loss)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
