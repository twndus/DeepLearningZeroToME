{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 09: XOR problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime as dt\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
    "y_data = np.array([[0], [1], [1], [0]], dtype=np.float32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 층이 하나일 때"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_w = tf.random.normal(shape=(2,1), dtype='float32')\n",
    "rand_b = tf.random.normal(shape=(1,), dtype='float32')\n",
    "\n",
    "w = tf.Variable(rand_w, dtype='float32')\n",
    "b = tf.Variable(rand_b, dtype='float32')\n",
    "\n",
    "hypothesis = lambda x, w, b: tf.nn.sigmoid(tf.matmul(x, w) + b)\n",
    "# loss = lambda hypothesis,y : tf.reduce_sum(tf.square(hypothesis - y))\n",
    "# define cross entropy loss\n",
    "loss = lambda prob, y: -y*tf.math.log(prob) - (1-y)*tf.math.log(1-prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_acc(true, pred):\n",
    "    acc = sum(tf.cast(tf.equal(true, pred>=.5), dtype='int32'))[0]/true.shape[0]\n",
    "    return acc.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0 , cost:  0.76937044 , accuracy:  0.5\n",
      "epoch:  1 , cost:  0.76716805 , accuracy:  0.5\n",
      "epoch:  2 , cost:  0.76502377 , accuracy:  0.5\n",
      "epoch:  3 , cost:  0.76293653 , accuracy:  0.5\n",
      "epoch:  4 , cost:  0.7609049 , accuracy:  0.5\n",
      "epoch:  5 , cost:  0.75892794 , accuracy:  0.5\n",
      "epoch:  6 , cost:  0.75700414 , accuracy:  0.5\n",
      "epoch:  7 , cost:  0.7551326 , accuracy:  0.5\n",
      "epoch:  8 , cost:  0.75331193 , accuracy:  0.5\n",
      "epoch:  9 , cost:  0.751541 , accuracy:  0.5\n",
      "epoch:  10 , cost:  0.7498187 , accuracy:  0.5\n",
      "epoch:  11 , cost:  0.7481438 , accuracy:  0.5\n",
      "epoch:  12 , cost:  0.7465153 , accuracy:  0.5\n",
      "epoch:  13 , cost:  0.74493206 , accuracy:  0.5\n",
      "epoch:  14 , cost:  0.7433929 , accuracy:  0.5\n",
      "epoch:  15 , cost:  0.74189687 , accuracy:  0.5\n",
      "epoch:  16 , cost:  0.74044275 , accuracy:  0.5\n",
      "epoch:  17 , cost:  0.73902977 , accuracy:  0.5\n",
      "epoch:  18 , cost:  0.73765665 , accuracy:  0.5\n",
      "epoch:  19 , cost:  0.7363225 , accuracy:  0.5\n",
      "epoch:  20 , cost:  0.73502636 , accuracy:  0.5\n",
      "epoch:  21 , cost:  0.7337672 , accuracy:  0.5\n",
      "epoch:  22 , cost:  0.7325442 , accuracy:  0.5\n",
      "epoch:  23 , cost:  0.73135626 , accuracy:  0.5\n",
      "epoch:  24 , cost:  0.7302026 , accuracy:  0.5\n",
      "epoch:  25 , cost:  0.7290823 , accuracy:  0.5\n",
      "epoch:  26 , cost:  0.7279944 , accuracy:  0.5\n",
      "epoch:  27 , cost:  0.72693807 , accuracy:  0.5\n",
      "epoch:  28 , cost:  0.7259125 , accuracy:  0.5\n",
      "epoch:  29 , cost:  0.72491693 , accuracy:  0.5\n",
      "epoch:  30 , cost:  0.7239505 , accuracy:  0.5\n",
      "epoch:  31 , cost:  0.72301245 , accuracy:  0.5\n",
      "epoch:  32 , cost:  0.7221019 , accuracy:  0.5\n",
      "epoch:  33 , cost:  0.7212182 , accuracy:  0.5\n",
      "epoch:  34 , cost:  0.7203605 , accuracy:  0.5\n",
      "epoch:  35 , cost:  0.7195284 , accuracy:  0.5\n",
      "epoch:  36 , cost:  0.71872085 , accuracy:  0.5\n",
      "epoch:  37 , cost:  0.71793723 , accuracy:  0.5\n",
      "epoch:  38 , cost:  0.71717703 , accuracy:  0.5\n",
      "epoch:  39 , cost:  0.7164395 , accuracy:  0.5\n",
      "epoch:  40 , cost:  0.71572393 , accuracy:  0.5\n",
      "epoch:  41 , cost:  0.7150299 , accuracy:  0.5\n",
      "epoch:  42 , cost:  0.7143566 , accuracy:  0.5\n",
      "epoch:  43 , cost:  0.7137035 , accuracy:  0.5\n",
      "epoch:  44 , cost:  0.71307003 , accuracy:  0.5\n",
      "epoch:  45 , cost:  0.71245563 , accuracy:  0.5\n",
      "epoch:  46 , cost:  0.7118598 , accuracy:  0.5\n",
      "epoch:  47 , cost:  0.71128196 , accuracy:  0.5\n",
      "epoch:  48 , cost:  0.7107216 , accuracy:  0.5\n",
      "epoch:  49 , cost:  0.71017826 , accuracy:  0.5\n",
      "epoch:  50 , cost:  0.70965135 , accuracy:  0.5\n",
      "epoch:  51 , cost:  0.7091405 , accuracy:  0.5\n",
      "epoch:  52 , cost:  0.7086451 , accuracy:  0.5\n",
      "epoch:  53 , cost:  0.7081648 , accuracy:  0.5\n",
      "epoch:  54 , cost:  0.7076992 , accuracy:  0.5\n",
      "epoch:  55 , cost:  0.70724773 , accuracy:  0.5\n",
      "epoch:  56 , cost:  0.7068101 , accuracy:  0.5\n",
      "epoch:  57 , cost:  0.7063858 , accuracy:  0.5\n",
      "epoch:  58 , cost:  0.7059746 , accuracy:  0.5\n",
      "epoch:  59 , cost:  0.70557594 , accuracy:  0.5\n",
      "epoch:  60 , cost:  0.70518947 , accuracy:  0.5\n",
      "epoch:  61 , cost:  0.7048149 , accuracy:  0.5\n",
      "epoch:  62 , cost:  0.7044518 , accuracy:  0.5\n",
      "epoch:  63 , cost:  0.70409995 , accuracy:  0.5\n",
      "epoch:  64 , cost:  0.70375884 , accuracy:  0.5\n",
      "epoch:  65 , cost:  0.7034284 , accuracy:  0.5\n",
      "epoch:  66 , cost:  0.70310795 , accuracy:  0.5\n",
      "epoch:  67 , cost:  0.70279753 , accuracy:  0.5\n",
      "epoch:  68 , cost:  0.7024966 , accuracy:  0.5\n",
      "epoch:  69 , cost:  0.702205 , accuracy:  0.5\n",
      "epoch:  70 , cost:  0.7019224 , accuracy:  0.5\n",
      "epoch:  71 , cost:  0.7016486 , accuracy:  0.5\n",
      "epoch:  72 , cost:  0.7013832 , accuracy:  0.5\n",
      "epoch:  73 , cost:  0.7011261 , accuracy:  0.5\n",
      "epoch:  74 , cost:  0.7008769 , accuracy:  0.5\n",
      "epoch:  75 , cost:  0.70063543 , accuracy:  0.5\n",
      "epoch:  76 , cost:  0.7004015 , accuracy:  0.5\n",
      "epoch:  77 , cost:  0.70017487 , accuracy:  0.5\n",
      "epoch:  78 , cost:  0.6999552 , accuracy:  0.5\n",
      "epoch:  79 , cost:  0.69974244 , accuracy:  0.5\n",
      "epoch:  80 , cost:  0.6995362 , accuracy:  0.5\n",
      "epoch:  81 , cost:  0.69933647 , accuracy:  0.5\n",
      "epoch:  82 , cost:  0.69914293 , accuracy:  0.5\n",
      "epoch:  83 , cost:  0.6989554 , accuracy:  0.5\n",
      "epoch:  84 , cost:  0.6987738 , accuracy:  0.5\n",
      "epoch:  85 , cost:  0.6985978 , accuracy:  0.5\n",
      "epoch:  86 , cost:  0.6984272 , accuracy:  0.5\n",
      "epoch:  87 , cost:  0.69826204 , accuracy:  0.5\n",
      "epoch:  88 , cost:  0.69810194 , accuracy:  0.5\n",
      "epoch:  89 , cost:  0.6979469 , accuracy:  0.5\n",
      "epoch:  90 , cost:  0.69779676 , accuracy:  0.5\n",
      "epoch:  91 , cost:  0.6976512 , accuracy:  0.5\n",
      "epoch:  92 , cost:  0.69751024 , accuracy:  0.5\n",
      "epoch:  93 , cost:  0.69737375 , accuracy:  0.5\n",
      "epoch:  94 , cost:  0.6972415 , accuracy:  0.5\n",
      "epoch:  95 , cost:  0.6971132 , accuracy:  0.5\n",
      "epoch:  96 , cost:  0.6969892 , accuracy:  0.5\n",
      "epoch:  97 , cost:  0.6968689 , accuracy:  0.5\n",
      "epoch:  98 , cost:  0.69675237 , accuracy:  0.5\n",
      "epoch:  99 , cost:  0.69663966 , accuracy:  0.5\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "learning_rate = 0.01\n",
    "\n",
    "for e in range(epochs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        prob = hypothesis(x_data, w, b)\n",
    "        cost = loss(prob, y_data)\n",
    "        dl_dw, dl_db = tape.gradient(cost, [w,b])\n",
    "        \n",
    "    w.assign_sub(learning_rate*dl_dw)\n",
    "    b.assign_sub(learning_rate*dl_db)\n",
    "\n",
    "    acc = compute_acc(y_data, prob)\n",
    "\n",
    "    print(\"epoch: \", e, \", cost: \", tf.reduce_mean(cost).numpy(), \", accuracy: \", acc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "거듭 학습해도, 정확도는 0.5에서 좋아지지 않음    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 여러 층을 쌓아보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rand_W1 = tf.random.normal(shape=(2,2), dtype='float32')\n",
    "# rand_b1 = tf.random.normal(shape=(2,), dtype='float32')\n",
    "\n",
    "# rand_W2 = tf.random.normal(shape=(2,1), dtype='float32')\n",
    "# rand_b2 = tf.random.normal(shape=(1,), dtype='float32')\n",
    "\n",
    "# # W1 = tf.Variable(((-8,-8), (-5,-5)), dtype='float32')\n",
    "# # b1 = tf.Variable((5, 8), dtype='float32')\n",
    "\n",
    "# # W2 = tf.Variable(((-5,), (-5,)), dtype='float32')\n",
    "# # b2 = tf.Variable((8), dtype='float32')\n",
    "# hypothesis = lambda x, w, b: tf.sigmoid(tf.matmul(x, w) + b)\n",
    "\n",
    "# W1 = tf.Variable(rand_W1, dtype='float32')\n",
    "# b1 = tf.Variable(rand_b1, dtype='float32')\n",
    "# l1_output = hypothesis(x_data, W1, b1)\n",
    "\n",
    "# W2 = tf.Variable(rand_W2, dtype='float32')\n",
    "# b2 = tf.Variable(rand_b2, dtype='float32')\n",
    "# l2_output = hypothesis(l1_output, W2, b2)\n",
    "\n",
    "# loss = lambda prob, y: -y*tf.math.log(prob) - (1-y)*tf.math.log(1-prob)\n",
    "\n",
    "sgd = tf.optimizers.legacy.SGD(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# rand_w11 = tf.random.normal(shape=(2,1), dtype='float32')\n",
    "# rand_b11 = tf.random.normal(shape=(1,), dtype='float32')\n",
    "\n",
    "# rand_w12 = tf.random.normal(shape=(2,1), dtype='float32')\n",
    "# rand_b12 = tf.random.normal(shape=(1,), dtype='float32')\n",
    "\n",
    "# rand_W2 = tf.random.normal(shape=(2,1), dtype='float32')\n",
    "# rand_b2 = tf.random.normal(shape=(1,), dtype='float32')\n",
    "\n",
    "# w11 = tf.Variable(rand_w11, dtype='float32')\n",
    "# b11 = tf.Variable(rand_b11, dtype='float32')\n",
    "\n",
    "# w12 = tf.Variable(rand_w12, dtype='float32')\n",
    "# b12 = tf.Variable(rand_b12, dtype='float32')\n",
    "\n",
    "# W2 = tf.Variable(rand_W2, dtype='float32')\n",
    "# b2 = tf.Variable(rand_b2, dtype='float32')\n",
    "\n",
    "# def model(x_data):\n",
    "#     # rand_W1 = tf.random.normal(shape=(2,2), dtype='float32')\n",
    "#     # rand_b1 = tf.random.normal(shape=(2,), dtype='float32')\n",
    "\n",
    "#     # rand_W2 = tf.random.normal(shape=(2,1), dtype='float32')\n",
    "#     # rand_b2 = tf.random.normal(shape=(1,), dtype='float32')\n",
    "\n",
    "#     hypothesis = lambda x, w, b: tf.sigmoid(tf.matmul(x, w) + b)\n",
    "\n",
    "#     # W1 = tf.Variable(rand_W1, dtype='float32')\n",
    "#     # b1 = tf.Variable(rand_b1, dtype='float32')\n",
    "#     # l1_output = hypothesis(x_data, W1, b1)\n",
    "\n",
    "#     # W2 = tf.Variable(rand_W2, dtype='float32')\n",
    "#     # b2 = tf.Variable(rand_b2, dtype='float32')\n",
    "#     l1n1_output = hypothesis(x_data, w11, b11)\n",
    "#     l1n2_output = hypothesis(x_data, w12, b12)\n",
    "#     l1_output = tf.concat([l1n1_output, l1n2_output], axis=1)\n",
    "    \n",
    "#     model = hypothesis(l1_output, W2, b2)\n",
    "\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model(x_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 이 자료를 참고해보자\n",
    "- https://velog.io/@lybin10/Lec-09-XOR-문제-딥러닝으로-풀기\n",
    "- https://wikidocs.net/111472\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 1\n",
    "# # learning_rate = 0.01\n",
    "\n",
    "# for e in range(epochs):\n",
    "#     with tf.GradientTape() as tape:\n",
    "#         # l2_output = hypothesis(l1_output, W2, b2)\n",
    "#         model_output = model(x_data)\n",
    "#         cost = tf.reduce_sum(loss(model_output, y_data))\n",
    "    \n",
    "#     # dl_dW11, dl_db11, dl_dW12, dl_db12, dl_dW2, dl_db2 = tape.gradient(cost, [w11, b11, w12, b12, W2,b2])\n",
    "\n",
    "#     # 파라미터 업데이트\n",
    "#     sgd.minimize(cost, var_list=[w11, b11, w12, b12, W2, b2], tape=tape)\n",
    "#     # sgd.apply_gradients(zip([dl_dW11, dl_db11, dl_dW12, dl_db12, dl_dW2, dl_db2], [w11, b11, w12, b12, W2, b2]))\n",
    "#     # print(dl_dW1, dl_db1, dl_dW2, dl_db2)\n",
    "\n",
    "#     # # L1 update\n",
    "#     # W1.assign_sub(learning_rate*dl_dW1)\n",
    "#     # b1.assign_sub(learning_rate*dl_db1)\n",
    "    \n",
    "#     # # L2 update\n",
    "#     # W2.assign_sub(learning_rate*dl_dW2)\n",
    "#     # b2.assign_sub(learning_rate*dl_db2)\n",
    "\n",
    "#     acc = compute_acc(y_data, prob)\n",
    "\n",
    "#     print(\"epoch: \", e, \", cost: \", tf.reduce_mean(cost).numpy(), \", accuracy: \", acc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "learning_rate = 0.5\n",
    "\n",
    "W1 = tf.Variable(tf.random.normal(shape=(2,2)), dtype='float32')\n",
    "b1 = tf.Variable(tf.random.normal(shape=(2,)), dtype='float32')\n",
    "W2 = tf.Variable(tf.random.normal(shape=(2,1)), dtype='float32')\n",
    "b2 = tf.Variable(tf.random.normal(shape=(1,)), dtype='float32')\n",
    "\n",
    "def model(X):\n",
    "    l1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "    l2 = tf.sigmoid(tf.matmul(l1, W2) + b2)\n",
    "    return l2\n",
    "\n",
    "loss = lambda prob, y: tf.reduce_sum(-y*tf.math.log(prob) - (1-y)*tf.math.log(1-prob))\n",
    "sgd = tf.optimizers.legacy.SGD(learning_rate)\n",
    "\n",
    "def compute_acc(true, pred):\n",
    "    acc = sum(tf.cast(tf.equal(true, pred>=.5), dtype='int32'))[0]/true.shape[0]\n",
    "    return acc.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0 , cost:  3.0682106 , accuracy:  0.5\n",
      "epoch:  1 , cost:  2.8103166 , accuracy:  0.5\n",
      "epoch:  2 , cost:  2.7759566 , accuracy:  0.5\n",
      "epoch:  3 , cost:  2.772011 , accuracy:  0.5\n",
      "epoch:  4 , cost:  2.7714763 , accuracy:  0.5\n",
      "epoch:  5 , cost:  2.7713108 , accuracy:  0.5\n",
      "epoch:  6 , cost:  2.7711844 , accuracy:  0.5\n",
      "epoch:  7 , cost:  2.77106 , accuracy:  0.5\n",
      "epoch:  8 , cost:  2.7709336 , accuracy:  0.5\n",
      "epoch:  9 , cost:  2.7708051 , accuracy:  0.5\n",
      "epoch:  10 , cost:  2.7706738 , accuracy:  0.5\n",
      "epoch:  11 , cost:  2.7705398 , accuracy:  0.5\n",
      "epoch:  12 , cost:  2.7704022 , accuracy:  0.5\n",
      "epoch:  13 , cost:  2.7702615 , accuracy:  0.5\n",
      "epoch:  14 , cost:  2.7701173 , accuracy:  0.5\n",
      "epoch:  15 , cost:  2.7699692 , accuracy:  0.5\n",
      "epoch:  16 , cost:  2.769817 , accuracy:  0.5\n",
      "epoch:  17 , cost:  2.7696605 , accuracy:  0.5\n",
      "epoch:  18 , cost:  2.7694995 , accuracy:  0.5\n",
      "epoch:  19 , cost:  2.7693338 , accuracy:  0.5\n",
      "epoch:  20 , cost:  2.7691631 , accuracy:  0.5\n",
      "epoch:  21 , cost:  2.7689867 , accuracy:  0.5\n",
      "epoch:  22 , cost:  2.768805 , accuracy:  0.5\n",
      "epoch:  23 , cost:  2.7686172 , accuracy:  0.5\n",
      "epoch:  24 , cost:  2.768423 , accuracy:  0.5\n",
      "epoch:  25 , cost:  2.768222 , accuracy:  0.5\n",
      "epoch:  26 , cost:  2.768015 , accuracy:  0.5\n",
      "epoch:  27 , cost:  2.7677999 , accuracy:  0.5\n",
      "epoch:  28 , cost:  2.767577 , accuracy:  0.5\n",
      "epoch:  29 , cost:  2.7673464 , accuracy:  0.5\n",
      "epoch:  30 , cost:  2.767107 , accuracy:  0.5\n",
      "epoch:  31 , cost:  2.7668595 , accuracy:  0.5\n",
      "epoch:  32 , cost:  2.7666023 , accuracy:  0.5\n",
      "epoch:  33 , cost:  2.7663355 , accuracy:  0.5\n",
      "epoch:  34 , cost:  2.7660582 , accuracy:  0.5\n",
      "epoch:  35 , cost:  2.7657704 , accuracy:  0.5\n",
      "epoch:  36 , cost:  2.7654715 , accuracy:  0.5\n",
      "epoch:  37 , cost:  2.7651606 , accuracy:  0.5\n",
      "epoch:  38 , cost:  2.7648373 , accuracy:  0.5\n",
      "epoch:  39 , cost:  2.7645016 , accuracy:  0.5\n",
      "epoch:  40 , cost:  2.764152 , accuracy:  0.5\n",
      "epoch:  41 , cost:  2.7637882 , accuracy:  0.5\n",
      "epoch:  42 , cost:  2.7634096 , accuracy:  0.5\n",
      "epoch:  43 , cost:  2.7630157 , accuracy:  0.5\n",
      "epoch:  44 , cost:  2.7626054 , accuracy:  0.5\n",
      "epoch:  45 , cost:  2.762178 , accuracy:  0.5\n",
      "epoch:  46 , cost:  2.7617326 , accuracy:  0.5\n",
      "epoch:  47 , cost:  2.7612686 , accuracy:  0.5\n",
      "epoch:  48 , cost:  2.7607849 , accuracy:  0.5\n",
      "epoch:  49 , cost:  2.7602806 , accuracy:  0.5\n",
      "epoch:  50 , cost:  2.759755 , accuracy:  0.5\n",
      "epoch:  51 , cost:  2.759206 , accuracy:  0.5\n",
      "epoch:  52 , cost:  2.7586336 , accuracy:  0.5\n",
      "epoch:  53 , cost:  2.7580364 , accuracy:  0.5\n",
      "epoch:  54 , cost:  2.757413 , accuracy:  0.5\n",
      "epoch:  55 , cost:  2.756762 , accuracy:  0.5\n",
      "epoch:  56 , cost:  2.756082 , accuracy:  0.5\n",
      "epoch:  57 , cost:  2.7553725 , accuracy:  0.5\n",
      "epoch:  58 , cost:  2.7546306 , accuracy:  0.5\n",
      "epoch:  59 , cost:  2.7538552 , accuracy:  0.5\n",
      "epoch:  60 , cost:  2.7530456 , accuracy:  0.5\n",
      "epoch:  61 , cost:  2.7521982 , accuracy:  0.5\n",
      "epoch:  62 , cost:  2.7513127 , accuracy:  0.5\n",
      "epoch:  63 , cost:  2.7503862 , accuracy:  0.5\n",
      "epoch:  64 , cost:  2.7494173 , accuracy:  0.5\n",
      "epoch:  65 , cost:  2.7484035 , accuracy:  0.5\n",
      "epoch:  66 , cost:  2.7473426 , accuracy:  0.5\n",
      "epoch:  67 , cost:  2.7462316 , accuracy:  0.5\n",
      "epoch:  68 , cost:  2.7450686 , accuracy:  0.5\n",
      "epoch:  69 , cost:  2.7438507 , accuracy:  0.5\n",
      "epoch:  70 , cost:  2.7425754 , accuracy:  0.5\n",
      "epoch:  71 , cost:  2.7412393 , accuracy:  0.5\n",
      "epoch:  72 , cost:  2.7398393 , accuracy:  0.5\n",
      "epoch:  73 , cost:  2.7383728 , accuracy:  0.5\n",
      "epoch:  74 , cost:  2.736836 , accuracy:  0.5\n",
      "epoch:  75 , cost:  2.735225 , accuracy:  0.5\n",
      "epoch:  76 , cost:  2.7335362 , accuracy:  0.5\n",
      "epoch:  77 , cost:  2.731766 , accuracy:  0.5\n",
      "epoch:  78 , cost:  2.7299101 , accuracy:  0.75\n",
      "epoch:  79 , cost:  2.7279644 , accuracy:  0.75\n",
      "epoch:  80 , cost:  2.725925 , accuracy:  0.75\n",
      "epoch:  81 , cost:  2.7237864 , accuracy:  0.75\n",
      "epoch:  82 , cost:  2.7215447 , accuracy:  0.75\n",
      "epoch:  83 , cost:  2.7191944 , accuracy:  0.75\n",
      "epoch:  84 , cost:  2.716731 , accuracy:  0.75\n",
      "epoch:  85 , cost:  2.7141495 , accuracy:  0.75\n",
      "epoch:  86 , cost:  2.7114441 , accuracy:  0.75\n",
      "epoch:  87 , cost:  2.70861 , accuracy:  0.75\n",
      "epoch:  88 , cost:  2.7056415 , accuracy:  0.75\n",
      "epoch:  89 , cost:  2.702533 , accuracy:  0.75\n",
      "epoch:  90 , cost:  2.6992788 , accuracy:  0.75\n",
      "epoch:  91 , cost:  2.6958737 , accuracy:  0.75\n",
      "epoch:  92 , cost:  2.6923118 , accuracy:  0.75\n",
      "epoch:  93 , cost:  2.6885877 , accuracy:  0.75\n",
      "epoch:  94 , cost:  2.6846957 , accuracy:  0.75\n",
      "epoch:  95 , cost:  2.680631 , accuracy:  0.75\n",
      "epoch:  96 , cost:  2.6763878 , accuracy:  0.75\n",
      "epoch:  97 , cost:  2.671961 , accuracy:  0.75\n",
      "epoch:  98 , cost:  2.667346 , accuracy:  0.75\n",
      "epoch:  99 , cost:  2.6625385 , accuracy:  0.75\n",
      "epoch:  100 , cost:  2.6575344 , accuracy:  0.75\n",
      "epoch:  101 , cost:  2.6523297 , accuracy:  0.75\n",
      "epoch:  102 , cost:  2.646921 , accuracy:  0.75\n",
      "epoch:  103 , cost:  2.641306 , accuracy:  0.75\n",
      "epoch:  104 , cost:  2.6354814 , accuracy:  0.75\n",
      "epoch:  105 , cost:  2.6294465 , accuracy:  0.75\n",
      "epoch:  106 , cost:  2.6232004 , accuracy:  0.75\n",
      "epoch:  107 , cost:  2.6167426 , accuracy:  0.75\n",
      "epoch:  108 , cost:  2.610073 , accuracy:  0.75\n",
      "epoch:  109 , cost:  2.603194 , accuracy:  0.75\n",
      "epoch:  110 , cost:  2.5961065 , accuracy:  0.75\n",
      "epoch:  111 , cost:  2.5888138 , accuracy:  0.75\n",
      "epoch:  112 , cost:  2.5813196 , accuracy:  0.75\n",
      "epoch:  113 , cost:  2.5736284 , accuracy:  0.75\n",
      "epoch:  114 , cost:  2.5657449 , accuracy:  0.75\n",
      "epoch:  115 , cost:  2.5576758 , accuracy:  0.75\n",
      "epoch:  116 , cost:  2.549428 , accuracy:  0.75\n",
      "epoch:  117 , cost:  2.541008 , accuracy:  0.75\n",
      "epoch:  118 , cost:  2.5324252 , accuracy:  0.75\n",
      "epoch:  119 , cost:  2.5236876 , accuracy:  0.75\n",
      "epoch:  120 , cost:  2.5148048 , accuracy:  0.75\n",
      "epoch:  121 , cost:  2.505786 , accuracy:  0.75\n",
      "epoch:  122 , cost:  2.4966424 , accuracy:  0.75\n",
      "epoch:  123 , cost:  2.487383 , accuracy:  0.75\n",
      "epoch:  124 , cost:  2.4780195 , accuracy:  0.75\n",
      "epoch:  125 , cost:  2.4685626 , accuracy:  0.75\n",
      "epoch:  126 , cost:  2.4590225 , accuracy:  0.75\n",
      "epoch:  127 , cost:  2.4494104 , accuracy:  0.75\n",
      "epoch:  128 , cost:  2.4397364 , accuracy:  0.75\n",
      "epoch:  129 , cost:  2.4300113 , accuracy:  0.75\n",
      "epoch:  130 , cost:  2.420245 , accuracy:  0.75\n",
      "epoch:  131 , cost:  2.4104471 , accuracy:  0.75\n",
      "epoch:  132 , cost:  2.4006276 , accuracy:  0.75\n",
      "epoch:  133 , cost:  2.3907945 , accuracy:  0.75\n",
      "epoch:  134 , cost:  2.380957 , accuracy:  0.75\n",
      "epoch:  135 , cost:  2.3711221 , accuracy:  0.75\n",
      "epoch:  136 , cost:  2.3612971 , accuracy:  0.75\n",
      "epoch:  137 , cost:  2.3514893 , accuracy:  0.75\n",
      "epoch:  138 , cost:  2.3417044 , accuracy:  0.75\n",
      "epoch:  139 , cost:  2.3319478 , accuracy:  0.75\n",
      "epoch:  140 , cost:  2.3222241 , accuracy:  0.75\n",
      "epoch:  141 , cost:  2.3125381 , accuracy:  0.75\n",
      "epoch:  142 , cost:  2.3028932 , accuracy:  0.75\n",
      "epoch:  143 , cost:  2.2932925 , accuracy:  0.75\n",
      "epoch:  144 , cost:  2.2837386 , accuracy:  0.75\n",
      "epoch:  145 , cost:  2.274233 , accuracy:  0.75\n",
      "epoch:  146 , cost:  2.264778 , accuracy:  0.75\n",
      "epoch:  147 , cost:  2.2553742 , accuracy:  0.75\n",
      "epoch:  148 , cost:  2.2460222 , accuracy:  0.75\n",
      "epoch:  149 , cost:  2.2367227 , accuracy:  0.75\n",
      "epoch:  150 , cost:  2.2274747 , accuracy:  0.75\n",
      "epoch:  151 , cost:  2.2182784 , accuracy:  0.75\n",
      "epoch:  152 , cost:  2.2091324 , accuracy:  0.75\n",
      "epoch:  153 , cost:  2.2000356 , accuracy:  0.75\n",
      "epoch:  154 , cost:  2.1909864 , accuracy:  0.75\n",
      "epoch:  155 , cost:  2.1819835 , accuracy:  0.75\n",
      "epoch:  156 , cost:  2.173025 , accuracy:  0.75\n",
      "epoch:  157 , cost:  2.1641083 , accuracy:  0.75\n",
      "epoch:  158 , cost:  2.1552312 , accuracy:  0.75\n",
      "epoch:  159 , cost:  2.1463914 , accuracy:  0.75\n",
      "epoch:  160 , cost:  2.1375864 , accuracy:  0.75\n",
      "epoch:  161 , cost:  2.1288128 , accuracy:  0.75\n",
      "epoch:  162 , cost:  2.120068 , accuracy:  0.75\n",
      "epoch:  163 , cost:  2.1113489 , accuracy:  0.75\n",
      "epoch:  164 , cost:  2.1026516 , accuracy:  0.75\n",
      "epoch:  165 , cost:  2.0939734 , accuracy:  0.75\n",
      "epoch:  166 , cost:  2.08531 , accuracy:  0.75\n",
      "epoch:  167 , cost:  2.0766575 , accuracy:  0.75\n",
      "epoch:  168 , cost:  2.0680118 , accuracy:  0.75\n",
      "epoch:  169 , cost:  2.0593677 , accuracy:  0.75\n",
      "epoch:  170 , cost:  2.0507207 , accuracy:  0.75\n",
      "epoch:  171 , cost:  2.0420644 , accuracy:  0.75\n",
      "epoch:  172 , cost:  2.0333931 , accuracy:  0.75\n",
      "epoch:  173 , cost:  2.0246987 , accuracy:  0.75\n",
      "epoch:  174 , cost:  2.015973 , accuracy:  0.75\n",
      "epoch:  175 , cost:  2.007207 , accuracy:  0.75\n",
      "epoch:  176 , cost:  1.9983885 , accuracy:  0.75\n",
      "epoch:  177 , cost:  1.9895048 , accuracy:  0.75\n",
      "epoch:  178 , cost:  1.9805398 , accuracy:  0.75\n",
      "epoch:  179 , cost:  1.9714751 , accuracy:  0.75\n",
      "epoch:  180 , cost:  1.9622884 , accuracy:  0.75\n",
      "epoch:  181 , cost:  1.9529523 , accuracy:  0.75\n",
      "epoch:  182 , cost:  1.9434348 , accuracy:  0.75\n",
      "epoch:  183 , cost:  1.9336967 , accuracy:  0.75\n",
      "epoch:  184 , cost:  1.9236906 , accuracy:  0.75\n",
      "epoch:  185 , cost:  1.9133589 , accuracy:  0.75\n",
      "epoch:  186 , cost:  1.9026318 , accuracy:  0.75\n",
      "epoch:  187 , cost:  1.8914256 , accuracy:  0.75\n",
      "epoch:  188 , cost:  1.8796387 , accuracy:  0.75\n",
      "epoch:  189 , cost:  1.86715 , accuracy:  0.75\n",
      "epoch:  190 , cost:  1.8538158 , accuracy:  0.75\n",
      "epoch:  191 , cost:  1.8394691 , accuracy:  0.75\n",
      "epoch:  192 , cost:  1.8239195 , accuracy:  0.75\n",
      "epoch:  193 , cost:  1.8069588 , accuracy:  0.75\n",
      "epoch:  194 , cost:  1.7883682 , accuracy:  0.75\n",
      "epoch:  195 , cost:  1.767938 , accuracy:  0.75\n",
      "epoch:  196 , cost:  1.7454921 , accuracy:  0.75\n",
      "epoch:  197 , cost:  1.7209184 , accuracy:  0.75\n",
      "epoch:  198 , cost:  1.6942021 , accuracy:  0.75\n",
      "epoch:  199 , cost:  1.6654456 , accuracy:  0.75\n",
      "epoch:  200 , cost:  1.6348695 , accuracy:  0.75\n",
      "epoch:  201 , cost:  1.6027853 , accuracy:  0.75\n",
      "epoch:  202 , cost:  1.5695515 , accuracy:  0.75\n",
      "epoch:  203 , cost:  1.5355215 , accuracy:  0.75\n",
      "epoch:  204 , cost:  1.5010061 , accuracy:  0.75\n",
      "epoch:  205 , cost:  1.4662542 , accuracy:  0.75\n",
      "epoch:  206 , cost:  1.4314556 , accuracy:  0.75\n",
      "epoch:  207 , cost:  1.3967501 , accuracy:  0.75\n",
      "epoch:  208 , cost:  1.3622439 , accuracy:  1.0\n",
      "epoch:  209 , cost:  1.3280199 , accuracy:  1.0\n",
      "epoch:  210 , cost:  1.2941508 , accuracy:  1.0\n",
      "epoch:  211 , cost:  1.2607012 , accuracy:  1.0\n",
      "epoch:  212 , cost:  1.2277306 , accuracy:  1.0\n",
      "epoch:  213 , cost:  1.1952971 , accuracy:  1.0\n",
      "epoch:  214 , cost:  1.1634521 , accuracy:  1.0\n",
      "epoch:  215 , cost:  1.1322438 , accuracy:  1.0\n",
      "epoch:  216 , cost:  1.1017148 , accuracy:  1.0\n",
      "epoch:  217 , cost:  1.0719017 , accuracy:  1.0\n",
      "epoch:  218 , cost:  1.0428352 , accuracy:  1.0\n",
      "epoch:  219 , cost:  1.0145394 , accuracy:  1.0\n",
      "epoch:  220 , cost:  0.9870318 , accuracy:  1.0\n",
      "epoch:  221 , cost:  0.9603251 , accuracy:  1.0\n",
      "epoch:  222 , cost:  0.93442535 , accuracy:  1.0\n",
      "epoch:  223 , cost:  0.90933454 , accuracy:  1.0\n",
      "epoch:  224 , cost:  0.88504946 , accuracy:  1.0\n",
      "epoch:  225 , cost:  0.8615634 , accuracy:  1.0\n",
      "epoch:  226 , cost:  0.8388662 , accuracy:  1.0\n",
      "epoch:  227 , cost:  0.8169446 , accuracy:  1.0\n",
      "epoch:  228 , cost:  0.79578304 , accuracy:  1.0\n",
      "epoch:  229 , cost:  0.77536386 , accuracy:  1.0\n",
      "epoch:  230 , cost:  0.755668 , accuracy:  1.0\n",
      "epoch:  231 , cost:  0.7366755 , accuracy:  1.0\n",
      "epoch:  232 , cost:  0.71836436 , accuracy:  1.0\n",
      "epoch:  233 , cost:  0.70071346 , accuracy:  1.0\n",
      "epoch:  234 , cost:  0.68369997 , accuracy:  1.0\n",
      "epoch:  235 , cost:  0.6673019 , accuracy:  1.0\n",
      "epoch:  236 , cost:  0.65149707 , accuracy:  1.0\n",
      "epoch:  237 , cost:  0.6362629 , accuracy:  1.0\n",
      "epoch:  238 , cost:  0.62157786 , accuracy:  1.0\n",
      "epoch:  239 , cost:  0.60742056 , accuracy:  1.0\n",
      "epoch:  240 , cost:  0.5937698 , accuracy:  1.0\n",
      "epoch:  241 , cost:  0.5806054 , accuracy:  1.0\n",
      "epoch:  242 , cost:  0.56790715 , accuracy:  1.0\n",
      "epoch:  243 , cost:  0.5556561 , accuracy:  1.0\n",
      "epoch:  244 , cost:  0.543834 , accuracy:  1.0\n",
      "epoch:  245 , cost:  0.5324224 , accuracy:  1.0\n",
      "epoch:  246 , cost:  0.5214045 , accuracy:  1.0\n",
      "epoch:  247 , cost:  0.5107638 , accuracy:  1.0\n",
      "epoch:  248 , cost:  0.5004842 , accuracy:  1.0\n",
      "epoch:  249 , cost:  0.49055055 , accuracy:  1.0\n",
      "epoch:  250 , cost:  0.48094818 , accuracy:  1.0\n",
      "epoch:  251 , cost:  0.4716636 , accuracy:  1.0\n",
      "epoch:  252 , cost:  0.46268305 , accuracy:  1.0\n",
      "epoch:  253 , cost:  0.453994 , accuracy:  1.0\n",
      "epoch:  254 , cost:  0.4455843 , accuracy:  1.0\n",
      "epoch:  255 , cost:  0.43744227 , accuracy:  1.0\n",
      "epoch:  256 , cost:  0.42955688 , accuracy:  1.0\n",
      "epoch:  257 , cost:  0.42191753 , accuracy:  1.0\n",
      "epoch:  258 , cost:  0.41451424 , accuracy:  1.0\n",
      "epoch:  259 , cost:  0.40733746 , accuracy:  1.0\n",
      "epoch:  260 , cost:  0.40037775 , accuracy:  1.0\n",
      "epoch:  261 , cost:  0.39362675 , accuracy:  1.0\n",
      "epoch:  262 , cost:  0.38707578 , accuracy:  1.0\n",
      "epoch:  263 , cost:  0.38071716 , accuracy:  1.0\n",
      "epoch:  264 , cost:  0.37454325 , accuracy:  1.0\n",
      "epoch:  265 , cost:  0.36854672 , accuracy:  1.0\n",
      "epoch:  266 , cost:  0.3627209 , accuracy:  1.0\n",
      "epoch:  267 , cost:  0.35705888 , accuracy:  1.0\n",
      "epoch:  268 , cost:  0.35155463 , accuracy:  1.0\n",
      "epoch:  269 , cost:  0.34620222 , accuracy:  1.0\n",
      "epoch:  270 , cost:  0.3409958 , accuracy:  1.0\n",
      "epoch:  271 , cost:  0.33593 , accuracy:  1.0\n",
      "epoch:  272 , cost:  0.33099976 , accuracy:  1.0\n",
      "epoch:  273 , cost:  0.3261997 , accuracy:  1.0\n",
      "epoch:  274 , cost:  0.32152537 , accuracy:  1.0\n",
      "epoch:  275 , cost:  0.31697235 , accuracy:  1.0\n",
      "epoch:  276 , cost:  0.31253585 , accuracy:  1.0\n",
      "epoch:  277 , cost:  0.30821216 , accuracy:  1.0\n",
      "epoch:  278 , cost:  0.30399713 , accuracy:  1.0\n",
      "epoch:  279 , cost:  0.29988697 , accuracy:  1.0\n",
      "epoch:  280 , cost:  0.2958781 , accuracy:  1.0\n",
      "epoch:  281 , cost:  0.29196697 , accuracy:  1.0\n",
      "epoch:  282 , cost:  0.28815037 , accuracy:  1.0\n",
      "epoch:  283 , cost:  0.2844251 , accuracy:  1.0\n",
      "epoch:  284 , cost:  0.28078794 , accuracy:  1.0\n",
      "epoch:  285 , cost:  0.27723613 , accuracy:  1.0\n",
      "epoch:  286 , cost:  0.27376682 , accuracy:  1.0\n",
      "epoch:  287 , cost:  0.27037728 , accuracy:  1.0\n",
      "epoch:  288 , cost:  0.267065 , accuracy:  1.0\n",
      "epoch:  289 , cost:  0.26382732 , accuracy:  1.0\n",
      "epoch:  290 , cost:  0.26066226 , accuracy:  1.0\n",
      "epoch:  291 , cost:  0.25756708 , accuracy:  1.0\n",
      "epoch:  292 , cost:  0.2545399 , accuracy:  1.0\n",
      "epoch:  293 , cost:  0.25157845 , accuracy:  1.0\n",
      "epoch:  294 , cost:  0.24868089 , accuracy:  1.0\n",
      "epoch:  295 , cost:  0.24584508 , accuracy:  1.0\n",
      "epoch:  296 , cost:  0.24306922 , accuracy:  1.0\n",
      "epoch:  297 , cost:  0.2403517 , accuracy:  1.0\n",
      "epoch:  298 , cost:  0.23769026 , accuracy:  1.0\n",
      "epoch:  299 , cost:  0.23508391 , accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "epochs = 300\n",
    "\n",
    "for e in range(epochs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        prob = model(x_data)\n",
    "        cost = loss(prob, y_data)\n",
    "\n",
    "    # 파라미터 업데이트\n",
    "    sgd.minimize(cost, var_list=[W1, b1, W2, b2], tape=tape)\n",
    "    acc = compute_acc(y_data, prob)\n",
    "    \n",
    "    print(\"epoch: \", e, \", cost: \", tf.reduce_mean(cost).numpy(), \", accuracy: \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(4, 1), dtype=float32, numpy=\n",
       " array([[0.05547209],\n",
       "        [0.9582954 ],\n",
       "        [0.91731244],\n",
       "        [0.04548714]], dtype=float32)>,\n",
       " 1.0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob = model(x_data)\n",
    "acc = compute_acc(y_data, prob)\n",
    "\n",
    "prob, acc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Tensorboard\n",
    "\n",
    "그래프를 로깅할 때에는, function으로 따로 추적해주어야 하며, scalar는 파일에 써주면 된다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.5\n",
    "\n",
    "W1 = tf.Variable(tf.random.normal(shape=(2,2)), dtype='float32', name='layer1_weights')\n",
    "b1 = tf.Variable(tf.random.normal(shape=(2,)), dtype='float32', name='layer1_bias')\n",
    "\n",
    "W2 = tf.Variable(tf.random.normal(shape=(2,1)), dtype='float32', name='layer2_weights')\n",
    "b2 = tf.Variable(tf.random.normal(shape=(1,)), dtype='float32', name='layer2_bias')\n",
    "\n",
    "@tf.function\n",
    "def model(X):\n",
    "    l1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "    l2 = tf.sigmoid(tf.matmul(l1, W2) + b2)\n",
    "    return l2\n",
    "\n",
    "# cost/loss function\n",
    "with tf.name_scope(\"Cost\"):\n",
    "    loss = lambda prob, y: tf.reduce_sum(-y*tf.math.log(prob) - (1-y)*tf.math.log(1-prob))\n",
    "\n",
    "sgd = tf.optimizers.legacy.SGD(learning_rate)\n",
    "\n",
    "def compute_acc(true, pred):\n",
    "    acc = sum(tf.cast(tf.equal(true, pred>=.5), dtype='int32'))[0]/true.shape[0]\n",
    "    tf.summary.scalar(\"accuracy\", acc)\n",
    "    return acc.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/homebrew/anaconda3/envs/general/lib/python3.10/site-packages/tensorflow/python/ops/summary_ops_v2.py:1332: start (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.start` instead.\n",
      "WARNING:tensorflow:From /opt/homebrew/anaconda3/envs/general/lib/python3.10/site-packages/tensorflow/python/ops/summary_ops_v2.py:1383: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "WARNING:tensorflow:From /opt/homebrew/anaconda3/envs/general/lib/python3.10/site-packages/tensorflow/python/ops/summary_ops_v2.py:1383: save (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "`tf.python.eager.profiler` has deprecated, use `tf.profiler` instead.\n",
      "WARNING:tensorflow:From /opt/homebrew/anaconda3/envs/general/lib/python3.10/site-packages/tensorflow/python/eager/profiler.py:150: maybe_create_event_file (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "`tf.python.eager.profiler` has deprecated, use `tf.profiler` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-23 15:15:31.190652: I tensorflow/tsl/profiler/lib/profiler_session.cc:104] Profiler session initializing.\n",
      "2023-06-23 15:15:31.190660: I tensorflow/tsl/profiler/lib/profiler_session.cc:119] Profiler session started.\n",
      "2023-06-23 15:15:31.267229: I tensorflow/tsl/profiler/lib/profiler_session.cc:70] Profiler session collecting data.\n",
      "2023-06-23 15:15:31.267449: I tensorflow/tsl/profiler/lib/profiler_session.cc:131] Profiler session tear down.\n"
     ]
    }
   ],
   "source": [
    "# Set up logging\n",
    "stamp = dt.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "graph_log_dir = 'logs/func/%s' % stamp\n",
    "writer = tf.summary.create_file_writer(graph_log_dir)\n",
    "\n",
    "# Bracket the function call with\n",
    "# tf.summary.trace_on() and tf.summary.trace_export().\n",
    "tf.summary.trace_on(graph=True, profiler=True)\n",
    "prob = model(x_data)\n",
    "\n",
    "with writer.as_default():\n",
    "  tf.summary.trace_export(\n",
    "      name=\"my_func_trace\",\n",
    "      step=0,\n",
    "      profiler_outdir=graph_log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_log_dir = 'logs/gradient_tape/%s/train' % stamp\n",
    "\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0 , cost:  2.8205018 , accuracy:  0.5\n",
      "epoch:  1 , cost:  2.7846165 , accuracy:  0.25\n",
      "epoch:  2 , cost:  2.7836256 , accuracy:  0.5\n",
      "epoch:  3 , cost:  2.7830794 , accuracy:  0.5\n",
      "epoch:  4 , cost:  2.7825673 , accuracy:  0.5\n",
      "epoch:  5 , cost:  2.7820826 , accuracy:  0.5\n",
      "epoch:  6 , cost:  2.7816224 , accuracy:  0.5\n",
      "epoch:  7 , cost:  2.7811854 , accuracy:  0.5\n",
      "epoch:  8 , cost:  2.7807693 , accuracy:  0.5\n",
      "epoch:  9 , cost:  2.7803717 , accuracy:  0.5\n",
      "epoch:  10 , cost:  2.7799919 , accuracy:  0.5\n",
      "epoch:  11 , cost:  2.779628 , accuracy:  0.5\n",
      "epoch:  12 , cost:  2.7792792 , accuracy:  0.5\n",
      "epoch:  13 , cost:  2.7789435 , accuracy:  0.5\n",
      "epoch:  14 , cost:  2.7786205 , accuracy:  0.5\n",
      "epoch:  15 , cost:  2.7783089 , accuracy:  0.5\n",
      "epoch:  16 , cost:  2.778008 , accuracy:  0.5\n",
      "epoch:  17 , cost:  2.7777166 , accuracy:  0.5\n",
      "epoch:  18 , cost:  2.7774348 , accuracy:  0.5\n",
      "epoch:  19 , cost:  2.7771606 , accuracy:  0.5\n",
      "epoch:  20 , cost:  2.7768943 , accuracy:  0.5\n",
      "epoch:  21 , cost:  2.7766352 , accuracy:  0.5\n",
      "epoch:  22 , cost:  2.7763822 , accuracy:  0.5\n",
      "epoch:  23 , cost:  2.7761352 , accuracy:  0.5\n",
      "epoch:  24 , cost:  2.775894 , accuracy:  0.5\n",
      "epoch:  25 , cost:  2.7756572 , accuracy:  0.5\n",
      "epoch:  26 , cost:  2.775425 , accuracy:  0.5\n",
      "epoch:  27 , cost:  2.775197 , accuracy:  0.5\n",
      "epoch:  28 , cost:  2.774973 , accuracy:  0.5\n",
      "epoch:  29 , cost:  2.7747517 , accuracy:  0.5\n",
      "epoch:  30 , cost:  2.7745335 , accuracy:  0.5\n",
      "epoch:  31 , cost:  2.7743177 , accuracy:  0.5\n",
      "epoch:  32 , cost:  2.7741048 , accuracy:  0.5\n",
      "epoch:  33 , cost:  2.7738936 , accuracy:  0.5\n",
      "epoch:  34 , cost:  2.7736838 , accuracy:  0.5\n",
      "epoch:  35 , cost:  2.7734756 , accuracy:  0.5\n",
      "epoch:  36 , cost:  2.7732682 , accuracy:  0.5\n",
      "epoch:  37 , cost:  2.7730618 , accuracy:  0.5\n",
      "epoch:  38 , cost:  2.7728555 , accuracy:  0.5\n",
      "epoch:  39 , cost:  2.7726498 , accuracy:  0.5\n",
      "epoch:  40 , cost:  2.7724435 , accuracy:  0.5\n",
      "epoch:  41 , cost:  2.7722378 , accuracy:  0.5\n",
      "epoch:  42 , cost:  2.772031 , accuracy:  0.5\n",
      "epoch:  43 , cost:  2.7718236 , accuracy:  0.5\n",
      "epoch:  44 , cost:  2.771615 , accuracy:  0.5\n",
      "epoch:  45 , cost:  2.771405 , accuracy:  0.5\n",
      "epoch:  46 , cost:  2.7711937 , accuracy:  0.5\n",
      "epoch:  47 , cost:  2.7709806 , accuracy:  0.5\n",
      "epoch:  48 , cost:  2.7707653 , accuracy:  0.5\n",
      "epoch:  49 , cost:  2.7705479 , accuracy:  0.5\n",
      "epoch:  50 , cost:  2.7703276 , accuracy:  0.5\n",
      "epoch:  51 , cost:  2.770105 , accuracy:  0.5\n",
      "epoch:  52 , cost:  2.7698786 , accuracy:  0.5\n",
      "epoch:  53 , cost:  2.7696495 , accuracy:  0.5\n",
      "epoch:  54 , cost:  2.7694168 , accuracy:  0.5\n",
      "epoch:  55 , cost:  2.7691803 , accuracy:  0.5\n",
      "epoch:  56 , cost:  2.7689395 , accuracy:  0.5\n",
      "epoch:  57 , cost:  2.7686944 , accuracy:  0.5\n",
      "epoch:  58 , cost:  2.7684453 , accuracy:  0.5\n",
      "epoch:  59 , cost:  2.7681904 , accuracy:  0.5\n",
      "epoch:  60 , cost:  2.767931 , accuracy:  0.5\n",
      "epoch:  61 , cost:  2.7676656 , accuracy:  0.25\n",
      "epoch:  62 , cost:  2.7673948 , accuracy:  0.25\n",
      "epoch:  63 , cost:  2.767118 , accuracy:  0.25\n",
      "epoch:  64 , cost:  2.7668345 , accuracy:  0.25\n",
      "epoch:  65 , cost:  2.7665448 , accuracy:  0.5\n",
      "epoch:  66 , cost:  2.7662477 , accuracy:  0.5\n",
      "epoch:  67 , cost:  2.7659435 , accuracy:  0.5\n",
      "epoch:  68 , cost:  2.7656312 , accuracy:  0.5\n",
      "epoch:  69 , cost:  2.7653115 , accuracy:  0.5\n",
      "epoch:  70 , cost:  2.7649832 , accuracy:  0.5\n",
      "epoch:  71 , cost:  2.764646 , accuracy:  0.5\n",
      "epoch:  72 , cost:  2.7642999 , accuracy:  0.5\n",
      "epoch:  73 , cost:  2.7639441 , accuracy:  0.5\n",
      "epoch:  74 , cost:  2.7635784 , accuracy:  0.5\n",
      "epoch:  75 , cost:  2.763202 , accuracy:  0.5\n",
      "epoch:  76 , cost:  2.7628152 , accuracy:  0.5\n",
      "epoch:  77 , cost:  2.7624173 , accuracy:  0.75\n",
      "epoch:  78 , cost:  2.7620072 , accuracy:  0.75\n",
      "epoch:  79 , cost:  2.7615852 , accuracy:  0.75\n",
      "epoch:  80 , cost:  2.7611508 , accuracy:  0.75\n",
      "epoch:  81 , cost:  2.7607026 , accuracy:  0.75\n",
      "epoch:  82 , cost:  2.7602406 , accuracy:  0.75\n",
      "epoch:  83 , cost:  2.7597647 , accuracy:  0.75\n",
      "epoch:  84 , cost:  2.759274 , accuracy:  0.75\n",
      "epoch:  85 , cost:  2.7587674 , accuracy:  0.75\n",
      "epoch:  86 , cost:  2.758245 , accuracy:  0.75\n",
      "epoch:  87 , cost:  2.7577057 , accuracy:  0.75\n",
      "epoch:  88 , cost:  2.7571492 , accuracy:  0.75\n",
      "epoch:  89 , cost:  2.7565746 , accuracy:  0.75\n",
      "epoch:  90 , cost:  2.7559814 , accuracy:  0.75\n",
      "epoch:  91 , cost:  2.7553682 , accuracy:  0.75\n",
      "epoch:  92 , cost:  2.754735 , accuracy:  0.75\n",
      "epoch:  93 , cost:  2.7540805 , accuracy:  0.75\n",
      "epoch:  94 , cost:  2.7534041 , accuracy:  0.75\n",
      "epoch:  95 , cost:  2.7527049 , accuracy:  0.75\n",
      "epoch:  96 , cost:  2.751982 , accuracy:  0.75\n",
      "epoch:  97 , cost:  2.7512348 , accuracy:  0.75\n",
      "epoch:  98 , cost:  2.750461 , accuracy:  0.75\n",
      "epoch:  99 , cost:  2.7496612 , accuracy:  0.75\n",
      "epoch:  100 , cost:  2.7488334 , accuracy:  0.75\n",
      "epoch:  101 , cost:  2.7479768 , accuracy:  0.75\n",
      "epoch:  102 , cost:  2.74709 , accuracy:  0.75\n",
      "epoch:  103 , cost:  2.7461724 , accuracy:  0.75\n",
      "epoch:  104 , cost:  2.745222 , accuracy:  0.75\n",
      "epoch:  105 , cost:  2.744238 , accuracy:  0.75\n",
      "epoch:  106 , cost:  2.7432184 , accuracy:  0.75\n",
      "epoch:  107 , cost:  2.7421627 , accuracy:  0.75\n",
      "epoch:  108 , cost:  2.7410693 , accuracy:  0.75\n",
      "epoch:  109 , cost:  2.7399356 , accuracy:  0.75\n",
      "epoch:  110 , cost:  2.738761 , accuracy:  0.75\n",
      "epoch:  111 , cost:  2.7375436 , accuracy:  0.75\n",
      "epoch:  112 , cost:  2.7362814 , accuracy:  0.75\n",
      "epoch:  113 , cost:  2.7349727 , accuracy:  0.75\n",
      "epoch:  114 , cost:  2.7336159 , accuracy:  0.75\n",
      "epoch:  115 , cost:  2.732208 , accuracy:  0.75\n",
      "epoch:  116 , cost:  2.7307484 , accuracy:  0.75\n",
      "epoch:  117 , cost:  2.7292342 , accuracy:  0.75\n",
      "epoch:  118 , cost:  2.7276633 , accuracy:  0.75\n",
      "epoch:  119 , cost:  2.7260332 , accuracy:  0.75\n",
      "epoch:  120 , cost:  2.7243414 , accuracy:  0.75\n",
      "epoch:  121 , cost:  2.722586 , accuracy:  0.75\n",
      "epoch:  122 , cost:  2.7207637 , accuracy:  0.75\n",
      "epoch:  123 , cost:  2.7188718 , accuracy:  0.75\n",
      "epoch:  124 , cost:  2.716908 , accuracy:  0.75\n",
      "epoch:  125 , cost:  2.714869 , accuracy:  0.75\n",
      "epoch:  126 , cost:  2.7127519 , accuracy:  0.75\n",
      "epoch:  127 , cost:  2.7105534 , accuracy:  0.75\n",
      "epoch:  128 , cost:  2.7082708 , accuracy:  0.75\n",
      "epoch:  129 , cost:  2.7058997 , accuracy:  0.75\n",
      "epoch:  130 , cost:  2.7034383 , accuracy:  0.75\n",
      "epoch:  131 , cost:  2.7008815 , accuracy:  0.75\n",
      "epoch:  132 , cost:  2.6982265 , accuracy:  0.75\n",
      "epoch:  133 , cost:  2.6954691 , accuracy:  0.75\n",
      "epoch:  134 , cost:  2.692606 , accuracy:  0.75\n",
      "epoch:  135 , cost:  2.6896334 , accuracy:  0.75\n",
      "epoch:  136 , cost:  2.6865468 , accuracy:  0.75\n",
      "epoch:  137 , cost:  2.683343 , accuracy:  0.75\n",
      "epoch:  138 , cost:  2.680017 , accuracy:  0.75\n",
      "epoch:  139 , cost:  2.6765652 , accuracy:  0.75\n",
      "epoch:  140 , cost:  2.6729836 , accuracy:  0.75\n",
      "epoch:  141 , cost:  2.6692686 , accuracy:  0.75\n",
      "epoch:  142 , cost:  2.6654158 , accuracy:  0.75\n",
      "epoch:  143 , cost:  2.6614208 , accuracy:  0.75\n",
      "epoch:  144 , cost:  2.6572804 , accuracy:  0.75\n",
      "epoch:  145 , cost:  2.6529903 , accuracy:  0.75\n",
      "epoch:  146 , cost:  2.6485474 , accuracy:  0.75\n",
      "epoch:  147 , cost:  2.643948 , accuracy:  0.75\n",
      "epoch:  148 , cost:  2.6391888 , accuracy:  0.75\n",
      "epoch:  149 , cost:  2.6342669 , accuracy:  0.75\n",
      "epoch:  150 , cost:  2.6291788 , accuracy:  0.75\n",
      "epoch:  151 , cost:  2.623923 , accuracy:  0.75\n",
      "epoch:  152 , cost:  2.6184971 , accuracy:  0.75\n",
      "epoch:  153 , cost:  2.6128995 , accuracy:  0.75\n",
      "epoch:  154 , cost:  2.6071286 , accuracy:  0.75\n",
      "epoch:  155 , cost:  2.6011834 , accuracy:  0.75\n",
      "epoch:  156 , cost:  2.5950637 , accuracy:  0.75\n",
      "epoch:  157 , cost:  2.5887694 , accuracy:  0.75\n",
      "epoch:  158 , cost:  2.5823011 , accuracy:  0.75\n",
      "epoch:  159 , cost:  2.57566 , accuracy:  0.75\n",
      "epoch:  160 , cost:  2.5688472 , accuracy:  0.75\n",
      "epoch:  161 , cost:  2.5618653 , accuracy:  0.75\n",
      "epoch:  162 , cost:  2.5547163 , accuracy:  0.75\n",
      "epoch:  163 , cost:  2.5474038 , accuracy:  0.75\n",
      "epoch:  164 , cost:  2.539932 , accuracy:  0.75\n",
      "epoch:  165 , cost:  2.532304 , accuracy:  0.75\n",
      "epoch:  166 , cost:  2.5245256 , accuracy:  0.75\n",
      "epoch:  167 , cost:  2.5166006 , accuracy:  0.75\n",
      "epoch:  168 , cost:  2.5085359 , accuracy:  0.75\n",
      "epoch:  169 , cost:  2.5003371 , accuracy:  0.75\n",
      "epoch:  170 , cost:  2.4920106 , accuracy:  0.75\n",
      "epoch:  171 , cost:  2.4835622 , accuracy:  0.75\n",
      "epoch:  172 , cost:  2.475 , accuracy:  0.75\n",
      "epoch:  173 , cost:  2.4663305 , accuracy:  0.75\n",
      "epoch:  174 , cost:  2.4575608 , accuracy:  0.75\n",
      "epoch:  175 , cost:  2.448699 , accuracy:  0.75\n",
      "epoch:  176 , cost:  2.4397526 , accuracy:  0.75\n",
      "epoch:  177 , cost:  2.4307287 , accuracy:  0.75\n",
      "epoch:  178 , cost:  2.4216359 , accuracy:  0.75\n",
      "epoch:  179 , cost:  2.4124813 , accuracy:  0.75\n",
      "epoch:  180 , cost:  2.4032726 , accuracy:  0.75\n",
      "epoch:  181 , cost:  2.3940177 , accuracy:  0.75\n",
      "epoch:  182 , cost:  2.384723 , accuracy:  0.75\n",
      "epoch:  183 , cost:  2.3753967 , accuracy:  0.75\n",
      "epoch:  184 , cost:  2.366046 , accuracy:  0.75\n",
      "epoch:  185 , cost:  2.3566768 , accuracy:  0.75\n",
      "epoch:  186 , cost:  2.3472972 , accuracy:  0.75\n",
      "epoch:  187 , cost:  2.337912 , accuracy:  0.75\n",
      "epoch:  188 , cost:  2.328529 , accuracy:  0.75\n",
      "epoch:  189 , cost:  2.3191528 , accuracy:  0.75\n",
      "epoch:  190 , cost:  2.3097906 , accuracy:  0.75\n",
      "epoch:  191 , cost:  2.3004465 , accuracy:  0.75\n",
      "epoch:  192 , cost:  2.291126 , accuracy:  0.75\n",
      "epoch:  193 , cost:  2.2818341 , accuracy:  0.75\n",
      "epoch:  194 , cost:  2.2725754 , accuracy:  0.75\n",
      "epoch:  195 , cost:  2.2633538 , accuracy:  0.75\n",
      "epoch:  196 , cost:  2.254174 , accuracy:  0.75\n",
      "epoch:  197 , cost:  2.2450395 , accuracy:  0.75\n",
      "epoch:  198 , cost:  2.2359533 , accuracy:  0.75\n",
      "epoch:  199 , cost:  2.2269201 , accuracy:  0.75\n",
      "epoch:  200 , cost:  2.217941 , accuracy:  0.75\n",
      "epoch:  201 , cost:  2.2090201 , accuracy:  0.75\n",
      "epoch:  202 , cost:  2.20016 , accuracy:  0.75\n",
      "epoch:  203 , cost:  2.191362 , accuracy:  0.75\n",
      "epoch:  204 , cost:  2.182629 , accuracy:  0.75\n",
      "epoch:  205 , cost:  2.1739626 , accuracy:  0.75\n",
      "epoch:  206 , cost:  2.1653645 , accuracy:  0.75\n",
      "epoch:  207 , cost:  2.1568372 , accuracy:  0.75\n",
      "epoch:  208 , cost:  2.1483808 , accuracy:  0.75\n",
      "epoch:  209 , cost:  2.1399965 , accuracy:  0.75\n",
      "epoch:  210 , cost:  2.1316864 , accuracy:  0.75\n",
      "epoch:  211 , cost:  2.123451 , accuracy:  0.75\n",
      "epoch:  212 , cost:  2.1152906 , accuracy:  0.75\n",
      "epoch:  213 , cost:  2.1072063 , accuracy:  0.75\n",
      "epoch:  214 , cost:  2.0991983 , accuracy:  0.75\n",
      "epoch:  215 , cost:  2.091268 , accuracy:  0.75\n",
      "epoch:  216 , cost:  2.083414 , accuracy:  0.75\n",
      "epoch:  217 , cost:  2.075638 , accuracy:  0.75\n",
      "epoch:  218 , cost:  2.0679398 , accuracy:  0.75\n",
      "epoch:  219 , cost:  2.0603185 , accuracy:  0.75\n",
      "epoch:  220 , cost:  2.052775 , accuracy:  0.75\n",
      "epoch:  221 , cost:  2.045309 , accuracy:  0.75\n",
      "epoch:  222 , cost:  2.0379202 , accuracy:  0.75\n",
      "epoch:  223 , cost:  2.0306082 , accuracy:  0.75\n",
      "epoch:  224 , cost:  2.0233727 , accuracy:  0.75\n",
      "epoch:  225 , cost:  2.0162134 , accuracy:  0.75\n",
      "epoch:  226 , cost:  2.0091295 , accuracy:  0.75\n",
      "epoch:  227 , cost:  2.0021205 , accuracy:  0.75\n",
      "epoch:  228 , cost:  1.9951857 , accuracy:  0.75\n",
      "epoch:  229 , cost:  1.9883245 , accuracy:  0.75\n",
      "epoch:  230 , cost:  1.9815358 , accuracy:  0.75\n",
      "epoch:  231 , cost:  1.9748193 , accuracy:  0.75\n",
      "epoch:  232 , cost:  1.9681733 , accuracy:  0.75\n",
      "epoch:  233 , cost:  1.9615973 , accuracy:  0.75\n",
      "epoch:  234 , cost:  1.95509 , accuracy:  0.75\n",
      "epoch:  235 , cost:  1.9486498 , accuracy:  0.75\n",
      "epoch:  236 , cost:  1.9422755 , accuracy:  0.75\n",
      "epoch:  237 , cost:  1.9359664 , accuracy:  0.75\n",
      "epoch:  238 , cost:  1.92972 , accuracy:  0.75\n",
      "epoch:  239 , cost:  1.9235351 , accuracy:  0.75\n",
      "epoch:  240 , cost:  1.9174097 , accuracy:  0.75\n",
      "epoch:  241 , cost:  1.9113429 , accuracy:  0.75\n",
      "epoch:  242 , cost:  1.9053304 , accuracy:  0.75\n",
      "epoch:  243 , cost:  1.8993721 , accuracy:  0.75\n",
      "epoch:  244 , cost:  1.8934648 , accuracy:  0.75\n",
      "epoch:  245 , cost:  1.8876063 , accuracy:  0.75\n",
      "epoch:  246 , cost:  1.8817928 , accuracy:  0.75\n",
      "epoch:  247 , cost:  1.8760221 , accuracy:  0.75\n",
      "epoch:  248 , cost:  1.870291 , accuracy:  0.75\n",
      "epoch:  249 , cost:  1.8645958 , accuracy:  0.75\n",
      "epoch:  250 , cost:  1.858933 , accuracy:  0.75\n",
      "epoch:  251 , cost:  1.8532975 , accuracy:  0.75\n",
      "epoch:  252 , cost:  1.8476858 , accuracy:  0.75\n",
      "epoch:  253 , cost:  1.842092 , accuracy:  0.75\n",
      "epoch:  254 , cost:  1.8365119 , accuracy:  0.75\n",
      "epoch:  255 , cost:  1.8309383 , accuracy:  0.75\n",
      "epoch:  256 , cost:  1.8253663 , accuracy:  0.75\n",
      "epoch:  257 , cost:  1.8197861 , accuracy:  0.75\n",
      "epoch:  258 , cost:  1.8141928 , accuracy:  0.75\n",
      "epoch:  259 , cost:  1.8085757 , accuracy:  0.75\n",
      "epoch:  260 , cost:  1.8029267 , accuracy:  0.75\n",
      "epoch:  261 , cost:  1.7972339 , accuracy:  0.75\n",
      "epoch:  262 , cost:  1.7914859 , accuracy:  0.75\n",
      "epoch:  263 , cost:  1.78567 , accuracy:  0.75\n",
      "epoch:  264 , cost:  1.7797716 , accuracy:  0.75\n",
      "epoch:  265 , cost:  1.7737738 , accuracy:  0.75\n",
      "epoch:  266 , cost:  1.7676595 , accuracy:  0.75\n",
      "epoch:  267 , cost:  1.7614083 , accuracy:  0.75\n",
      "epoch:  268 , cost:  1.7549971 , accuracy:  0.75\n",
      "epoch:  269 , cost:  1.7484014 , accuracy:  0.75\n",
      "epoch:  270 , cost:  1.7415936 , accuracy:  0.75\n",
      "epoch:  271 , cost:  1.7345419 , accuracy:  0.75\n",
      "epoch:  272 , cost:  1.7272128 , accuracy:  0.75\n",
      "epoch:  273 , cost:  1.7195683 , accuracy:  0.75\n",
      "epoch:  274 , cost:  1.7115651 , accuracy:  0.75\n",
      "epoch:  275 , cost:  1.7031598 , accuracy:  0.75\n",
      "epoch:  276 , cost:  1.6943011 , accuracy:  0.75\n",
      "epoch:  277 , cost:  1.6849365 , accuracy:  0.75\n",
      "epoch:  278 , cost:  1.6750107 , accuracy:  0.75\n",
      "epoch:  279 , cost:  1.6644648 , accuracy:  0.75\n",
      "epoch:  280 , cost:  1.65324 , accuracy:  0.75\n",
      "epoch:  281 , cost:  1.6412795 , accuracy:  0.75\n",
      "epoch:  282 , cost:  1.6285299 , accuracy:  0.75\n",
      "epoch:  283 , cost:  1.6149442 , accuracy:  0.75\n",
      "epoch:  284 , cost:  1.6004862 , accuracy:  0.75\n",
      "epoch:  285 , cost:  1.585134 , accuracy:  0.75\n",
      "epoch:  286 , cost:  1.5688844 , accuracy:  0.75\n",
      "epoch:  287 , cost:  1.5517539 , accuracy:  0.75\n",
      "epoch:  288 , cost:  1.533781 , accuracy:  0.75\n",
      "epoch:  289 , cost:  1.5150274 , accuracy:  0.75\n",
      "epoch:  290 , cost:  1.4955747 , accuracy:  0.75\n",
      "epoch:  291 , cost:  1.4755201 , accuracy:  1.0\n",
      "epoch:  292 , cost:  1.4549744 , accuracy:  1.0\n",
      "epoch:  293 , cost:  1.4340523 , accuracy:  1.0\n",
      "epoch:  294 , cost:  1.412869 , accuracy:  1.0\n",
      "epoch:  295 , cost:  1.3915336 , accuracy:  1.0\n",
      "epoch:  296 , cost:  1.3701463 , accuracy:  1.0\n",
      "epoch:  297 , cost:  1.3487952 , accuracy:  1.0\n",
      "epoch:  298 , cost:  1.3275554 , accuracy:  1.0\n",
      "epoch:  299 , cost:  1.3064888 , accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "epochs = 300\n",
    "\n",
    "for e in range(epochs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        prob = model(x_data)\n",
    "        cost = loss(prob, y_data)\n",
    "\n",
    "    # 파라미터 업데이트\n",
    "    sgd.minimize(cost, var_list=[W1, b1, W2, b2], tape=tape)\n",
    "    acc = compute_acc(y_data, prob)\n",
    "    \n",
    "    print(\"epoch: \", e, \", cost: \", tf.reduce_mean(cost).numpy(), \", accuracy: \", acc)\n",
    "\n",
    "    with train_summary_writer.as_default(): #텐서 보드\n",
    "        tf.summary.scalar('loss', cost, step=e)\n",
    "        tf.summary.scalar('accuracy', acc, step=e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recsys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
