{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN practice\n",
    "\n",
    "1. simple CNN\n",
    "2. deep CNN\n",
    "3. ?\n",
    "4. ?\n",
    "5. ensemble\n",
    "6. low memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-03 15:07:48.825662: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-03 15:07:48.941310: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-07-03 15:07:49.597818: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda-11.7/lib64:/usr/local/cuda/extras/CUPTI/:/usr/local/cuda-11.7/lib64:/usr/local/cuda/extras/CUPTI/\n",
      "2023-07-03 15:07:49.597926: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda-11.7/lib64:/usr/local/cuda/extras/CUPTI/:/usr/local/cuda-11.7/lib64:/usr/local/cuda/extras/CUPTI/\n",
      "2023-07-03 15:07:49.597935: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from mnist import MNIST"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28, 1), (60000,), (10000, 28, 28, 1), (10000,))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mndata = MNIST('../data/mnist/')\n",
    "\n",
    "train_images, train_labels = mndata.load_training()\n",
    "test_images, test_labels = mndata.load_testing()\n",
    "\n",
    "train_images = np.array(train_images).reshape((-1, 28, 28, 1))\n",
    "train_labels = np.array(train_labels)\n",
    "test_images = np.array(test_images).reshape((-1, 28, 28, 1))\n",
    "test_labels = np.array(test_labels)\n",
    "\n",
    "train_images.shape, train_labels.shape, test_images.shape, test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = 0\n",
    "\n",
    "plt.title(train_labels[0])\n",
    "plt.imshow(train_images[0], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-03 15:07:55.318002: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-03 15:07:55.318274: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-03 15:07:55.351389: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-03 15:07:55.351642: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-03 15:07:55.351811: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-03 15:07:55.351975: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-03 15:07:55.353250: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-03 15:07:55.555236: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-03 15:07:55.555488: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-03 15:07:55.555703: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-03 15:07:55.555865: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-03 15:07:55.556030: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-03 15:07:55.556185: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-03 15:07:56.672796: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-03 15:07:56.673086: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-03 15:07:56.673294: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-03 15:07:56.673472: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-03 15:07:56.673639: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-03 15:07:56.673809: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30961 MB memory:  -> device: 0, name: Tesla V100S-PCIE-32GB, pci bus id: 0000:00:06.0, compute capability: 7.0\n",
      "2023-07-03 15:07:56.674333: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-03 15:07:56.674509: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 30961 MB memory:  -> device: 1, name: Tesla V100S-PCIE-32GB, pci bus id: 0000:00:07.0, compute capability: 7.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(TensorShape([60000, 10]), TensorShape([10000, 10]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels_1hot = tf.one_hot(train_labels, 10)\n",
    "test_labels_1hot = tf.one_hot(test_labels, 10)\n",
    "\n",
    "train_labels_1hot.shape, test_labels_1hot.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((600, 100, 28, 28, 1), (600, 100, 10), (100, 100, 28, 28, 1), (100, 100, 10))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 100\n",
    "\n",
    "train_batch_images = np.array([train_images[idx: idx+100] for idx in range(train_images.shape[0]//batch_size)])\n",
    "train_batch_labels = np.array([train_labels_1hot[idx: idx+100] for idx in range(train_images.shape[0]//batch_size)])\n",
    "test_batch_images = np.array([test_images[idx: idx+100] for idx in range(test_images.shape[0]//batch_size)])\n",
    "test_batch_labels = np.array([test_labels_1hot[idx: idx+100] for idx in range(test_images.shape[0]//batch_size)])\n",
    "\n",
    "\n",
    "train_batch_images.shape, train_batch_labels.shape, test_batch_images.shape, test_batch_labels.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple CNN\n",
    "\n",
    "5 layers (last FC-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(x, filter_nums, dropout_rate, name, weights=None):\n",
    "    if isinstance(weights, np.ndarray):\n",
    "        filters = weights\n",
    "    else:\n",
    "        filters = tf.Variable(tf.random.normal([3, 3, 1, filter_nums]))\n",
    "    conv_output = tf.nn.conv2d(x, filters, (1,1,1,1), 'SAME', name=name)\n",
    "    relu_output = tf.nn.relu(conv_output)\n",
    "    pool_output = tf.nn.max_pool2d(\n",
    "        relu_output, ksize=(1,2,2,1), strides=(1,2,2,1), padding='SAME')\n",
    "    block_output = tf.nn.dropout(pool_output, rate=dropout_rate)\n",
    "    return block_output\n",
    "\n",
    "def fc_layer(x, units, weights=None):\n",
    "    if not isinstance(weights, np.ndarray):\n",
    "        weights = tf.Variable(tf.random.normal((x.shape[-1], units)))\n",
    "    fc_output = tf.nn.relu(tf.matmul(x, weights))#+b)\n",
    "    return fc_output\n",
    "\n",
    "def last_layer(x, classes, weights=None):\n",
    "    if not isinstance(weights, np.ndarray):\n",
    "        weights = tf.Variable(tf.random.normal((x.shape[-1], classes)))\n",
    "    logits = tf.matmul(x, weights)\n",
    "    return logits\n",
    "\n",
    "weights = [\n",
    "    tf.Variable(tf.random.normal([3, 3, 1, 32])),\n",
    "    tf.Variable(tf.random.normal([3, 3, 32, 64])),\n",
    "    tf.Variable(tf.random.normal([3, 3, 64, 128])),\n",
    "    tf.Variable(tf.random.normal((2048, 4096))),\n",
    "    tf.Variable(tf.random.normal((4096, 4096))),\n",
    "    tf.Variable(tf.random.normal((16777216, 10))),\n",
    "]\n",
    "    \n",
    "\n",
    "def model(x, dropout_rate):\n",
    "\n",
    "    # convolution blocks\n",
    "    l1_output = conv_block(x, 32, dropout_rate, 'conv1', weights=weights[0])\n",
    "    print(\"l1 output shape: \", l1_output.shape)\n",
    "    l2_output = conv_block(l1_output, 64, dropout_rate, 'conv2', weights=weights[1])\n",
    "    print(\"l2 output shape: \", l2_output.shape)\n",
    "    l3_output = conv_block(l2_output, 128, dropout_rate, 'conv3', weights=weights[2])\n",
    "    print(\"l3 output shape: \", l3_output.shape)\n",
    "\n",
    "    # fc layers\n",
    "    flatten = tf.reshape(l3_output, (x.shape[0], -1))\n",
    "    print(\"flatten output shape: \", flatten.shape)\n",
    "    l4_output = fc_layer(flatten, 4096, weights=weights[3])\n",
    "    print(\"l4 output shape: \", l4_output.shape)\n",
    "    l5_output = fc_layer(l4_output, 4096, weights=weights[4])\n",
    "    print(\"l5 output shape: \", l5_output.shape)\n",
    "    l6_output = last_layer(l5_output, 10, weights=weights[5])\n",
    "    print(\"l6 output shape: \", l6_output.shape)\n",
    "\n",
    "    return l6_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-03 15:08:05.909893: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l1 output shape:  (10000, 14, 14, 32)\n",
      "l2 output shape:  (10000, 7, 7, 64)\n",
      "l3 output shape:  (10000, 4, 4, 128)\n",
      "flatten output shape:  (10000, 2048)\n",
      "l4 output shape:  (10000, 4096)\n",
      "l5 output shape:  (10000, 4096)\n",
      "l6 output shape:  (10000, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10000, 10), dtype=float32, numpy=\n",
       "array([[-1.63816064e+08,  9.59083200e+06, -1.13485680e+08, ...,\n",
       "        -4.03661696e+08,  5.93513200e+07, -8.09990400e+07],\n",
       "       [-1.23288288e+08, -2.46083408e+08, -3.19826240e+08, ...,\n",
       "        -2.53948256e+08, -5.12824000e+05, -6.39689200e+06],\n",
       "       [ 7.95191280e+07,  1.71299240e+07, -8.00841760e+07, ...,\n",
       "        -2.55723584e+08,  1.23938984e+08, -1.39868440e+07],\n",
       "       ...,\n",
       "       [-3.42086400e+06, -4.60014400e+07, -2.21364960e+08, ...,\n",
       "        -4.75228672e+08,  1.60268464e+08, -1.26697408e+08],\n",
       "       [ 1.67859584e+08, -7.78992640e+07, -2.43200600e+06, ...,\n",
       "        -3.46797280e+08,  1.56494480e+08, -1.08418304e+08],\n",
       "       [-1.61473600e+07, -1.28449712e+08, -1.65830688e+08, ...,\n",
       "        -5.39381056e+08,  5.27235600e+07, -1.73937312e+08]], dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(test_images, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "sgd = tf.keras.optimizers.SGD(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-03 15:08:13.461564: W tensorflow/core/kernels/gpu_utils.cc:50] Failed to allocate memory for convolution redzone checking; skipping this check. This is benign and only means that we won't check cudnn for out-of-bounds reads and writes. This message will only be printed once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l1 output shape:  (60000, 14, 14, 32)\n",
      "l2 output shape:  (60000, 7, 7, 64)\n",
      "l3 output shape:  (60000, 4, 4, 128)\n",
      "flatten output shape:  (60000, 2048)\n",
      "l4 output shape:  (60000, 4096)\n",
      "l5 output shape:  (60000, 4096)\n",
      "l6 output shape:  (60000, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-03 15:08:25.355633: W tensorflow/tsl/framework/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 6.90GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-07-03 15:08:25.355851: W tensorflow/tsl/framework/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 6.90GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No gradients provided for any variable: (['Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0'],). Provided `grads_and_vars` is ((None, <tf.Variable 'Variable:0' shape=(3, 3, 1, 32) dtype=float32, numpy=\narray([[[[ 0.6466784 ,  1.3636801 ,  0.2349704 , -0.68872184,\n           1.2708534 , -1.5039062 , -0.791255  , -0.93443525,\n          -0.81759375,  1.4529178 , -0.93918484, -1.431791  ,\n           0.19984002,  0.23216824, -2.6451733 , -1.4437139 ,\n           0.92665076,  1.3199555 , -0.17193408, -0.8771541 ,\n           0.250492  , -0.1008746 , -0.8012736 , -0.09100357,\n           0.25474587,  0.5349384 , -0.04049704,  0.7610705 ,\n          -2.3481848 ,  0.4639459 ,  0.67247576,  0.22725704]],\n\n        [[ 0.07807124,  0.44073853,  1.9378729 , -0.52310944,\n           1.4352068 , -0.53696984,  0.51148885, -0.36410838,\n          -0.5732031 ,  1.829762  ,  0.14513761,  1.0515347 ,\n          -0.68788207,  0.45367172, -1.7454324 ,  1.1318425 ,\n          -0.4222951 , -0.3760836 ,  0.0555972 , -0.21359812,\n          -1.0526816 ,  0.09952651, -1.172967  ,  0.08245756,\n           0.27712423,  0.08589459, -0.7428122 ,  0.3805969 ,\n           0.33666667,  0.83065766, -0.34107158,  0.34638366]],\n\n        [[-1.5269016 , -0.0960945 ,  0.07590263, -0.54164255,\n          -0.10711495, -1.380478  ,  1.0823843 ,  0.15391645,\n          -0.924603  ,  0.18526798, -1.1310121 ,  0.2559383 ,\n          -0.66276866,  0.68127084, -0.75414395, -0.07397736,\n           0.5319093 , -0.71319413, -0.26535887,  0.2075439 ,\n          -0.58865744, -0.9440555 , -0.88900894, -0.911177  ,\n           0.65423936,  0.05467885, -0.9757518 , -1.5119768 ,\n          -0.6218669 , -0.08284172, -0.51931447, -1.9477992 ]]],\n\n\n       [[[ 0.09281781,  0.31672472, -0.15312561,  0.6576407 ,\n          -0.06364119,  0.34230626,  0.09905884, -1.5473095 ,\n          -2.164004  , -1.163612  , -0.6128797 , -0.92909676,\n          -0.19283934,  1.3884772 , -0.33209106, -0.37142402,\n          -0.8569481 ,  1.3128698 , -0.5343361 , -0.44202265,\n          -0.28659397, -0.9150159 ,  1.0567747 ,  0.20239772,\n           0.15196304,  1.8412681 ,  1.4837056 ,  0.0178719 ,\n          -0.0156993 , -0.14400351,  1.2510555 ,  0.0341127 ]],\n\n        [[-1.8503035 ,  1.8630394 , -0.8387479 , -0.99032676,\n           0.88508016,  0.5592865 , -0.02534952,  0.11326825,\n          -0.9367322 , -0.6940385 ,  1.6300179 ,  0.56942695,\n          -0.1246592 , -0.2373819 ,  0.08806559, -0.35537207,\n           0.26375145,  1.5500369 , -1.231266  ,  1.4507219 ,\n           1.2303547 , -0.5123373 , -0.5643674 , -0.8016935 ,\n          -0.32689306,  0.5819346 , -0.4012869 ,  0.6183318 ,\n           0.5949261 , -0.49499986,  1.1726246 , -0.36204967]],\n\n        [[-1.0130643 , -0.11574296,  0.2570927 ,  0.19586007,\n           1.9387752 , -0.606752  , -2.0620928 , -0.58435684,\n          -0.3675099 ,  0.671357  , -1.0613112 , -0.10118797,\n          -1.6306659 , -1.4144033 ,  0.7392334 , -0.09799367,\n          -0.06009875, -0.01182518,  1.5021986 ,  1.0187073 ,\n           0.24639466, -0.6482466 , -0.09944835, -0.21933323,\n          -0.95047796, -0.15839301, -0.5509325 ,  0.84376884,\n          -1.1356637 ,  0.69268423,  0.31964287, -1.2675667 ]]],\n\n\n       [[[-0.80905783,  0.8667199 , -0.25556275,  0.8523271 ,\n          -0.48429334, -0.37295645, -0.64549637,  0.43377215,\n           0.21906446, -1.2036375 ,  0.52927613, -1.050511  ,\n           0.7938159 ,  0.11210593, -0.38645473, -0.41139477,\n          -0.9570582 , -1.6779778 ,  0.695617  ,  0.31683645,\n          -1.4159051 , -1.5264475 , -1.3047926 , -0.5657302 ,\n           0.18516006,  0.97149384,  0.6025923 ,  1.5755728 ,\n          -0.25813797,  0.44717535,  0.06383213, -0.2831915 ]],\n\n        [[-1.7273619 ,  2.3258348 , -0.34049243,  0.37726212,\n           0.57444376, -0.39842898,  0.8879237 ,  0.31475452,\n          -1.8742999 , -1.6084414 ,  0.46315092, -1.3360512 ,\n          -0.45779213, -0.3701041 ,  0.61123586, -1.5635326 ,\n          -0.4760754 , -1.2992508 , -1.0136776 ,  0.9237997 ,\n           0.67889476,  1.4121991 ,  0.15044291,  1.1735282 ,\n          -0.12994803, -0.40697154,  0.0719725 , -0.19998203,\n          -0.0301965 , -0.5427759 ,  1.5363353 ,  0.9429556 ]],\n\n        [[-0.33671847,  1.3157446 ,  0.480834  , -1.2014211 ,\n          -1.0534788 ,  0.27704492,  0.7215707 ,  0.938482  ,\n          -0.04459808, -0.16646737, -0.31469342,  0.78269255,\n          -0.95669174, -0.3777315 ,  2.0498457 , -2.0977733 ,\n          -1.5962831 ,  0.28712907,  2.7829552 , -1.0619161 ,\n          -0.6878612 ,  0.37806195,  1.3146728 , -0.10095701,\n           0.36397767,  0.34573236,  0.8799807 , -2.338214  ,\n           0.49898982,  0.26629388,  0.8019123 ,  1.2743303 ]]]],\n      dtype=float32)>), (None, <tf.Variable 'Variable:0' shape=(3, 3, 32, 64) dtype=float32, numpy=\narray([[[[-1.28501177e+00,  2.21001297e-01, -1.37426555e+00, ...,\n          -7.44226277e-01, -9.01089236e-02,  1.89547133e+00],\n         [ 1.73789752e+00, -1.28992593e+00,  1.06083751e+00, ...,\n          -2.66165912e-01,  7.71940112e-01,  6.72777832e-01],\n         [-9.56282794e-01,  2.73842901e-01, -2.83352554e-01, ...,\n          -6.81241751e-01, -1.31792462e+00, -2.04445437e-01],\n         ...,\n         [-2.27847815e-01,  2.79485703e-01,  1.22582293e+00, ...,\n           7.80237913e-01, -7.70506680e-01, -1.97411627e-01],\n         [-1.78352848e-01,  8.35640788e-01, -2.52330065e-01, ...,\n           4.80343681e-03, -3.38507563e-01,  1.60697401e+00],\n         [-3.83276939e-01, -1.60952067e+00,  2.42307987e-02, ...,\n          -7.19320998e-02,  7.17415333e-01,  7.48562574e-01]],\n\n        [[ 1.74640489e+00, -1.36689615e+00,  3.07348445e-02, ...,\n           1.89505562e-01,  3.25365514e-01,  6.95929885e-01],\n         [-2.58481950e-01, -2.76109487e-01, -6.57977462e-01, ...,\n           4.59287524e-01, -6.16989017e-01,  6.87181175e-01],\n         [-1.85145330e+00,  4.10361409e-01,  1.00966406e+00, ...,\n           3.64643186e-01, -1.56013739e+00,  1.02543700e+00],\n         ...,\n         [ 1.21084666e-02, -3.81899476e-01,  2.83397675e-01, ...,\n          -2.53390819e-01,  1.99577296e+00, -7.69565940e-01],\n         [ 9.05332386e-01, -1.18099320e+00, -1.12758732e+00, ...,\n          -6.83032632e-01, -1.12046218e+00,  4.43460017e-01],\n         [-1.26735258e+00,  7.43613780e-01,  5.62835336e-01, ...,\n           9.49347675e-01,  7.06774831e-01,  6.03103220e-01]],\n\n        [[ 2.03611404e-01,  7.58214593e-02,  6.95457822e-03, ...,\n           8.46060753e-01,  1.03755224e+00, -2.07883655e-03],\n         [ 4.37285274e-01, -9.67204347e-02,  7.23015547e-01, ...,\n           1.40392435e+00,  1.46620274e+00, -6.98349357e-01],\n         [ 5.68565607e-01,  1.01817906e-01,  2.43913159e-01, ...,\n           7.95711339e-01,  6.75419927e-01,  1.53690740e-01],\n         ...,\n         [-6.52542263e-02,  1.11705661e+00,  1.25255966e+00, ...,\n          -1.21668458e+00, -7.80488551e-01,  1.33168280e+00],\n         [-2.92690784e-01, -8.48050654e-01,  2.51032615e+00, ...,\n          -1.04444051e+00, -6.02462649e-01, -2.07154226e+00],\n         [-4.88085985e-01,  3.60181391e-01, -1.11222577e+00, ...,\n           1.79422820e+00,  8.90368223e-01, -2.17254549e-01]]],\n\n\n       [[[ 5.66388190e-01,  5.52431226e-01, -1.06032372e+00, ...,\n           4.56092596e-01, -2.98385471e-01, -5.66740692e-01],\n         [ 2.51531512e-01, -1.98542383e-02, -2.35990620e+00, ...,\n          -2.03288943e-01,  9.82225418e-01, -4.97511894e-01],\n         [ 1.92048177e-02, -1.10779274e+00, -9.78298366e-01, ...,\n          -1.98955989e+00, -7.91188776e-02,  1.49958074e+00],\n         ...,\n         [ 7.49038756e-01,  4.85949188e-01, -3.97529900e-01, ...,\n           7.06884742e-01,  7.87809432e-01,  5.72056293e-01],\n         [ 5.58984280e-01,  3.48396271e-01,  1.01750338e+00, ...,\n           1.15113187e+00, -6.43325269e-01,  1.45859087e+00],\n         [ 1.02705218e-01,  7.51789689e-01, -4.87259388e-01, ...,\n           2.55517793e+00, -1.34734058e+00,  1.16846752e+00]],\n\n        [[-1.63249508e-01,  2.30371571e+00,  1.23100948e+00, ...,\n           9.77602184e-01,  5.49855351e-01,  1.31390858e+00],\n         [-6.36480272e-01, -4.88942385e-01, -5.80457270e-01, ...,\n           1.95314869e-01,  1.45450675e+00,  1.43101490e+00],\n         [-2.25590020e-01,  6.54116452e-01,  2.85436082e+00, ...,\n           2.47810006e-01, -8.95251513e-01, -3.72122496e-01],\n         ...,\n         [ 5.86112618e-01,  8.30173612e-01,  8.04925084e-01, ...,\n           4.59570229e-01,  9.66685772e-01, -4.50384229e-01],\n         [ 1.91618234e-01,  3.11860472e-01,  5.44516325e-01, ...,\n           3.15398395e-01, -1.78454518e+00,  2.30853662e-01],\n         [-2.05887055e+00, -1.42848730e-01,  2.95713991e-01, ...,\n           9.60756242e-01, -1.59561205e+00, -7.14043796e-01]],\n\n        [[-5.59556782e-01,  1.19380081e+00, -6.16898239e-01, ...,\n           6.91363037e-01, -2.61935860e-01,  1.29656661e+00],\n         [-1.13740611e+00, -1.11326313e+00, -1.13228655e+00, ...,\n           1.22339815e-01,  9.23173845e-01,  1.26417649e+00],\n         [ 1.60155237e-01, -1.07735980e+00,  6.75862312e-01, ...,\n          -3.56934905e-01, -1.21129811e+00,  2.20156479e+00],\n         ...,\n         [ 2.29946375e+00,  7.18432963e-01,  8.40500951e-01, ...,\n          -6.99656248e-01,  5.93610108e-01,  5.54241352e-02],\n         [ 7.92913973e-01, -1.10270572e+00, -1.94935203e+00, ...,\n           4.39173967e-01, -1.58008528e+00,  1.31170952e+00],\n         [ 1.01542366e+00, -5.94914198e-01,  1.94248885e-01, ...,\n          -4.80923131e-02,  1.81735128e-01,  8.80310655e-01]]],\n\n\n       [[[-4.99432415e-01, -8.21795389e-02,  2.74607778e-01, ...,\n           2.67639428e-01,  5.24476171e-01,  1.67700812e-01],\n         [-5.10142706e-02, -5.11534931e-03,  1.08154678e+00, ...,\n           5.95132947e-01, -6.60834134e-01, -9.80324686e-01],\n         [ 2.61661440e-01,  2.26042295e+00, -6.68971181e-01, ...,\n          -5.02248108e-01,  6.37900352e-01,  1.80661559e-01],\n         ...,\n         [-4.87568527e-01,  1.63645661e+00,  9.98283252e-02, ...,\n          -1.03419411e+00, -4.67379332e-01, -2.93497038e+00],\n         [-6.87842369e-01,  1.44466412e+00, -1.00083327e+00, ...,\n           4.60774601e-01,  1.77842766e-01,  1.02879369e+00],\n         [-2.31635243e-01,  2.50851393e+00, -1.49452639e+00, ...,\n           3.60352576e-01, -3.48015666e-01, -6.29485905e-01]],\n\n        [[ 1.14421999e+00, -5.10448813e-01, -3.91618758e-01, ...,\n          -7.21642077e-01, -1.51449889e-01, -1.64317489e-01],\n         [ 6.46852627e-02, -1.51381299e-01, -1.25070393e+00, ...,\n           6.31057978e-01, -1.58391738e+00,  1.11950660e+00],\n         [-4.01114486e-02,  9.87593755e-02,  1.36637902e+00, ...,\n          -7.25611448e-01,  1.43486941e+00, -8.32441211e-01],\n         ...,\n         [-2.63060302e-01, -1.48743796e+00,  4.01194155e-01, ...,\n          -1.04725742e+00, -1.18656561e-01,  1.58995032e-01],\n         [-5.28152324e-02,  7.22421229e-01,  9.43726376e-02, ...,\n          -5.82612276e-01, -9.56169590e-02, -1.02376628e+00],\n         [ 2.54276609e+00,  1.06798506e+00, -2.34502256e-01, ...,\n           4.90798354e-02, -3.00816238e-01, -1.93705845e+00]],\n\n        [[ 6.01700425e-01, -1.07160544e+00,  1.77797055e+00, ...,\n           1.70212722e+00,  6.23772442e-01,  2.59412318e-01],\n         [ 5.91366217e-02,  1.17788613e-01, -5.94481587e-01, ...,\n           3.91842723e-01,  1.88633993e-01,  3.74266833e-01],\n         [-1.51539779e+00, -1.07414007e+00, -5.71994185e-01, ...,\n          -4.74832922e-01, -1.80356479e+00,  7.09669828e-01],\n         ...,\n         [ 1.01923192e+00, -2.65792787e-01, -9.21754956e-01, ...,\n           2.44235921e+00,  4.34302092e-01, -2.03685474e+00],\n         [-1.00646071e-01, -1.88738561e+00, -1.57680050e-01, ...,\n           2.19879761e-01, -8.09267461e-01, -9.47671011e-02],\n         [ 4.97202247e-01,  1.97922444e+00,  1.67721021e+00, ...,\n           1.61690378e+00,  2.80394584e-01,  2.39872003e+00]]]],\n      dtype=float32)>), (None, <tf.Variable 'Variable:0' shape=(3, 3, 64, 128) dtype=float32, numpy=\narray([[[[ 1.3233073 ,  1.8064544 ,  1.2181501 , ...,  1.4138821 ,\n          -0.7718814 , -2.8054655 ],\n         [ 0.59898347, -0.5681487 , -1.2306    , ..., -0.26275933,\n          -2.212186  , -0.11318617],\n         [ 0.7224077 ,  0.06756593,  0.49214745, ..., -0.8811041 ,\n           2.0311434 , -0.20555237],\n         ...,\n         [ 1.6860093 , -1.311523  , -0.80788857, ..., -0.988668  ,\n          -1.5223459 ,  0.43591508],\n         [-0.4165783 ,  1.2000271 , -0.19116594, ..., -0.5828922 ,\n          -0.6342656 , -1.5716248 ],\n         [-1.0388817 , -0.49023685, -0.04606558, ..., -0.58129495,\n          -0.43218365,  0.4670838 ]],\n\n        [[ 0.94176763, -0.6309817 , -1.3777807 , ...,  1.1745195 ,\n          -0.78341216, -0.9645744 ],\n         [ 0.9940354 ,  0.0953831 ,  0.94416046, ...,  1.450422  ,\n           1.4965519 , -1.846254  ],\n         [ 1.6381598 , -0.19217548, -0.15104827, ...,  0.22620176,\n          -1.2182889 ,  0.7811105 ],\n         ...,\n         [-1.3768874 , -0.5630665 , -0.8562116 , ..., -0.17876022,\n           2.800444  , -0.18243377],\n         [ 0.23891221, -0.25161123, -0.03095458, ...,  0.02137929,\n           0.3498833 , -0.04577332],\n         [ 0.8274535 ,  1.1673629 , -0.8575508 , ...,  0.32776988,\n          -1.1606731 , -2.0019898 ]],\n\n        [[ 0.11348422, -1.3422955 ,  1.4626156 , ..., -2.4061184 ,\n          -1.0291145 ,  1.2351063 ],\n         [ 0.4678401 ,  0.38888273, -1.2453578 , ...,  1.4669999 ,\n           1.0906198 ,  0.48497868],\n         [-0.48183277, -2.0633283 , -0.6981974 , ..., -0.17239378,\n          -0.23128591,  0.8199734 ],\n         ...,\n         [ 1.7617092 ,  0.71536607,  0.6602622 , ..., -1.2144499 ,\n           2.331084  ,  0.07272591],\n         [-0.68717813,  1.4124928 , -0.12826145, ...,  0.4896559 ,\n           1.108442  ,  0.21481436],\n         [ 1.9387612 , -0.54419434,  0.98810196, ..., -0.19792007,\n          -1.4844624 ,  0.5702481 ]]],\n\n\n       [[[-0.37527004, -0.5415016 , -1.4366795 , ...,  0.8084037 ,\n          -0.11347638,  0.10058681],\n         [-0.8631066 ,  0.92784756, -0.46636513, ..., -1.0522844 ,\n           0.32450733, -0.1340888 ],\n         [ 0.5145735 ,  1.1600758 , -1.0015857 , ..., -0.34105203,\n          -0.24955966,  0.37508315],\n         ...,\n         [-2.2977474 ,  0.6054437 , -1.0540564 , ..., -1.6177864 ,\n           0.66252387,  0.49018764],\n         [ 0.8196066 , -0.6957084 ,  0.47732154, ...,  0.8105063 ,\n          -0.31021813, -0.86657584],\n         [ 1.1024907 ,  0.2810599 , -1.325965  , ..., -0.09398641,\n           0.62665004,  1.5063751 ]],\n\n        [[ 0.61357194, -0.6505677 ,  0.35216728, ...,  0.3819855 ,\n           1.5928088 , -0.9366353 ],\n         [-1.7234296 ,  0.8455515 ,  1.1481297 , ...,  0.5267668 ,\n          -0.9692933 , -1.195038  ],\n         [ 0.97974855, -0.8383894 ,  0.23320137, ..., -0.5704547 ,\n           0.30827522,  0.6087184 ],\n         ...,\n         [-0.514214  , -2.1994634 ,  0.3310736 , ..., -0.31598493,\n          -0.12177974,  1.4911239 ],\n         [-0.84602004,  0.68697983, -0.6419436 , ...,  0.5377825 ,\n           0.07640764, -0.42117974],\n         [-0.8744789 , -2.2752192 , -1.4658711 , ...,  1.3513074 ,\n          -1.9683368 , -1.1780033 ]],\n\n        [[ 0.57536733, -0.2511476 ,  0.5785398 , ...,  0.04475286,\n          -0.2982519 , -0.08370461],\n         [ 1.1266229 , -0.61560273,  0.34671578, ...,  0.6114166 ,\n          -0.64726585, -0.23780075],\n         [-0.03091933, -0.7379663 , -0.7063694 , ...,  0.84956455,\n          -1.0647038 , -1.0612644 ],\n         ...,\n         [-0.517869  ,  0.3420521 , -0.9193096 , ...,  1.7494043 ,\n           0.15477942, -0.94548184],\n         [-1.1123685 ,  0.00855371,  0.4165032 , ...,  0.08317707,\n           0.25753084,  0.57715577],\n         [ 0.7367319 , -0.59352654,  0.5554434 , ...,  1.2634659 ,\n           0.11332662, -0.8425391 ]]],\n\n\n       [[[ 0.99552125,  1.1221021 , -0.26882127, ..., -0.18767463,\n          -0.302475  ,  0.714045  ],\n         [ 1.1835625 , -0.2894701 , -0.3201054 , ..., -0.6023518 ,\n           0.1169323 ,  2.0028248 ],\n         [ 1.0045099 ,  0.24113937,  0.8757464 , ...,  0.36643156,\n           0.37209225, -1.5541159 ],\n         ...,\n         [-1.8474052 ,  1.4149529 , -1.8805466 , ..., -0.16018444,\n          -0.13593034,  0.90918756],\n         [ 1.0632154 , -0.8133915 ,  0.06553711, ..., -0.32622653,\n          -0.06655673, -0.69040704],\n         [-0.10322361,  0.24309842, -0.26507849, ..., -0.59569806,\n           0.79297817, -0.3443945 ]],\n\n        [[-0.2862197 , -1.2055069 , -0.7909686 , ..., -1.2329881 ,\n           0.09533951,  1.9988099 ],\n         [ 0.74162245, -1.9307323 ,  0.38849795, ...,  0.5705388 ,\n          -1.0413673 , -0.7350458 ],\n         [-0.64483964,  0.49457783,  1.1962279 , ..., -0.02270497,\n           1.7585356 , -0.55424166],\n         ...,\n         [-0.0251463 , -0.52311254, -1.3967589 , ..., -0.8843045 ,\n          -1.0770103 , -0.34927273],\n         [ 0.6053366 , -0.3187969 , -0.67199343, ...,  0.3371216 ,\n           1.0911036 , -0.25764167],\n         [ 0.21261099,  1.5784614 , -0.6913989 , ...,  0.06362043,\n          -2.5404527 , -0.7977484 ]],\n\n        [[ 1.8318902 ,  1.4868467 ,  1.9670925 , ...,  0.1330779 ,\n           0.6669271 ,  1.646579  ],\n         [-0.7797185 , -1.4560398 , -0.755379  , ..., -0.11187799,\n           1.4000828 , -0.94481117],\n         [-0.5794052 , -1.1983913 ,  0.06539527, ...,  1.1226453 ,\n           1.5906903 , -0.94647425],\n         ...,\n         [ 1.1998713 ,  1.2653767 ,  0.976917  , ...,  0.3518431 ,\n          -0.10479601, -0.55302554],\n         [ 0.2480868 ,  1.0960294 ,  1.3069167 , ...,  1.1822996 ,\n          -2.008866  , -0.7250279 ],\n         [ 0.23299439, -0.59291685,  0.8448248 , ..., -0.16138029,\n          -1.218463  , -0.26266512]]]], dtype=float32)>), (None, <tf.Variable 'Variable:0' shape=(2048, 4096) dtype=float32, numpy=\narray([[-1.0119869 , -0.40464824, -0.5461457 , ...,  0.08166128,\n        -0.7049107 , -0.2257863 ],\n       [-0.9411602 , -0.3384296 ,  0.08271101, ...,  0.07420039,\n        -0.1318149 ,  0.2679172 ],\n       [ 0.29058805,  0.07582489, -0.02803992, ...,  1.1054467 ,\n        -0.35974282,  0.2864533 ],\n       ...,\n       [ 0.2454947 ,  0.09079299,  0.7777444 , ..., -1.847688  ,\n         0.43146208,  1.0686647 ],\n       [ 0.14506939, -1.1283616 , -0.5485785 , ...,  1.425123  ,\n         0.52094734,  0.31447452],\n       [-0.79007554, -0.45471472,  1.7339969 , ..., -0.9152863 ,\n         0.08569179, -0.40256593]], dtype=float32)>), (None, <tf.Variable 'Variable:0' shape=(4096, 4096) dtype=float32, numpy=\narray([[-0.591581  , -1.2791404 ,  0.58100176, ..., -0.9487903 ,\n         1.0802866 , -1.4643636 ],\n       [ 0.04008683, -0.09805308,  1.228761  , ...,  1.2371086 ,\n         0.42483976, -1.2524638 ],\n       [ 0.39137298, -0.8011432 ,  0.92518723, ...,  0.34498268,\n        -3.269524  ,  0.09526349],\n       ...,\n       [ 1.98609   ,  0.4279652 ,  0.24282114, ..., -1.0728173 ,\n         1.0532737 ,  0.69181705],\n       [ 0.8825209 ,  0.88111   ,  0.62725663, ..., -0.18520081,\n         0.40083268, -0.9509889 ],\n       [-0.65724087, -1.2953994 , -0.35906908, ...,  3.1098936 ,\n         0.22726241, -1.4607003 ]], dtype=float32)>), (None, <tf.Variable 'Variable:0' shape=(16777216, 10) dtype=float32, numpy=\narray([[ 2.0216851 , -1.8725924 , -1.798161  , ...,  0.38258243,\n        -0.30104527,  0.19490388],\n       [-1.196796  , -0.14725482,  0.19585314, ..., -0.6332138 ,\n         1.0584236 ,  1.149426  ],\n       [ 1.4368621 , -0.3839124 , -0.27429628, ..., -0.200673  ,\n        -0.06156503,  0.19315498],\n       ...,\n       [ 0.9254828 ,  1.5334361 , -0.1104705 , ...,  1.0014472 ,\n         0.8921647 , -0.52656955],\n       [-0.61447763,  0.18046185,  0.05631652, ...,  1.1166717 ,\n         1.2567182 ,  0.3220139 ],\n       [ 0.15944856,  0.12971127,  0.48683923, ..., -0.1404805 ,\n         0.02118052, -0.7058533 ]], dtype=float32)>)).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[39m# else:\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     \u001b[39m#     pred, weights = model(batch_images, 0.01)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     loss \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39msoftmax_cross_entropy_with_logits(train_labels_1hot, pred)\n\u001b[0;32m---> 13\u001b[0m sgd\u001b[39m.\u001b[39;49mminimize(loss, var_list\u001b[39m=\u001b[39;49mweights, tape\u001b[39m=\u001b[39;49mtape)\n\u001b[1;32m     14\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mepoch: \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39mloss: \u001b[39m\u001b[39m{\u001b[39;00mtf\u001b[39m.\u001b[39mreduce_sum(loss)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[39m# print(f\"epoch: {i}, batches: {b}, loss: {tf.reduce_sum(loss)}\")\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/recommender/lib/python3.10/site-packages/keras/optimizers/optimizer_experimental/optimizer.py:527\u001b[0m, in \u001b[0;36m_BaseOptimizer.minimize\u001b[0;34m(self, loss, var_list, tape)\u001b[0m\n\u001b[1;32m    506\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Minimize `loss` by updating `var_list`.\u001b[39;00m\n\u001b[1;32m    507\u001b[0m \n\u001b[1;32m    508\u001b[0m \u001b[39mThis method simply computes gradient using `tf.GradientTape` and calls\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    524\u001b[0m \u001b[39m  None\u001b[39;00m\n\u001b[1;32m    525\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    526\u001b[0m grads_and_vars \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_gradients(loss, var_list, tape)\n\u001b[0;32m--> 527\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_gradients(grads_and_vars)\n",
      "File \u001b[0;32m~/miniconda3/envs/recommender/lib/python3.10/site-packages/keras/optimizers/optimizer_experimental/optimizer.py:1139\u001b[0m, in \u001b[0;36mOptimizer.apply_gradients\u001b[0;34m(self, grads_and_vars, name, skip_gradients_aggregation, **kwargs)\u001b[0m\n\u001b[1;32m   1135\u001b[0m experimental_aggregate_gradients \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\n\u001b[1;32m   1136\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mexperimental_aggregate_gradients\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m )\n\u001b[1;32m   1138\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m skip_gradients_aggregation \u001b[39mand\u001b[39;00m experimental_aggregate_gradients:\n\u001b[0;32m-> 1139\u001b[0m     grads_and_vars \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maggregate_gradients(grads_and_vars)\n\u001b[1;32m   1140\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mapply_gradients(grads_and_vars, name\u001b[39m=\u001b[39mname)\n",
      "File \u001b[0;32m~/miniconda3/envs/recommender/lib/python3.10/site-packages/keras/optimizers/optimizer_experimental/optimizer.py:1105\u001b[0m, in \u001b[0;36mOptimizer.aggregate_gradients\u001b[0;34m(self, grads_and_vars)\u001b[0m\n\u001b[1;32m   1093\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39maggregate_gradients\u001b[39m(\u001b[39mself\u001b[39m, grads_and_vars):\n\u001b[1;32m   1094\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Aggregate gradients on all devices.\u001b[39;00m\n\u001b[1;32m   1095\u001b[0m \n\u001b[1;32m   1096\u001b[0m \u001b[39m    By default we will perform reduce_sum of gradients across devices. Users\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1103\u001b[0m \u001b[39m      List of (gradient, variable) pairs.\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1105\u001b[0m     \u001b[39mreturn\u001b[39;00m optimizer_utils\u001b[39m.\u001b[39;49mall_reduce_sum_gradients(grads_and_vars)\n",
      "File \u001b[0;32m~/miniconda3/envs/recommender/lib/python3.10/site-packages/keras/optimizers/optimizer_v2/utils.py:33\u001b[0m, in \u001b[0;36mall_reduce_sum_gradients\u001b[0;34m(grads_and_vars)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Returns all-reduced gradients aggregated via summation.\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \n\u001b[1;32m     26\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39m  List of (gradient, variable) pairs where gradients have been all-reduced.\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     32\u001b[0m grads_and_vars \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(grads_and_vars)\n\u001b[0;32m---> 33\u001b[0m filtered_grads_and_vars \u001b[39m=\u001b[39m filter_empty_gradients(grads_and_vars)\n\u001b[1;32m     34\u001b[0m \u001b[39mif\u001b[39;00m filtered_grads_and_vars:\n\u001b[1;32m     35\u001b[0m     \u001b[39mif\u001b[39;00m tf\u001b[39m.\u001b[39m__internal__\u001b[39m.\u001b[39mdistribute\u001b[39m.\u001b[39mstrategy_supports_no_merge_call():\n",
      "File \u001b[0;32m~/miniconda3/envs/recommender/lib/python3.10/site-packages/keras/optimizers/optimizer_v2/utils.py:77\u001b[0m, in \u001b[0;36mfilter_empty_gradients\u001b[0;34m(grads_and_vars)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m filtered:\n\u001b[1;32m     76\u001b[0m     variable \u001b[39m=\u001b[39m ([v\u001b[39m.\u001b[39mname \u001b[39mfor\u001b[39;00m _, v \u001b[39min\u001b[39;00m grads_and_vars],)\n\u001b[0;32m---> 77\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     78\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNo gradients provided for any variable: \u001b[39m\u001b[39m{\u001b[39;00mvariable\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     79\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mProvided `grads_and_vars` is \u001b[39m\u001b[39m{\u001b[39;00mgrads_and_vars\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     80\u001b[0m     )\n\u001b[1;32m     81\u001b[0m \u001b[39mif\u001b[39;00m vars_with_empty_grads:\n\u001b[1;32m     82\u001b[0m     logging\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m     83\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mGradients do not exist for variables \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m when minimizing the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     84\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mloss. If you\u001b[39m\u001b[39m'\u001b[39m\u001b[39mre using `model.compile()`, did you forget to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     85\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mprovide a `loss` argument?\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     86\u001b[0m         ([v\u001b[39m.\u001b[39mname \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m vars_with_empty_grads]),\n\u001b[1;32m     87\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: No gradients provided for any variable: (['Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0'],). Provided `grads_and_vars` is ((None, <tf.Variable 'Variable:0' shape=(3, 3, 1, 32) dtype=float32, numpy=\narray([[[[ 0.6466784 ,  1.3636801 ,  0.2349704 , -0.68872184,\n           1.2708534 , -1.5039062 , -0.791255  , -0.93443525,\n          -0.81759375,  1.4529178 , -0.93918484, -1.431791  ,\n           0.19984002,  0.23216824, -2.6451733 , -1.4437139 ,\n           0.92665076,  1.3199555 , -0.17193408, -0.8771541 ,\n           0.250492  , -0.1008746 , -0.8012736 , -0.09100357,\n           0.25474587,  0.5349384 , -0.04049704,  0.7610705 ,\n          -2.3481848 ,  0.4639459 ,  0.67247576,  0.22725704]],\n\n        [[ 0.07807124,  0.44073853,  1.9378729 , -0.52310944,\n           1.4352068 , -0.53696984,  0.51148885, -0.36410838,\n          -0.5732031 ,  1.829762  ,  0.14513761,  1.0515347 ,\n          -0.68788207,  0.45367172, -1.7454324 ,  1.1318425 ,\n          -0.4222951 , -0.3760836 ,  0.0555972 , -0.21359812,\n          -1.0526816 ,  0.09952651, -1.172967  ,  0.08245756,\n           0.27712423,  0.08589459, -0.7428122 ,  0.3805969 ,\n           0.33666667,  0.83065766, -0.34107158,  0.34638366]],\n\n        [[-1.5269016 , -0.0960945 ,  0.07590263, -0.54164255,\n          -0.10711495, -1.380478  ,  1.0823843 ,  0.15391645,\n          -0.924603  ,  0.18526798, -1.1310121 ,  0.2559383 ,\n          -0.66276866,  0.68127084, -0.75414395, -0.07397736,\n           0.5319093 , -0.71319413, -0.26535887,  0.2075439 ,\n          -0.58865744, -0.9440555 , -0.88900894, -0.911177  ,\n           0.65423936,  0.05467885, -0.9757518 , -1.5119768 ,\n          -0.6218669 , -0.08284172, -0.51931447, -1.9477992 ]]],\n\n\n       [[[ 0.09281781,  0.31672472, -0.15312561,  0.6576407 ,\n          -0.06364119,  0.34230626,  0.09905884, -1.5473095 ,\n          -2.164004  , -1.163612  , -0.6128797 , -0.92909676,\n          -0.19283934,  1.3884772 , -0.33209106, -0.37142402,\n          -0.8569481 ,  1.3128698 , -0.5343361 , -0.44202265,\n          -0.28659397, -0.9150159 ,  1.0567747 ,  0.20239772,\n           0.15196304,  1.8412681 ,  1.4837056 ,  0.0178719 ,\n          -0.0156993 , -0.14400351,  1.2510555 ,  0.0341127 ]],\n\n        [[-1.8503035 ,  1.8630394 , -0.8387479 , -0.99032676,\n           0.88508016,  0.5592865 , -0.02534952,  0.11326825,\n          -0.9367322 , -0.6940385 ,  1.6300179 ,  0.56942695,\n          -0.1246592 , -0.2373819 ,  0.08806559, -0.35537207,\n           0.26375145,  1.5500369 , -1.231266  ,  1.4507219 ,\n           1.2303547 , -0.5123373 , -0.5643674 , -0.8016935 ,\n          -0.32689306,  0.5819346 , -0.4012869 ,  0.6183318 ,\n           0.5949261 , -0.49499986,  1.1726246 , -0.36204967]],\n\n        [[-1.0130643 , -0.11574296,  0.2570927 ,  0.19586007,\n           1.9387752 , -0.606752  , -2.0620928 , -0.58435684,\n          -0.3675099 ,  0.671357  , -1.0613112 , -0.10118797,\n          -1.6306659 , -1.4144033 ,  0.7392334 , -0.09799367,\n          -0.06009875, -0.01182518,  1.5021986 ,  1.0187073 ,\n           0.24639466, -0.6482466 , -0.09944835, -0.21933323,\n          -0.95047796, -0.15839301, -0.5509325 ,  0.84376884,\n          -1.1356637 ,  0.69268423,  0.31964287, -1.2675667 ]]],\n\n\n       [[[-0.80905783,  0.8667199 , -0.25556275,  0.8523271 ,\n          -0.48429334, -0.37295645, -0.64549637,  0.43377215,\n           0.21906446, -1.2036375 ,  0.52927613, -1.050511  ,\n           0.7938159 ,  0.11210593, -0.38645473, -0.41139477,\n          -0.9570582 , -1.6779778 ,  0.695617  ,  0.31683645,\n          -1.4159051 , -1.5264475 , -1.3047926 , -0.5657302 ,\n           0.18516006,  0.97149384,  0.6025923 ,  1.5755728 ,\n          -0.25813797,  0.44717535,  0.06383213, -0.2831915 ]],\n\n        [[-1.7273619 ,  2.3258348 , -0.34049243,  0.37726212,\n           0.57444376, -0.39842898,  0.8879237 ,  0.31475452,\n          -1.8742999 , -1.6084414 ,  0.46315092, -1.3360512 ,\n          -0.45779213, -0.3701041 ,  0.61123586, -1.5635326 ,\n          -0.4760754 , -1.2992508 , -1.0136776 ,  0.9237997 ,\n           0.67889476,  1.4121991 ,  0.15044291,  1.1735282 ,\n          -0.12994803, -0.40697154,  0.0719725 , -0.19998203,\n          -0.0301965 , -0.5427759 ,  1.5363353 ,  0.9429556 ]],\n\n        [[-0.33671847,  1.3157446 ,  0.480834  , -1.2014211 ,\n          -1.0534788 ,  0.27704492,  0.7215707 ,  0.938482  ,\n          -0.04459808, -0.16646737, -0.31469342,  0.78269255,\n          -0.95669174, -0.3777315 ,  2.0498457 , -2.0977733 ,\n          -1.5962831 ,  0.28712907,  2.7829552 , -1.0619161 ,\n          -0.6878612 ,  0.37806195,  1.3146728 , -0.10095701,\n           0.36397767,  0.34573236,  0.8799807 , -2.338214  ,\n           0.49898982,  0.26629388,  0.8019123 ,  1.2743303 ]]]],\n      dtype=float32)>), (None, <tf.Variable 'Variable:0' shape=(3, 3, 32, 64) dtype=float32, numpy=\narray([[[[-1.28501177e+00,  2.21001297e-01, -1.37426555e+00, ...,\n          -7.44226277e-01, -9.01089236e-02,  1.89547133e+00],\n         [ 1.73789752e+00, -1.28992593e+00,  1.06083751e+00, ...,\n          -2.66165912e-01,  7.71940112e-01,  6.72777832e-01],\n         [-9.56282794e-01,  2.73842901e-01, -2.83352554e-01, ...,\n          -6.81241751e-01, -1.31792462e+00, -2.04445437e-01],\n         ...,\n         [-2.27847815e-01,  2.79485703e-01,  1.22582293e+00, ...,\n           7.80237913e-01, -7.70506680e-01, -1.97411627e-01],\n         [-1.78352848e-01,  8.35640788e-01, -2.52330065e-01, ...,\n           4.80343681e-03, -3.38507563e-01,  1.60697401e+00],\n         [-3.83276939e-01, -1.60952067e+00,  2.42307987e-02, ...,\n          -7.19320998e-02,  7.17415333e-01,  7.48562574e-01]],\n\n        [[ 1.74640489e+00, -1.36689615e+00,  3.07348445e-02, ...,\n           1.89505562e-01,  3.25365514e-01,  6.95929885e-01],\n         [-2.58481950e-01, -2.76109487e-01, -6.57977462e-01, ...,\n           4.59287524e-01, -6.16989017e-01,  6.87181175e-01],\n         [-1.85145330e+00,  4.10361409e-01,  1.00966406e+00, ...,\n           3.64643186e-01, -1.56013739e+00,  1.02543700e+00],\n         ...,\n         [ 1.21084666e-02, -3.81899476e-01,  2.83397675e-01, ...,\n          -2.53390819e-01,  1.99577296e+00, -7.69565940e-01],\n         [ 9.05332386e-01, -1.18099320e+00, -1.12758732e+00, ...,\n          -6.83032632e-01, -1.12046218e+00,  4.43460017e-01],\n         [-1.26735258e+00,  7.43613780e-01,  5.62835336e-01, ...,\n           9.49347675e-01,  7.06774831e-01,  6.03103220e-01]],\n\n        [[ 2.03611404e-01,  7.58214593e-02,  6.95457822e-03, ...,\n           8.46060753e-01,  1.03755224e+00, -2.07883655e-03],\n         [ 4.37285274e-01, -9.67204347e-02,  7.23015547e-01, ...,\n           1.40392435e+00,  1.46620274e+00, -6.98349357e-01],\n         [ 5.68565607e-01,  1.01817906e-01,  2.43913159e-01, ...,\n           7.95711339e-01,  6.75419927e-01,  1.53690740e-01],\n         ...,\n         [-6.52542263e-02,  1.11705661e+00,  1.25255966e+00, ...,\n          -1.21668458e+00, -7.80488551e-01,  1.33168280e+00],\n         [-2.92690784e-01, -8.48050654e-01,  2.51032615e+00, ...,\n          -1.04444051e+00, -6.02462649e-01, -2.07154226e+00],\n         [-4.88085985e-01,  3.60181391e-01, -1.11222577e+00, ...,\n           1.79422820e+00,  8.90368223e-01, -2.17254549e-01]]],\n\n\n       [[[ 5.66388190e-01,  5.52431226e-01, -1.06032372e+00, ...,\n           4.56092596e-01, -2.98385471e-01, -5.66740692e-01],\n         [ 2.51531512e-01, -1.98542383e-02, -2.35990620e+00, ...,\n          -2.03288943e-01,  9.82225418e-01, -4.97511894e-01],\n         [ 1.92048177e-02, -1.10779274e+00, -9.78298366e-01, ...,\n          -1.98955989e+00, -7.91188776e-02,  1.49958074e+00],\n         ...,\n         [ 7.49038756e-01,  4.85949188e-01, -3.97529900e-01, ...,\n           7.06884742e-01,  7.87809432e-01,  5.72056293e-01],\n         [ 5.58984280e-01,  3.48396271e-01,  1.01750338e+00, ...,\n           1.15113187e+00, -6.43325269e-01,  1.45859087e+00],\n         [ 1.02705218e-01,  7.51789689e-01, -4.87259388e-01, ...,\n           2.55517793e+00, -1.34734058e+00,  1.16846752e+00]],\n\n        [[-1.63249508e-01,  2.30371571e+00,  1.23100948e+00, ...,\n           9.77602184e-01,  5.49855351e-01,  1.31390858e+00],\n         [-6.36480272e-01, -4.88942385e-01, -5.80457270e-01, ...,\n           1.95314869e-01,  1.45450675e+00,  1.43101490e+00],\n         [-2.25590020e-01,  6.54116452e-01,  2.85436082e+00, ...,\n           2.47810006e-01, -8.95251513e-01, -3.72122496e-01],\n         ...,\n         [ 5.86112618e-01,  8.30173612e-01,  8.04925084e-01, ...,\n           4.59570229e-01,  9.66685772e-01, -4.50384229e-01],\n         [ 1.91618234e-01,  3.11860472e-01,  5.44516325e-01, ...,\n           3.15398395e-01, -1.78454518e+00,  2.30853662e-01],\n         [-2.05887055e+00, -1.42848730e-01,  2.95713991e-01, ...,\n           9.60756242e-01, -1.59561205e+00, -7.14043796e-01]],\n\n        [[-5.59556782e-01,  1.19380081e+00, -6.16898239e-01, ...,\n           6.91363037e-01, -2.61935860e-01,  1.29656661e+00],\n         [-1.13740611e+00, -1.11326313e+00, -1.13228655e+00, ...,\n           1.22339815e-01,  9.23173845e-01,  1.26417649e+00],\n         [ 1.60155237e-01, -1.07735980e+00,  6.75862312e-01, ...,\n          -3.56934905e-01, -1.21129811e+00,  2.20156479e+00],\n         ...,\n         [ 2.29946375e+00,  7.18432963e-01,  8.40500951e-01, ...,\n          -6.99656248e-01,  5.93610108e-01,  5.54241352e-02],\n         [ 7.92913973e-01, -1.10270572e+00, -1.94935203e+00, ...,\n           4.39173967e-01, -1.58008528e+00,  1.31170952e+00],\n         [ 1.01542366e+00, -5.94914198e-01,  1.94248885e-01, ...,\n          -4.80923131e-02,  1.81735128e-01,  8.80310655e-01]]],\n\n\n       [[[-4.99432415e-01, -8.21795389e-02,  2.74607778e-01, ...,\n           2.67639428e-01,  5.24476171e-01,  1.67700812e-01],\n         [-5.10142706e-02, -5.11534931e-03,  1.08154678e+00, ...,\n           5.95132947e-01, -6.60834134e-01, -9.80324686e-01],\n         [ 2.61661440e-01,  2.26042295e+00, -6.68971181e-01, ...,\n          -5.02248108e-01,  6.37900352e-01,  1.80661559e-01],\n         ...,\n         [-4.87568527e-01,  1.63645661e+00,  9.98283252e-02, ...,\n          -1.03419411e+00, -4.67379332e-01, -2.93497038e+00],\n         [-6.87842369e-01,  1.44466412e+00, -1.00083327e+00, ...,\n           4.60774601e-01,  1.77842766e-01,  1.02879369e+00],\n         [-2.31635243e-01,  2.50851393e+00, -1.49452639e+00, ...,\n           3.60352576e-01, -3.48015666e-01, -6.29485905e-01]],\n\n        [[ 1.14421999e+00, -5.10448813e-01, -3.91618758e-01, ...,\n          -7.21642077e-01, -1.51449889e-01, -1.64317489e-01],\n         [ 6.46852627e-02, -1.51381299e-01, -1.25070393e+00, ...,\n           6.31057978e-01, -1.58391738e+00,  1.11950660e+00],\n         [-4.01114486e-02,  9.87593755e-02,  1.36637902e+00, ...,\n          -7.25611448e-01,  1.43486941e+00, -8.32441211e-01],\n         ...,\n         [-2.63060302e-01, -1.48743796e+00,  4.01194155e-01, ...,\n          -1.04725742e+00, -1.18656561e-01,  1.58995032e-01],\n         [-5.28152324e-02,  7.22421229e-01,  9.43726376e-02, ...,\n          -5.82612276e-01, -9.56169590e-02, -1.02376628e+00],\n         [ 2.54276609e+00,  1.06798506e+00, -2.34502256e-01, ...,\n           4.90798354e-02, -3.00816238e-01, -1.93705845e+00]],\n\n        [[ 6.01700425e-01, -1.07160544e+00,  1.77797055e+00, ...,\n           1.70212722e+00,  6.23772442e-01,  2.59412318e-01],\n         [ 5.91366217e-02,  1.17788613e-01, -5.94481587e-01, ...,\n           3.91842723e-01,  1.88633993e-01,  3.74266833e-01],\n         [-1.51539779e+00, -1.07414007e+00, -5.71994185e-01, ...,\n          -4.74832922e-01, -1.80356479e+00,  7.09669828e-01],\n         ...,\n         [ 1.01923192e+00, -2.65792787e-01, -9.21754956e-01, ...,\n           2.44235921e+00,  4.34302092e-01, -2.03685474e+00],\n         [-1.00646071e-01, -1.88738561e+00, -1.57680050e-01, ...,\n           2.19879761e-01, -8.09267461e-01, -9.47671011e-02],\n         [ 4.97202247e-01,  1.97922444e+00,  1.67721021e+00, ...,\n           1.61690378e+00,  2.80394584e-01,  2.39872003e+00]]]],\n      dtype=float32)>), (None, <tf.Variable 'Variable:0' shape=(3, 3, 64, 128) dtype=float32, numpy=\narray([[[[ 1.3233073 ,  1.8064544 ,  1.2181501 , ...,  1.4138821 ,\n          -0.7718814 , -2.8054655 ],\n         [ 0.59898347, -0.5681487 , -1.2306    , ..., -0.26275933,\n          -2.212186  , -0.11318617],\n         [ 0.7224077 ,  0.06756593,  0.49214745, ..., -0.8811041 ,\n           2.0311434 , -0.20555237],\n         ...,\n         [ 1.6860093 , -1.311523  , -0.80788857, ..., -0.988668  ,\n          -1.5223459 ,  0.43591508],\n         [-0.4165783 ,  1.2000271 , -0.19116594, ..., -0.5828922 ,\n          -0.6342656 , -1.5716248 ],\n         [-1.0388817 , -0.49023685, -0.04606558, ..., -0.58129495,\n          -0.43218365,  0.4670838 ]],\n\n        [[ 0.94176763, -0.6309817 , -1.3777807 , ...,  1.1745195 ,\n          -0.78341216, -0.9645744 ],\n         [ 0.9940354 ,  0.0953831 ,  0.94416046, ...,  1.450422  ,\n           1.4965519 , -1.846254  ],\n         [ 1.6381598 , -0.19217548, -0.15104827, ...,  0.22620176,\n          -1.2182889 ,  0.7811105 ],\n         ...,\n         [-1.3768874 , -0.5630665 , -0.8562116 , ..., -0.17876022,\n           2.800444  , -0.18243377],\n         [ 0.23891221, -0.25161123, -0.03095458, ...,  0.02137929,\n           0.3498833 , -0.04577332],\n         [ 0.8274535 ,  1.1673629 , -0.8575508 , ...,  0.32776988,\n          -1.1606731 , -2.0019898 ]],\n\n        [[ 0.11348422, -1.3422955 ,  1.4626156 , ..., -2.4061184 ,\n          -1.0291145 ,  1.2351063 ],\n         [ 0.4678401 ,  0.38888273, -1.2453578 , ...,  1.4669999 ,\n           1.0906198 ,  0.48497868],\n         [-0.48183277, -2.0633283 , -0.6981974 , ..., -0.17239378,\n          -0.23128591,  0.8199734 ],\n         ...,\n         [ 1.7617092 ,  0.71536607,  0.6602622 , ..., -1.2144499 ,\n           2.331084  ,  0.07272591],\n         [-0.68717813,  1.4124928 , -0.12826145, ...,  0.4896559 ,\n           1.108442  ,  0.21481436],\n         [ 1.9387612 , -0.54419434,  0.98810196, ..., -0.19792007,\n          -1.4844624 ,  0.5702481 ]]],\n\n\n       [[[-0.37527004, -0.5415016 , -1.4366795 , ...,  0.8084037 ,\n          -0.11347638,  0.10058681],\n         [-0.8631066 ,  0.92784756, -0.46636513, ..., -1.0522844 ,\n           0.32450733, -0.1340888 ],\n         [ 0.5145735 ,  1.1600758 , -1.0015857 , ..., -0.34105203,\n          -0.24955966,  0.37508315],\n         ...,\n         [-2.2977474 ,  0.6054437 , -1.0540564 , ..., -1.6177864 ,\n           0.66252387,  0.49018764],\n         [ 0.8196066 , -0.6957084 ,  0.47732154, ...,  0.8105063 ,\n          -0.31021813, -0.86657584],\n         [ 1.1024907 ,  0.2810599 , -1.325965  , ..., -0.09398641,\n           0.62665004,  1.5063751 ]],\n\n        [[ 0.61357194, -0.6505677 ,  0.35216728, ...,  0.3819855 ,\n           1.5928088 , -0.9366353 ],\n         [-1.7234296 ,  0.8455515 ,  1.1481297 , ...,  0.5267668 ,\n          -0.9692933 , -1.195038  ],\n         [ 0.97974855, -0.8383894 ,  0.23320137, ..., -0.5704547 ,\n           0.30827522,  0.6087184 ],\n         ...,\n         [-0.514214  , -2.1994634 ,  0.3310736 , ..., -0.31598493,\n          -0.12177974,  1.4911239 ],\n         [-0.84602004,  0.68697983, -0.6419436 , ...,  0.5377825 ,\n           0.07640764, -0.42117974],\n         [-0.8744789 , -2.2752192 , -1.4658711 , ...,  1.3513074 ,\n          -1.9683368 , -1.1780033 ]],\n\n        [[ 0.57536733, -0.2511476 ,  0.5785398 , ...,  0.04475286,\n          -0.2982519 , -0.08370461],\n         [ 1.1266229 , -0.61560273,  0.34671578, ...,  0.6114166 ,\n          -0.64726585, -0.23780075],\n         [-0.03091933, -0.7379663 , -0.7063694 , ...,  0.84956455,\n          -1.0647038 , -1.0612644 ],\n         ...,\n         [-0.517869  ,  0.3420521 , -0.9193096 , ...,  1.7494043 ,\n           0.15477942, -0.94548184],\n         [-1.1123685 ,  0.00855371,  0.4165032 , ...,  0.08317707,\n           0.25753084,  0.57715577],\n         [ 0.7367319 , -0.59352654,  0.5554434 , ...,  1.2634659 ,\n           0.11332662, -0.8425391 ]]],\n\n\n       [[[ 0.99552125,  1.1221021 , -0.26882127, ..., -0.18767463,\n          -0.302475  ,  0.714045  ],\n         [ 1.1835625 , -0.2894701 , -0.3201054 , ..., -0.6023518 ,\n           0.1169323 ,  2.0028248 ],\n         [ 1.0045099 ,  0.24113937,  0.8757464 , ...,  0.36643156,\n           0.37209225, -1.5541159 ],\n         ...,\n         [-1.8474052 ,  1.4149529 , -1.8805466 , ..., -0.16018444,\n          -0.13593034,  0.90918756],\n         [ 1.0632154 , -0.8133915 ,  0.06553711, ..., -0.32622653,\n          -0.06655673, -0.69040704],\n         [-0.10322361,  0.24309842, -0.26507849, ..., -0.59569806,\n           0.79297817, -0.3443945 ]],\n\n        [[-0.2862197 , -1.2055069 , -0.7909686 , ..., -1.2329881 ,\n           0.09533951,  1.9988099 ],\n         [ 0.74162245, -1.9307323 ,  0.38849795, ...,  0.5705388 ,\n          -1.0413673 , -0.7350458 ],\n         [-0.64483964,  0.49457783,  1.1962279 , ..., -0.02270497,\n           1.7585356 , -0.55424166],\n         ...,\n         [-0.0251463 , -0.52311254, -1.3967589 , ..., -0.8843045 ,\n          -1.0770103 , -0.34927273],\n         [ 0.6053366 , -0.3187969 , -0.67199343, ...,  0.3371216 ,\n           1.0911036 , -0.25764167],\n         [ 0.21261099,  1.5784614 , -0.6913989 , ...,  0.06362043,\n          -2.5404527 , -0.7977484 ]],\n\n        [[ 1.8318902 ,  1.4868467 ,  1.9670925 , ...,  0.1330779 ,\n           0.6669271 ,  1.646579  ],\n         [-0.7797185 , -1.4560398 , -0.755379  , ..., -0.11187799,\n           1.4000828 , -0.94481117],\n         [-0.5794052 , -1.1983913 ,  0.06539527, ...,  1.1226453 ,\n           1.5906903 , -0.94647425],\n         ...,\n         [ 1.1998713 ,  1.2653767 ,  0.976917  , ...,  0.3518431 ,\n          -0.10479601, -0.55302554],\n         [ 0.2480868 ,  1.0960294 ,  1.3069167 , ...,  1.1822996 ,\n          -2.008866  , -0.7250279 ],\n         [ 0.23299439, -0.59291685,  0.8448248 , ..., -0.16138029,\n          -1.218463  , -0.26266512]]]], dtype=float32)>), (None, <tf.Variable 'Variable:0' shape=(2048, 4096) dtype=float32, numpy=\narray([[-1.0119869 , -0.40464824, -0.5461457 , ...,  0.08166128,\n        -0.7049107 , -0.2257863 ],\n       [-0.9411602 , -0.3384296 ,  0.08271101, ...,  0.07420039,\n        -0.1318149 ,  0.2679172 ],\n       [ 0.29058805,  0.07582489, -0.02803992, ...,  1.1054467 ,\n        -0.35974282,  0.2864533 ],\n       ...,\n       [ 0.2454947 ,  0.09079299,  0.7777444 , ..., -1.847688  ,\n         0.43146208,  1.0686647 ],\n       [ 0.14506939, -1.1283616 , -0.5485785 , ...,  1.425123  ,\n         0.52094734,  0.31447452],\n       [-0.79007554, -0.45471472,  1.7339969 , ..., -0.9152863 ,\n         0.08569179, -0.40256593]], dtype=float32)>), (None, <tf.Variable 'Variable:0' shape=(4096, 4096) dtype=float32, numpy=\narray([[-0.591581  , -1.2791404 ,  0.58100176, ..., -0.9487903 ,\n         1.0802866 , -1.4643636 ],\n       [ 0.04008683, -0.09805308,  1.228761  , ...,  1.2371086 ,\n         0.42483976, -1.2524638 ],\n       [ 0.39137298, -0.8011432 ,  0.92518723, ...,  0.34498268,\n        -3.269524  ,  0.09526349],\n       ...,\n       [ 1.98609   ,  0.4279652 ,  0.24282114, ..., -1.0728173 ,\n         1.0532737 ,  0.69181705],\n       [ 0.8825209 ,  0.88111   ,  0.62725663, ..., -0.18520081,\n         0.40083268, -0.9509889 ],\n       [-0.65724087, -1.2953994 , -0.35906908, ...,  3.1098936 ,\n         0.22726241, -1.4607003 ]], dtype=float32)>), (None, <tf.Variable 'Variable:0' shape=(16777216, 10) dtype=float32, numpy=\narray([[ 2.0216851 , -1.8725924 , -1.798161  , ...,  0.38258243,\n        -0.30104527,  0.19490388],\n       [-1.196796  , -0.14725482,  0.19585314, ..., -0.6332138 ,\n         1.0584236 ,  1.149426  ],\n       [ 1.4368621 , -0.3839124 , -0.27429628, ..., -0.200673  ,\n        -0.06156503,  0.19315498],\n       ...,\n       [ 0.9254828 ,  1.5334361 , -0.1104705 , ...,  1.0014472 ,\n         0.8921647 , -0.52656955],\n       [-0.61447763,  0.18046185,  0.05631652, ...,  1.1166717 ,\n         1.2567182 ,  0.3220139 ],\n       [ 0.15944856,  0.12971127,  0.48683923, ..., -0.1404805 ,\n         0.02118052, -0.7058533 ]], dtype=float32)>))."
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "\n",
    "for i in range(epochs):\n",
    "\n",
    "    # for b, (batch_images, batch_labels) in enumerate(zip(train_batch_images, train_batch_labels)):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # if len(weights):\n",
    "        #     pred, _ = model(batch_images, 0.01, weights)\n",
    "        pred = model(train_images, 0.01)\n",
    "        # else:\n",
    "        #     pred, weights = model(batch_images, 0.01)\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits(train_labels_1hot, pred)\n",
    "    sgd.minimize(loss, var_list=weights, tape=tape)\n",
    "    print(f\"epoch: {i}loss: {tf.reduce_sum(loss)}\")\n",
    "    # print(f\"epoch: {i}, batches: {b}, loss: {tf.reduce_sum(loss)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
